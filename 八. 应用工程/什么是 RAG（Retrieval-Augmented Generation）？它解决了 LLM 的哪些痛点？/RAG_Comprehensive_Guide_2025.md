# RAG（检索增强生成）：让模型先去“翻资料”再回答（2025/2026）

我喜欢把大模型想成一个很会说话、很会联想的人：你问它什么，它总能回你一段听起来像那么回事的答案。

问题是：它不一定“知道”。它只是很擅长“像知道一样说”。

RAG 的想法特别朴素：别让模型硬猜。先去资料库里把相关内容找出来，再把这些内容塞给模型，让它基于材料回答。

---

## 第一部分：为什么我们需要 RAG？

不用 RAG 也能用大模型，但在真实业务里，很快会撞上这三堵墙：

### 1. 知识过期（Knowledge Cutoff）

模型的知识来自训练数据。训练结束之后，它就不会自动更新“今天发生了什么”。

- 你问：“今天 DeepSeek 的股价是多少？”
- 模型：要么答不上来，要么开始瞎编一个看起来像真的数字
- RAG 的处理方式：先去搜最新数据/新闻，再让模型基于结果回答

### 2. 幻觉（Hallucination）

当模型没有足够信息时，它有个“坏习惯”：把空白用合理的句子填满。

- 你问：“公司最新的报销政策是什么？”
- 你最不想要的结果：一份排版很工整、逻辑很顺、但完全错误的“政策”
- RAG 的处理方式：要求模型只根据检索到的文档回答；没找到就说“我没查到”

### 3. 私有数据不可知（Private Data）

公司里的合同、代码、知识库、会议纪要……这些都不在公有模型的训练集里。

- 你想让模型回答内部问题，但它“根本看不到材料”
- RAG 的处理方式：把私有数据做成可检索的索引，回答时临时取用

---

## 第二部分：RAG 是如何工作的？

你可以把现代 RAG 当成一条流水线。一句话概括就是：把问题变清楚 → 把材料找回来 → 挑最相关的几段 → 让模型写答案。

对应到常见组件，大概是这样：

```text
Query Translation（查询转换） → Retrieve（检索） → Rerank（重排） → Generate（生成）
```

1. **Query Translation（查询转换）**：用户的提问往往不完整或含糊。
   - **HyDE**：先让模型写一个“可能的答案草稿”，再拿草稿去搜（有点像先写关键词再搜索）。
   - **Multi-query**：把一个问题换三种问法并行检索，防止漏掉关键信息。
2. **Retrieve（混合检索）**：
   - **向量检索**：找“意思接近”的内容。
   - **关键词检索（BM25）**：找“字面命中”的内容，尤其对专有名词很管用。
   - 现在更常见的是两者一起用（Hybrid Search），有的系统还会加 Learned Sparse Embeddings（比如 SPLADE）。
3. **Rerank（重排序）**：
   - 你检索回来的可能是 100 段，但真正有用的也许只有 5 段。
   - 用精排模型（Cross-Encoder 或 ColBERT）重新打分，留下最相关的 Top-K（比如 Top-5）。
   - ColBERT（Late Interaction）很受欢迎：通常比 Cross-Encoder 快，又比纯向量召回更准。
4. **Generate（生成）**：LLM 阅读 Prompt 并生成答案。

---

## 第三部分：核心组件——向量数据库（Vector Database）

向量数据库（或者更泛一点：向量检索系统）就是 RAG 的“资料柜”。你把文档放进去，之后可以按语义把相关段落拿出来。

### 1. 为什么不能只用 MySQL 或 Elasticsearch？

一个简单的心智模型是：你其实在做两种不同的“找东西”。

- **关键词搜索（BM25）**：靠字面匹配。
  - 优点：搜“SN-2024-X1”这种精确编号，基本不会错。
  - 缺点：你搜“重置密码”，可能找不到写着“修改口令”的文档（语义鸿沟）。
- **向量搜索（Vector Search）**：靠语义相似。
  - 优点：它能知道“重置密码”和“修改口令”大概率在讲同一件事。
  - 缺点：它也会“太聪明”。你搜“iPhone 15”，它觉得“iPhone 14”也挺像，于是给你一堆相近但不等价的结果（电商里这类事会直接变成事故）。

### 2. 前沿趋势：Matryoshka Embeddings (MRL) 与 Binary Quantization

当数据规模上去之后，问题就变成：怎么让它跑得更快、存得更省。

- **Matryoshka Embeddings (MRL)**：允许你“像俄罗斯套娃一样”只取向量的前 64 维或 128 维进行粗排，效果损失极小，但速度提升几十倍。
- **Binary Quantization (BQ)**：将浮点数向量压缩为 0/1 比特串，内存占用减少 30 倍，检索速度提升明显。

---

## 第四部分：成败关键——文本分块（Chunking）

分块这件事，经常决定你 RAG 能不能用。

如果你把一段话切得支离破碎，模型就算检索到了那一块，也看不懂上下文；反过来，如果切得太大，又会把噪音一起塞进来。

### 1. 传统的切分策略

- **固定大小切分（Fixed-Size）**：简单粗暴，记得加 Overlap。
- **递归字符切分（Recursive Character）**：LangChain 默认。按段落、句子优先切。
- **语义切分（Semantic Chunking）**：按“意思”突变点切，效果好但慢。

### 2. 2025/2026 前沿策略：Contextual Retrieval & Late Chunking

近一两年大家在这里做了很多“补救上下文”的工作：

- **Contextual Retrieval (上下文感知检索)**：由 Anthropic 推广。在切分前，让 LLM 为**每个 Chunk 生成一段简短的 Context 说明**并拼接到 Chunk 前面。
  - 单独看“它增长了 50%”你完全不知道“它”是谁；加上 Context 后变成“Apple 2024 Q4 财报：营收增长了 50%”，检索命中率会好很多。
- **Late Chunking (迟滞切分)**：由 Jina AI 提出。先对全文做 Embedding，利用 Transformer 的注意力机制保留全局语义，然后再切分向量。这比“先切文本再 Embedding”能保留更多跨 Chunk 的语义信息。

---

## 第五部分：进阶挑战与解法

### 1. 长文本搜短标签（非对称检索）

“一大段长文本”去搜“几个短语标签”，经常会搜得很差（两边的信息密度不对称）。

常见解法：

- **降维打击**：先用 LLM 把长文本总结成短语，再用“短语搜短语”。
- **ColBERT**：利用其 Late Interaction 机制，能捕捉细粒度的语义匹配，比单纯的向量余弦相似度更强。

### 2. GraphRAG：知识图谱的降维打击

传统 RAG 更擅长回答“某个点是什么”，不太擅长“它们之间怎么关联”“整体长什么样”。GraphRAG 的思路是：把实体和关系抽出来，先搭个图，再基于图做检索与总结。

- **原理**：LLM 提取实体和关系，构建图谱。
- **趋势**：**LightRAG** 和 **HippoRAG** 正在降低构建图谱的成本，使其不再是微软专享的贵族技术。

---

## 第六部分：Agentic RAG（代理式 RAG）

很多系统已经不满足“搜一次就写”。它们会边写边判断：我是不是该再查一下？

- **Self-RAG (自我修正 RAG)**：LLM 在生成时会自我反思：“我这段话需要查资料吗？”、“查回来的资料有用吗？”、“我生成的答案对吗？”。如果不对，它会自己重新搜。
- **CRAG (Corrective RAG)**：引入一个轻量级的评估器，如果检索结果质量差，直接触发 Web Search (用 Google 搜索) 来补充信息，而不是强行回答。

---

## 结语：RAG 的未来

长上下文模型越来越强，有人会问：那还需要 RAG 吗？

我觉得答案是：两者做的事情不一样。

一个更实用的分工是：

- **Long Context 用于“推理”**：把相关的几十篇文档全部塞进窗口，让 LLM 进行深度对比分析。
- **RAG 用于“回忆”**：从海量数据库中快速捞出那几十篇文档。

RAG 看起来像“外挂记忆”，但更像是：你给模型配了一套可靠的找资料流程。模型不再凭感觉回答，而是学会了先把证据摆在桌面上。

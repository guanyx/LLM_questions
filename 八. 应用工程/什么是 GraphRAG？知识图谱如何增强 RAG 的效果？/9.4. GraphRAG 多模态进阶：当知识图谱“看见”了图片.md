# GraphRAG 多模态进阶：当知识图谱“看见”了图片

作为本系列的最后一篇，我们不打算只是对前面内容的简单总结，而是要把视线投向更远的未来。

在前面的文章中，我们讨论的 GraphRAG 几乎全是基于**纯文本**的。然而，现实世界的信息并不是只有文字。企业的 PDF 报告里有大量的图表，电商的商品详情里有丰富的图片，工厂的维修手册里有复杂的电路图。

如果你的 RAG 系统只能读懂文字，那它就相当于“半个盲人”。

**Multimodal GraphRAG（多模态图谱 RAG）** 正是 2025 年最激动人心的技术前沿。它试图解决一个核心问题：**如何将图片、音频、视频等非结构化数据，像文本一样“编织”进知识图谱里，并实现跨模态的逻辑推理？**

---

## 一、 为什么我们需要多模态图谱？

传统的 RAG 处理图片通常有两种简陋的方式：

1.  **OCR 转文字**：把图片里的字识别出来，丢掉图片本身。
    - _缺点_：丢失了视觉结构信息（如饼图的占比、流程图的箭头方向）。
2.  **Image Captioning**：用多模态大模型给图片写一句描述（如“这是一张财务报表”）。
    - _缺点_：描述往往太笼统，检索时很难通过细节（如“Q3 季度增长率”）召回这张图。

**多模态图谱（MM-KG）** 的核心价值在于：**它将视觉元素（Visual Entities）作为一等公民（First-class Citizen）存入图谱。**

- **场景示例**：
  - 用户问：“这款发动机的**进气阀**在哪里？”
  - 文本文档说：“进气阀位于气缸盖左侧。”
  - 设计图纸里画了一个阀门，并标注了 ID `Valve-A`。
  - **MM-KG**：建立一条边 `(实体:进气阀) --[visual_ref]--> (图片区域:设计图A_坐标xy)`。
  - **RAG 效果**：系统不仅回答位置，还能直接**把设计图的那一部分高亮裁剪出来**展示给用户。

---

## 二、 架构设计：如何“存”图片？

在工程落地时，我们面临的第一个挑战是：**图数据库（Neo4j/FalkorDB）并不适合存图片文件。**

### 1. 存储策略：引用而非内嵌

千万不要把 Base64 编码的图片存到图数据库的节点属性里！这会把数据库撑爆且极度拖慢查询。

**最佳实践架构（2025 Mainstream）**：

- **对象存储（S3/MinIO）**：存储原始图片文件（`image_01.jpg`）。
- **向量数据库（Milvus/Weaviate）**：存储图片的 Embedding（用 CLIP 或 SigLIP 模型编码）。
- **图数据库（Neo4j/FalkorDB）**：存储图片的**元数据节点**。

```json
// 图数据库中的“图片节点”结构示例
{
  "id": "img_node_8848",
  "type": "Image",
  "properties": {
    "s3_url": "s3://bucket/reports/2024/chart_05.png",
    "caption": "2024年Q3季度亚太区销售占比饼图",
    "vector_id": "vec_996", // 指向向量库
    "ocr_text": "Sales: China 40%, Japan 30%..."
  }
}
```

### 2. 进阶架构：ColPali 与“视觉即索引”

2024 年底爆火的 **ColPali** 提出了一种新范式：**不依赖 OCR，直接对文档页面截图进行多向量索引（Multi-Vector Indexing）。**

- **传统流**：PDF -> OCR -> Text Chunk -> Embedding。
- **ColPali 流**：PDF -> Page Image -> VLM -> Patch Embeddings (Late Interaction)。
- **结合 GraphRAG**：图谱中的文档节点直接指向 ColPali 索引的 Page ID。检索时，先用 ColPali 找到视觉上相关的页面，再通过图谱扩展到相关联的实体。

### 3. 关系建模：跨模态连接

这是最迷人的部分。我们需要建立文本实体与图片节点之间的关系。

- **显式关系**：如果 HTML `<img>` 标签有 `alt` 属性或上下文提到了“如下图所示”，直接建立边。
  - `(段落:Section_3) --[contains_image]--> (图片:img_01)`
- **隐式关系（视觉实体对齐）**：
  - 利用 **DeepSeek-VL2** 或 **Qwen2-VL** 的 **Visual Grounding（视觉定位）** 能力。
  - 如果 VLM 识别出图片里有一辆“特斯拉 Model 3”，并给出了 Bounding Box 坐标 `[x1, y1, x2, y2]`。
  - **自动建边**：`(实体:Tesla Model 3) --[appears_in {bbox: [...]}]--> (图片:img_tesla_promo)`

---

## 三、 检索策略：MM-RAG 的“三板斧”

建好了多模态图谱，检索时怎么玩？

### 1. 文本搜图片（Text-to-Image Retrieval）

- **Query**：“给我看一眼特斯拉的底盘结构。”
- **路径**：
  1.  实体链接：找到 `(实体:特斯拉底盘)`。
  2.  图遍历：查找 `(实体:特斯拉底盘) --[depicted_in]--> (图片:img_chassis_001)`。
  3.  返回：直接返回图片 URL，甚至可以利用 Bounding Box 裁剪出具体部位。

### 2. 图片搜文本（Image-to-Text Retrieval）

- **Query**：（用户上传了一张故障零件的照片）“这玩意儿坏了怎么修？”
- **路径**：
  1.  **视觉编码**：用 VLM 提取照片特征，或者生成描述“生锈的液压杆”。
  2.  **混合检索**：用“液压杆”去图谱里搜维修手册实体。
  3.  **图遍历**：找到 `(实体:液压杆) --[maintenance_guide]--> (文档:手册_第5章)`。
  4.  返回：维修步骤文本。

### 3. 动态图谱构建（Query-Driven Construction）

这是 **ACL 2025** 研究的前沿方向。对于复杂问题，静态图谱可能不够用。

- **Query**：“这张财报图表里的数据，和正文里写的增长率一致吗？”
- **执行流**：
  1.  **按需提取**：系统发现 Query 需要比对图片细节，实时调用 VLM（如 GPT-4o）将图片中的表格转化为临时的“表格节点”插入图谱。
  2.  **子图推理**：在包含“临时表格节点”和“正文节点”的子图上进行推理。
  3.  **LLM 仲裁**：输出比对结果。
  - _优势_：避免了预处理阶段对所有图片进行昂贵的细粒度抽取，只在查询时按需计算。

---

## 四、 2025 技术栈推荐

如果你想动手尝试 Multimodal GraphRAG，以下是目前的**黄金组合**：

1.  **VLM 模型（视觉理解）**：
    - **GPT-4o / Claude 3.5 Sonnet**：目前最强的读图与推理模型（贵但好用）。
    - **DeepSeek-VL2**：2024 年底发布的强者，采用 MoE 架构，**Visual Grounding（定位）** 能力极强，适合做精细化实体对齐。
    - **Qwen2.5-VL**：阿里通义千问最新版，处理高分辨率图片和长视频能力卓越，开源界的首选。
2.  **Retrieval 模型**：
    - **ColPali**：基于 VLM 的文档检索模型，直接“看”页面，效果吊打传统 OCR。
    - **BGE-Visual**：智源推出的多模态 Embedding。
3.  **编排框架**：
    - **LlamaIndex**：目前对 Multimodal RAG 支持最完善的框架，支持 `MultiModalVectorStoreIndex`。
    - **LangChain**：通过自定义 Chain 也可以实现，但生态组件略少于 LlamaIndex。

---

## 五、 未来展望：当图谱“听见”声音

多模态不仅仅是图片。**Omni-modal（全模态）** 正在到来。

- **Video GraphRAG**：将视频拆解为“关键帧（Keyframes）”和“音频脚本（Transcript）”。图谱连接 `(事件:发布会) --[has_video_clip]--> (视频片段:05分20秒-05分50秒)`。
- **Native Omni Models**：像 **Qwen2-Audio** 或 **GPT-4o-Audio** 这样原生支持音频理解的模型，允许用户直接用语音提问，系统检索图谱后，直接用语音生成带情感的回复。

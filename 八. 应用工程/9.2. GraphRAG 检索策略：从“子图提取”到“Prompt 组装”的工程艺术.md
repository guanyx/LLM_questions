# 从“子图提取”到“Prompt 组装”的工程艺术

在前两篇文章中，我们解决了 GraphRAG 的“原理”和“建图”问题。现在，我们面临一个更贴近运行时（Runtime）的核心挑战：**图建好了，怎么用？**

这并不是一个简单的问题。你不能把整个知识图谱（可能包含数百万个节点）直接扔给 LLM。你需要精准地从图谱中“切”出一块最相关的**子图（Subgraph）**，并将其转化为 LLM 能听懂的 Prompt。

如果说建图是“修路”，那么检索就是“导航”。导航导歪了，路修得再好也没用。本文将深入探讨 GraphRAG 在检索阶段的工程策略，特别是如何解决 **上下文爆炸（Context Explosion）** 和 **Prompt 格式化** 这两个拦路虎。

---

## 一、 检索（Retrieval）：如何在海量节点中“大海捞针”？

GraphRAG 的检索不仅仅是 `Find Node A`，而是要找到 `Node A` 周围真正有价值的上下文。

### 1. 入口识别（Entry Point Selection）

用户问：“马云最近有什么动态？”
系统首先要进行 **实体链接（Entity Linking）**，将自然语言中的“马云”映射到图谱中的节点 ID `node_10086`。

- **难点**：模糊匹配。用户可能搜“Jack Ma”、“马老师”。这里通常需要配合 **Vector Index**（对节点名做 Embedding）来实现模糊召回。

### 2. 子图扩展（Subgraph Expansion）

找到入口节点后，我们不能只看它自己，必须向外“走几步”。

- **K-Hop 邻居搜索**：最基础的策略。
  - **1-Hop**：获取马云的直接邻居（阿里巴巴、软银...）。
  - **2-Hop**：获取马云邻居的邻居（阿里巴巴 -> 淘宝，软银 -> 孙正义）。
- **痛点：爆炸系数**。图是高度互联的。1-Hop 可能有 50 个节点，2-Hop 可能就变成 2000 个节点了。直接把这 2000 个节点喂给 LLM，Token 瞬间爆炸。

### 3. 智能剪枝（Smart Pruning）—— 核心技术

为了解决爆炸问题，2024-2025 年的主流方案已经从单纯的 PageRank 转向了更动态的剪枝策略。

- **基于 PageRank 的静态剪枝**：计算邻居节点在全局图谱中的重要性，保留 Top-N 个“大 V”节点。
- **基于语义相似度的动态剪枝（Semantic Pruning）**：
  - 计算用户 Query 的向量与邻居节点（或边属性）的向量相似度。
  - **示例**：问“马云的**慈善**活动”，系统会保留连接着“公益基金会”的边（高权重），剪掉连接着“淘宝双 11”的边（低权重）。
- **Agentic 剪枝（Agentic Pruning）[2025 新趋势]**：
  - 不再用死规则，而是让一个小模型（Agent）充当“剪刀手”。
  - Agent 观察当前的子图状态，动态决定是“继续深挖这条路径”还是“到此为止”。这比静态规则更灵活，但延迟稍高。

---

## 二、 组装（Augmentation）：如何把“图”喂给 LLM？

剪枝完成后，我们手头有了一个包含几十个节点和边的小型子图。现在的任务是：**怎么把这个数据结构变成 Prompt？**

### 1. 格式化策略：自然语言 vs 结构化数据

- **方案 A：转换成自然语言（Natural Language Generation）**
  - 将三元组 `(马云, 创立, 阿里巴巴)` 转换成句子：“马云创立了阿里巴巴。”
  - **优点**：LLM 读起来最顺畅，注意力机制（Attention）损耗小。
  - **缺点**：Token 占用多，且容易丢失图的结构信息（如环状结构）。
- **方案 B：结构化描述（JSON / YAML / Adjacency List）**
  - **推荐格式**：`Markdown Adjacency List` 或 `YAML`。
  - **示例**：
    ```yaml
    Entities:
      - id: 马云
        type: Person
        desc: 阿里巴巴创始人
      - id: 阿里巴巴
        type: Company
    Relationships:
      - source: 马云
        target: 阿里巴巴
        relation: 创立
        time: 1999年
    ```
  - **优点**：Token 密度高，结构清晰，LLM（特别是 GPT-4o, Claude 3.5）对这种结构化数据的理解能力极强。
- **方案 C：Graph-of-Thoughts (GoT) Prompting [2025 新趋势]**
  - 不直接给数据，而是引导 LLM 进行图式思维。
  - 在 Prompt 中明确要求：“请先列出核心实体，再推导实体间的路径，最后得出结论。”这种 Chain-of-Thought 的升级版能显著提升推理准确率。

### 2. 提示词工程（Prompt Engineering）

一个优秀的 GraphRAG Prompt 模板通常长这样：

```markdown
# Role

你是一个基于知识图谱的智能助手。

# Context

以下是从知识图谱中检索到的相关信息（Entities & Relationships）：

<knowledge_graph>
{subgraph_yaml_content}
</knowledge_graph>

# Task

请根据上述图谱信息回答用户问题。
注意：

1. 必须基于图谱中的事实，不要编造。
2. 如果图谱中存在多跳路径（A->B->C），请在回答中明确指出这种推理链条。

# User Question

{user_query}
```

---

## 三、 进阶：Graph + Vector 的混合检索（Hybrid Retrieval）

在真实业务中，单一的 Graph 检索往往不够。我们需要“左右互搏”。

### 1. 为什么需要混合？

- **Vector RAG** 擅长：语义匹配。比如搜“那种像云一样的计算技术”（语义匹配到“云计算”）。
- **Graph RAG** 擅长：硬逻辑。比如搜“马云的公司的子公司的最大股东”。

### 2. 权重分配策略（Re-ranking）

当 Vector Search 返回了文档 A，Graph Search 返回了文档 B，如何排序？

- **Reciprocal Rank Fusion (RRF)**：一种经典的倒排融合算法。不依赖具体分数，只看排名。
  - `Score = 1 / (k + rank_vector) + 1 / (k + rank_graph)`
- **LLM Reranking**：
  - 最“豪横”但最有效的方法。把 Vector 和 Graph 找出来的 Top-50 结果全部扔给一个小模型（如 BGE-Reranker 或专门的 Prompt），让模型判断哪个更符合用户意图。
- **自适应检索（Adaptive Retrieval）[2025 新趋势]**：
  - 在检索前加一个分类器（Router）。
  - 简单问题 -> 走 Vector RAG（快）。
  - 复杂推理问题 -> 走 GraphRAG（准）。
  - 这解决了 GraphRAG “贵且慢”的问题，只在刀刃上花钱。

---

## 四、 总结与建议

对于初级 AI 工程师，在做检索和组装时，请记住以下 **Best Practices**：

1.  **别贪心**：不要试图把整个 2-Hop 邻居都塞进去。**语义剪枝**（Semantic Pruning）是 GraphRAG 能否上线的关键。
2.  **结构化 Prompt**：尽量使用 YAML 或 Markdown 列表来描述图数据，比纯自然语言更省 Token，且逻辑更清晰。
3.  **拥抱混合检索**：不要把 GraphRAG 当成 Vector RAG 的替代品，而是**互补品**。使用 RRF 或简单的 Router 来融合两者的优势。

图谱只是手段，精准的检索和优雅的 Prompt 组装，才是让 LLM 发挥逻辑推理能力的“临门一脚”。

# 从“意图”到“咒语”：Prompt 扩写模型的微调艺术

在“意图识别 + 模版/扩写”的双轨制设计中，**Prompt 扩写 (Prompt Expansion)** 承担着“右脑”的角色。它不负责理性的结构提取，而是专注于感性的艺术加工——将用户干瘪的指令（如“一只猫”），翻译成底层生图模型能听懂的、充满细节的“咒语”。

本文将跳过架构讨论，直接深入核心：**如何训练一个既懂用户意图，又懂模型方言的“扩写专家”？**

## 一、 为什么通用 LLM 还不够好？

虽然 GPT-4o 或 DeepSeek-V3 已经能写出很棒的 Prompt，但在工业级文生图产品中，直接调用通用 LLM 存在三个痛点：

1.  **方言不通 (Model Dialects)**：
    - **Flux.1** 喜欢自然语言长句，厌恶 Tag 堆砌。
    - **Pony Diffusion (SDXL)** 需要特殊的 `score_9, score_8_up` 前缀。
    - 通用 LLM 往往难以精准掌握这些“黑话”。
2.  **幻觉风险**：可能会编造不存在的 LoRA 触发词，或者输出错误的参数格式。
3.  **成本与延迟**：高并发场景下，Token 成本和流式输出的延迟是巨大的负担。

因此，**基于小模型（SLM）的专用微调 (Fine-tuning)** 才是 2025 年的最佳实践。

## 二、 核心技术：数据构造的革命

微调效果好不好，完全取决于数据质量。传统的“爬虫抓取 Civitai Prompt”方式已经落伍，因为很多高分图的 Prompt 其实写得很烂。

2025 年的标准范式是 **VLM 逆向工程 (The JoyCaption Paradigm)**。

### 1. 图像描述 (Captioning)

使用 **Florence-2** 或 **JoyCaption**（专门针对生图优化的 VLM）对高质量图片生成极度详细的描述（Dense Caption）。

> _JoyCaption Output_: "A digital art piece featuring a cyberpunk street scene. In the center, a cyborg girl with neon blue hair..."

### 2. 意图压缩 (Intent Compression)

使用 DeepSeek-V3 将上述长描述“压缩”回用户的原始简短意图。

> _Input_: (上面的长描述)
> _Output_: "赛博朋克风格的机械少女"

### 3. 数据集闭环

将 `(意图, JoyCaption描述)` 构成一组训练数据。
这种方式训练出的模型，扩写出的 Prompt **不仅文采好，而且画面对应性极强**，完美解决了“文不达意”的问题。

## 三、 训练目标：从文本到结构化 JSON

现代生图工作流（尤其是 ComfyUI）需要的不仅仅是一个 Prompt 字符串，而是参数包。
我们要训练扩写模型直接输出 **JSON**，实现“推理即配置”：

```json
{
  "positive_prompt": "A digital art piece featuring a cyberpunk street scene...",
  "negative_prompt": "blurry, low quality, illustration, 3d render", // Flux 可能为空，Pony 必填
  "parameters": {
    "cfg_scale": 3.5, // Flux 倾向于低 CFG
    "steps": 20
  },
  "lora_trigger_words": ["cyborg_style_v2"]
}
```

**多模型适配策略**：
在 System Prompt 中加入 `<model_type>` token，让一个模型学会多种方言：

- `<flux>` -> 输出自然语言，空负面词。
- `<pony>` -> 输出 `score_9` 前缀，重负面词。

## 四、 强化学习：ORPO 带来的质变

微调（SFT）只能让模型学会说话，强化学习（RL）才能让模型学会“讨好用户”。

推荐使用 **ORPO (Odds Ratio Preference Optimization)**，它比传统的 DPO 更高效，不需要训练单独的参考模型。

- **样本构建**：利用用户的 **“点赞 vs 重绘”** 数据。
- **优化目标**：让模型倾向于生成那些“一次通过率高”的 Prompt 结构。

## 五、 推理时的艺术：Min-P 采样

单纯调整 `temperature` 已经不够了。对于创意扩写，推荐使用 **Min-P (Minimum Probability)** 采样。

- **Min-P 机制**：它会根据当前 token 的最高概率动态调整截断阈值。
  - 在扩写确定性词汇（如主体）时，它很收敛，保证不跑题。
  - 在扩写形容词（如光影）时，它允许更多样性，提供惊喜感。
- **产品设计**：将 UI 上的“创意度”滑块直接映射到 **Min-P** 值（0.05 ~ 0.1）。

## 六、 总结

不要再迷信“提示词工程”了。在 2025 年，Prompt 扩写已经变成了一个**严谨的算法工程问题**：

1.  **数据**：用 VLM 逆向生成高质量语料。
2.  **模型**：微调 Qwen-2.5 或 Llama-3，输出结构化 JSON。
3.  **对齐**：用 ORPO 和用户行为数据做偏好对齐。
4.  **推理**：用 Min-P 采样平衡创意与准确性。

这才是构建“懂你”的 AI 产品的必经之路。

# 海量私有文档（PDF/Word/Excel）解析流水线：一份可落地的工程手册

> **一句话**：把解析当成一套“可运营”的数据工程系统，而不是一次性格式转换。核心是：先分诊再处理、先清洗再入库、同时保留可读中间态与结构化中间态、全链路可追溯可回放。

---

## 1. 先把问题说清楚：为什么“直接用 Loader”会翻车

面对几百万份异构文档（PDF、Word、Excel），直接调用 `Unstructured` 或 `LangChain` Loader，常见的工程后果是：

- **质量失控**：扫描件读不出字、双栏顺序错乱、表格被打散成一堆数字、乱码直接污染向量库。
- **成本爆炸**：对所有文档一视同仁地 OCR/VLM，GPU 被长尾任务拖死，整体吞吐不可控。
- **事实冲突**：Word 的修订、批注、页眉页脚混入正文，导致 RAG 检索命中“过期内容”或自相矛盾的证据。

这三类问题的共同点：你缺的不是“更强的模型”，而是**可控的流水线**。

---

## 2. 总体架构：从原文件到可检索知识的全链路

把系统拆成 5 个层次，任何一层都可以独立升级（版本化）并做回归评测：

1. **Ingest**：接入与确权（doc_id、source_uri、ACL、hash）（本文略）
2. **Route**：分诊（Fast/Slow）
3. **Parse & Clean**：格式专科处理与清洗（PDF/Office）
4. **Normalize**：统一中间态（Markdown + 结构化 JSONL/Parquet）
5. **Evaluate & Monitor**：质量监控与闭环（离线/在线评测）

---

## 3. Route：智能分诊（Router）决定成本与质量上限

分诊的目标不是“判断对错”，而是把绝大多数文档送进廉价的 Fast Track，把疑难杂症送进 Slow Track，并把风险显式记录到 `warnings`。

### 3.1 分诊输入与输出

- **输入**：建议只看第一页（或前 N 页抽样）
- **输出**：`FAST` / `SLOW`，并携带探针数据（coverage/unknown ratio/…）

### 3.2 三类探针（Probes）

- **字符数检测**：`len(text.strip()) < 50` -> 高概率扫描/提取失败
- **图片覆盖率**：`Image Area / Page Area > 80%` -> 基本可以判定扫描件
- **乱码检测**：
  - **Unicode Range**：统计 `U+FFFD` () 等未知字符比例，`Unknown Char Ratio > 20%` -> 编码表丢失，强制走 OCR
  - **Perplexity（进阶）**：用小型 NLP 模型测“像不像人话”，用于兜住非 U+FFFD 的乱码/符号噪声

### 3.3 两条处理通道

- **Fast Track（CPU）**：纯文本提取、基础结构恢复
- **Slow Track（GPU）**：OCR / VLM 视觉识别、复杂版面/表格识别（或使用端到端多模态模型直接提取结构化数据）

---

## 4. Parse & Clean：格式专科处理（PDF, Word, Excel）与“事实清洗”

### 4.1 PDF：文本、图片与版面还原

- **文本层**：推荐 `PyMuPDF`（快、可拿到坐标/字体/图片信息）。
- **图片层**：提取高分辨率图片，利用 VLM 生成 Image Caption。
- **表格/版面检测**：使用视觉模型锁定表格边界，避免将其误读为散文（提取后的表格处理参考 Excel 专科）。

### 4.2 Excel：表格结构化与“扁平化”

表格处理的核心原则是**“自包含” (Self-contained)**，无论是原生 Excel 还是 PDF 提取出的表格：

- **Header Re-injection（表头回注）**：任何一个表格切片必须携带表头，否则切分后变成“裸数字”。
- **Flattening（扁平化）**：多级表头对人友好，对系统不友好，建议压成一维列名或 KV（如 `2023-Q1-Revenue: 100`）。
  - **规则**：多行表头拼接成一维列名（如 `2023年-上半年-营收`）；单位/币种/时间范围提升为显式字段；确保任意表格切片必须“自包含”（至少包含表头、单位、时间范围、来源页码）。

### 4.3 Word：修订、批注与排版噪声

Word 最大的问题不是解析，而是“噪声=事实冲突”：

- **接受所有修订（Accept All Revisions）**：若合同从“~~100 万~~”改成“10 万”，不接受修订可能读成“100 万 10 万”，形成严重的证据冲突。
- **移除批注**：批注属于讨论过程，不是最终事实。
- **去重页眉页脚**：如“机密文件”每页重复，会在检索中占权重，挤掉正文证据。

---

## 5. Normalize：中间态不是只有 Markdown（Data Contract）

Markdown 是“可读中间站”，但工程系统里不建议作为唯一中间态。更稳妥的做法：**Markdown + 结构化中间结果（JSONL/Parquet）并行产出**，用于回放、追溯、增量更新与质量统计。

### 5.1 统一输出 Schema（建议最少包含）

- `doc_id`：内部唯一 ID（与业务系统/权限系统对齐）
- `source_uri`：原文件位置（对象存储路径或文件系统路径）
- `source_hash`：原文件内容哈希（用于幂等与增量）
- `parse_version`：解析器版本（含路由、OCR、表格策略版本）
- `page_index`：页码（PDF/图片流）
- `blocks[]`：块列表（段落、标题、表格、图片、脚注等）
  - `block_type`：text/table/image/header/footer/footnote/…
  - `text`：清洗后的文本
  - `bbox`：坐标（建议保留，支持顺序修复、回显、审计）
  - `table`：表格结构（建议同时存 `cells` 与 `markdown/html`）
  - `warnings`：探针命中的告警（乱码率、疑似扫描、表格破损等）
  - `timings`：耗时与关键阶段指标（用于性能剖析与容量规划）

### 5.2 幂等、增量与版本化（海量数据的生存线）

- **幂等**：同一份输入（`source_hash` + 参数）跑多少次都一致；失败重试不产生重复数据。
- **增量**：只重跑“受影响的部分”（换 OCR 模型/表格策略）。
- **版本化三件套**：
  - `parse_version`：解析输出变了就升版本

---

## 6. Evaluate：把“质量”做成可运营体系

### 6.1 无参考评估（Reference-free Heuristics）

没有全文标准答案时，先用启发式规则筛“明显坏掉”的样本：

- **结构破损**：Markdown 表格列数不一致
- **密度异常**：标题滥用（`#` 过多）或超长行（换行符丢失）
- **关键信息召回**：提取 PDF 目录页，检查正文是否包含目录标题

### 6.2 指标体系（Metrics）

- **解析层**：失败率、重试率、平均耗时、P95/P99、OCR 占比、乱码率、扫描件比例
- **结构层**：标题层级异常率、表格破损率、页眉页脚重复率、顺序修复触发率
- **覆盖层**：目录标题召回率、关键章节缺失率（按文档类型分桶）

### 6.3 金标集（Golden Set）与回归

- 建小型代表性集合（几十到几百份）：覆盖扫描/电子、双栏/单栏、复杂表格、图文混排、不同来源系统导出
- 每次升级策略/模型：对金标集全量跑，输出对比报告，作为上线门槛

### 6.4 指标口径（避免“统计口径漂移”）

| 指标       | 口径（示例）                                       | 粒度   | 建议告警                     |
| :--------- | :------------------------------------------------- | :----- | :--------------------------- |
| 解析失败率 | `failed_docs / total_docs`（已知不可解析类型另算） | 文档级 | > 0.5%                       |
| OCR 占比   | `ocr_pages / total_pages`                          | 页级   | 日波动 > 20%（可能路由坏了） |
| 乱码率     | `unknown_char_ratio > 0.2` 的页占比                | 页级   | > 1%                         |
| 表格破损率 | 表格中“列数不一致行”的表占比                       | 表级   | > 2%                         |
| 标题异常率 | 标题行比例 > 阈值 或 层级跳跃异常                  | 文档级 | > 5%                         |
| 目录召回率 | `matched_toc_headings / total_toc_headings`        | 文档级 | < 95%                        |

### 6.5 抽样复核（Spot Check）与高风险加权

- 分层抽样：按文档类型、来源系统、扫描/电子、OCR/非 OCR 分桶
- 风险加权：对命中告警探针的文档提高抽样比例
- 输出结构化结论：原因分类 + 建议修复路径，回流到规则/模型与金标集

---

## 7. 工具选择与避坑（Toolbox）

### 7.1 工具推荐表

| 场景          | 推荐工具                  | 语言       | 评价 / 避坑                                      |
| :------------ | :------------------------ | :--------- | :----------------------------------------------- |
| 综合 PDF 解析 | Docling / MinerU / Marker | Python     | **强推**：端到端转 Markdown，整合 OCR 与版面分析 |
| PDF 文本提取  | PyMuPDF (fitz)            | Python     | 速度快、功能全（适合 Fast Track）                |
| PDF 表格提取  | pdfplumber                | Python     | 表格调参方便，速度相对慢                         |
| OCR（中文）   | PaddleOCR / RapidOCR      | Python/C++ | 中文强，RapidOCR 更轻更快                        |
| 版面分析      | Surya                     | Python     | 适合段落/标题/图片区块识别                       |
| Word (.docx)  | python-docx               | Python     | 结构稳健                                         |
| Excel (.xlsx) | pandas                    | Python     | 合并单元格需自行 `ffill` 处理                    |
| 兜底          | GPT-4o / Qwen-VL          | API        | 强但贵，只用于疑难 1%                            |

### 7.2 避坑清单

1. LangChain `PyPDFLoader`：基础封装，难以处理乱码与表格，生产不建议直接用
2. Unstructured：功能强但依赖复杂，建议探索阶段用，生产谨慎引入

---

## 8. 收尾：这套流水线解决了什么

这套结构化设计的核心不是“选哪一个工具”，而是让你在海量数据压力下仍能做到：

- 成本可控：Fast/Slow 分流 + 缓存 + 背压
- 质量可控：启发式筛坏样本 + 指标体系 + 金标回归 + 抽样复核
- 迭代可控：幂等/增量/版本化，随时可以定向重跑与回放
- 生产可控：可引用、可审计、可删除、可兜底

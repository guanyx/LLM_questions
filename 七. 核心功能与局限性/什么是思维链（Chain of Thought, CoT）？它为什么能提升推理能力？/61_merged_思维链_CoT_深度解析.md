# CoT（思维链）：给模型一张草稿纸

我喜欢用一句很朴素的话来解释 CoT（Chain of Thought）：**让模型把草稿写出来。**

你在小学做应用题时，老师大概率说过类似的话：

“不要只写答案，把过程写清楚。”

思维链做的事情差不多：在输出最终答案之前，先把中间步骤（草稿）也输出出来。

这听起来像是提示词技巧，但它背后其实有一套挺“工程化”的原因：**多写几行字=多跑几步计算**。

下面我按“从直觉到工程落地再到未来走向”的顺序，把它讲清楚。

---

## 1）CoT 让模型从“快答”切到“慢想”

很多时候模型像在玩抢答：看到题目，立刻吐出一个看起来很像正确答案的东西。

这很像卡尼曼说的：

- 系统 1：快、直觉、反射式
- 系统 2：慢、需要注意力、一步步推

CoT 的核心效果是：**把模型从系统 1 叫到系统 2。**

你可能见过两种典型用法：

- 零样本：在问题后面加一句“请一步步思考”
- 少样本：先给几个“题目 + 详细过程 + 答案”的例子，让它模仿这种写法

这不是魔法，只是让模型进入一种更“愿意写草稿”的输出模式。

如果你想快速体会一下差异，可以做个小对比（同一道题，换两种问法）：

```text
问法 A：直接给我答案。
问法 B：请写出推理步骤，再给答案。
```

---

## 2）为什么写草稿会更会推理？一个很实用的视角：用时间换算力

Transformer 有个很重要的现实：**模型层数是固定的。**

不管你问的是：

- “1+1 等于几？”
- “这道题要分 7 步推理才能做出来”

它每生成一个 token，走的网络深度都差不多。

所以，如果你强行要求它“直接给最终答案”，其实是在要求它用很少的计算步数完成很复杂的映射。

CoT 的思路是：

- “那我多生成一些 token”
- “每生成一个 token，就多进行一次前向计算”

于是复杂任务就更容易被拆成小段做完。

一个粗糙但有用的图：

```text
没有 CoT：题目 ---------------> 答案
有 CoT： 题目 -> 步骤1 -> 步骤2 -> ... -> 答案
```

从机器学习角度看，上面第二条路径更“好学”：每一步的跳跃更小，更容易高概率地走对。

### 不是所有模型都吃这一套

一个容易被忽略的点是：CoT 往往更像“涌现能力”。

- 大模型：写草稿经常能显著提升效果
- 小模型：写草稿反而可能更乱（草稿写着写着跑偏，把自己带沟里）

---

## 3）CoT 在模型里到底“增加”了什么？（草稿本效应）

我喜欢把 CoT 看成给模型发了一本草稿本。

Transformer 的注意力机制可以“看见”已经生成的 token。也就是说：

- 你刚才写下的步骤，会变成后续步骤的上下文
- 后面的生成可以回头引用前面的中间结论

这有点像：你算 `123 * 456` 的时候，写下 `123*6=738`，后面再继续，不需要全靠脑内短期记忆。

工程上这也意味着：

- **“过程 token”不是废话，它们是可被注意力再次读取的中间状态**

这也是为什么 CoT 往往能显著提升算术、多步逻辑、规划类任务的表现。

---

## 4）把 CoT 放到生产里：别让所有问题都“写作文”

CoT 的副作用也很直接：

- token 多了 = 成本上升
- 生成长了 = 延迟变高

所以实际系统一般不会“全量开启 CoT”，而是做分层。

### 动态 CoT：先判断题目需不需要“慢想”

一种常见做法是加一个 Router（路由/分流器）：

- 简单问题：直接短答、或走检索/RAG
- 复杂问题：再走 CoT（甚至多次采样）

这样能显著降低平均成本和平均延迟。

### UI/体验：慢是慢，但别让用户干等

两招很有效：

- 流式输出（Streaming）
- 折叠过程：默认只展示结论，过程点开可看（很多产品会显示 “Thinking…”）

它解决的是“用户心理延迟”，不改变真实算力，但能改善体验。

### 蒸馏：把“慢想”尽量变回“快答”

如果你的目标是“线上要快、但效果也别掉太多”，常见路线是蒸馏：

1. 让强模型用 CoT 生成“题目-草稿-答案”
2. 用这些数据去微调一个更小的模型
3. 线上用小模型短答（必要时再回退到大模型慢想）

---

## 5）CoT 的大坑：事后诸葛亮（看起来很合理，但可能是编的）

一个很常见、也很危险的现象：

- 答案对了，但过程是错的
- 或者过程逻辑很顺，但最后答案错了

这说明：**“可读的推理文本”并不等于“真实的内部因果推理”。**

工程上怎么对付？我会优先考虑三类手段。

### A. 过程监督：别只奖励答案，也奖励每一步

传统 RLHF 更像“只看最终答案给分”。

过程监督（PRM 的思路）是：

- 对中间步骤逐步打分
- 让模型学会“每一步都别乱来”

它成本更高，但在高风险场景（金融、医疗）更有价值。

### B. 工具验证：需要算数/查事实时，别让模型硬编

一个简单原则：

- 数学让解释器算
- 事实让检索/数据库给

让模型负责“提出计算/查询的方式”，而不是靠权重去胡乱凑结果。

### C. 自洽投票：同一题多走几条路，看答案是否稳定

做法是让模型用不同随机性生成多条解法：

- 如果结论一致：可信度上升
- 如果结论分裂：说明它自己也不确定，系统就该降级（比如让它追问信息、或者人工接管）

缺点也很明显：成本按采样次数线性增加。

### 如果你还想更稳一点：从“链”到“树”

一条思维链最大的问题是：第一步走错了，后面越写越离谱。

更“工程味”的做法是把搜索结构变粗一点：

- 自洽（Self-Consistency）：多采样几条链，投票选多数
- 思维树（Tree of Thoughts）：每一步展开多个候选，评估、回溯，再继续向前走

---

## 6）垂直领域做 CoT 微调：关键不是“多”，而是“干净”

在医疗/法律这类垂直领域，很多团队会发现：

- 只用 (Q, A) 微调，模型能背答案，但不一定会推

更靠谱的训练目标是：

- (Q, Rationale, A)

也就是把“草稿”也当作学习目标。

### 数据怎么来？最常见的是蒸馏流水线

一个很实用的流程：

1. 收集业务真实问题 + 标准答案
2. 让更强的模型补出推理过程（按你希望的格式写）
3. 再让它自检一遍：哪里跳步、哪里引用了不存在的事实

### 最大的毒性：答案对但过程错

这类样本会教会模型一种坏习惯：

- “先蒙一个结论，再把话圆回来”

清洗思路通常是：

- 数学/代码：规则校验每一步
- 复杂推理：多路生成，一致性过滤
- 或者用专门的打分器评估“过程是否支持结论”

宁可少一点，也别把“逻辑毒药”喂进去。

---

## 7）CoT 会消失吗？可能会：从显式文字到隐式推理

显式 CoT 有个尴尬点：

- 为了思考，必须输出人类可读的文本
- 文本很贵（token、带宽、延迟）

所以一种自然的演进方向是：

- 推理发生在潜空间（latent）里
- 外部只看到最后答案（或者一段“翻译过”的解释）

这也是很多人谈 System 2 Attention、以及某些强化学习式推理模型时的直觉：

- 模型可能会学会一种“只有自己懂”的高效推理表示
- 人类看到的解释，可能变成二次翻译

你可以把它想成两种未来：

- 模型内部用“更省 token 的语言”思考（人类看不懂）
- 对外再输出一份“翻译版解释”

这也解释了为什么一些新模型会更强调“思考时间/推理预算”，而不只是输出 token 数。

---

## 8）最后一个更尖的问题：如果我们看不懂它的推理，还能信任吗？

CoT 在多智能体系统里有一个很现实的价值：

- 它可以当作 agent 之间的“交接单”
- A 走到一半把草稿交给 B，B 接着推
- 甚至可以互相审稿、找漏洞

但与此同时，如果模型为了效率把推理压缩成“人类看不懂的格式”，就会出现可解释性危机：

- 你看不懂
- 你审不了
- 你也不知道它有没有省略关键前提

一种可能的妥协是所谓“可读性税”：

- 允许模型内部用高效方式思考
- 但强制它输出一份人类可审查的解释（哪怕会慢一点、弱一点）

在高风险系统里，这笔税大概率得交。

更现实的架构想象是“两层系统”：

- 内部：高效推理（可以很不可读）
- 外部：解释翻译器（把内部推理投影到人类语言）

如果你要监管的东西真的变成了这个结构，那么监管重点会从“审模型”转向“审翻译器有没有漏翻、乱翻”。

---

## 结尾：把 CoT 当作一套“系统设计”，而不是一句提示词

如果你只记住一件事，我希望是这个：

- CoT 的价值不止是“让模型多写几句”
- 它是一种把复杂问题拆成小步、并把中间状态写进上下文的计算方式

而当你把它用到生产里，它会自动变成一套系统：

- 路由（哪些问题值得慢想）
- 工具（哪些步骤必须被验证）
- 评估与降级（不确定时怎么办）
- 数据治理（别喂逻辑毒药）

这样用，CoT 才会既“更会推理”，也“更像一个靠谱的产品组件”。

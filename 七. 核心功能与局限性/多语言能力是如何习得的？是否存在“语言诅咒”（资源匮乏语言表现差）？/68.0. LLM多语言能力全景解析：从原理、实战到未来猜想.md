# LLM 怎么学会这么多语言？（以及为什么小语种更费钱）

我第一次用 LLM 的时候，有个非常直觉的猜测：

“它是不是先把中文翻译成英文，再在英文里思考，然后再翻译回中文？”

后来发现：大多数时候不是。

更像是——它脑子里有一个“概念地图”，不同语言只是贴在同一块地图上的不同标签。

这篇文章想聊 5 件事：

1. 它到底怎么把不同语言“对齐”到一起的
2. 为什么多语言有时候会变成“语言诅咒”
3. 为什么你用小语种，会觉得模型更慢、更笨、还更贵
4. 工程上怎么修：词表扩充、Embedding 初始化、数据配比
5. 未来有没有可能不需要 Tokenizer

---

## 1) 它不是在“翻译”，更像是在用同一套概念说话

先做个小脑内实验：

你在一句话里混着写“苹果 / Apple / Apfel”，人类读起来可能别扭，但基本能懂。

对 LLM 来说，这种“别扭”反而很常见——它的内部表示（向量）天然就倾向于把这些词挤到同一片区域里。

你可以把它想象成一个超大的多维空间：

- “苹果”有个坐标
- “Apple”也有个坐标
- “Apfel”也有个坐标

训练久了之后，这三个坐标会靠得很近。

它学到的不是“Apple=苹果”这种词典表格，而更像是：它学到了“🍎 这个概念”，然后顺手知道了不同语言怎么指它。

---

## 2) 为什么代码经常像“多语言的粘合剂”

这里有个很有意思的“副作用”：代码经常帮了大忙。

原因很朴素：

- 你会在英文教程里看到 `print()`，旁边是英文解释
- 你也会在中文教程里看到 `print()`，旁边是中文解释
- 但 `print()` 的逻辑结构基本是固定的

于是模型在海量数据里会反复遇到同一段逻辑，外面包着不同语言的壳。

如果你喜欢比喻：代码像一个特别结实的挂钩（anchor），把不同语言的解释挂在同一个“逻辑骨架”上。

这也顺便解释了一个常见观感：代码能力强的模型，逻辑感和多语言通常也不差。

---

## 3) “零样本迁移”听起来像魔法，其实是“技能复用”

你可能见过这种现象：

你只教模型用英语写摘要，但它也能用德语写摘要（哪怕没专门训练过“德语摘要”）。

这听起来像魔法，但如果换个说法就很朴素：

模型学会的是“摘要这件事”——提取主旨、压缩表达、保持结构。

一旦技能是抽象的，它就可以借助共享的概念空间，把这套技能搬到别的语言上。

这就是大家常说的零样本跨语言迁移（zero-shot cross-lingual transfer）。

---

## 4) 多语言的代价：为什么会有“语言诅咒”

到这里，问题来了：既然它能学很多语言，那是不是语言越多越好？

不一定。

模型的容量是有限的（参数量有限）。

如果你拿一个小模型，硬塞十几种语言进去，很可能发生一种“资源争夺”：

- 它在 A 语言上更流利了
- 但 B 语言上开始磕巴
- 有时候甚至连原本擅长的英语都变差

这就是所谓的 “The Curse of Multilinguality”。

但也有另一面：

当模型足够大，语言之间开始出现正向协同（尤其是同语系、共享词根、共享结构的时候）。

这时候“诅咒”会变得不那么可怕，甚至会变成“额外收益”。

---

## 5) 你觉得小语种更慢、更贵、更笨：很多时候是 Tokenizer 的锅

这一段是我觉得最“工程味”的部分，也是很多体验差异的根源。

Tokenizer 是模型读文本的入口：它把字符串切成 token。

问题是：很多主流 tokenizer 的词表、合并规则，主要是在英语语料上长大的。

于是它对英语特别友好：

- 英语里一个长单词，可能就是 1 个 token

但对一些语言就很不友好：

- 它没见过、或者没法优雅切分
- 于是把词拆成一堆碎片（甚至更接近字节级的碎片）

这会带来三件非常具体的“坏事”：

1. **更贵**：API 常按 token 计费。同样一句话，小语种 token 数可能是英语的几倍
2. **更短**：上下文窗口是按 token 计的。token 越多，能装进窗口的“真实内容”越少
3. **更难推理**：模型要在更多碎片上分注意力，复杂推理时更容易“脑力被稀释”

如果你曾经觉得“我用某个语言，它突然不聪明了”，这往往不是错觉。

---

## 6) 工程怎么修（1）：词表扩充 + 新 token 的 embedding 初始化

一个直接的想法是：既然 tokenizer 对小语种切得太碎，那我就扩充词表，把常见词变成更完整的 token。

听起来简单，但立刻会遇到一个非常具体的问题：

“新加的 token，在 embedding 矩阵里那一行要填什么？”

### 6.1 为什么别用随机初始化

embedding 空间不是白板。

大模型预训练久了之后，embedding 里已经有很细腻的几何结构。

你突然往里面扔几千个随机向量，相当于往精密仪器里撒一把沙子：

- 训练初期会出现很怪的梯度
- 收敛会变慢
- 极端情况下会伤到旧能力

### 6.2 一个很常用的“热启动”：子词均值

做法很朴素：

1. 用旧 tokenizer 把这个新词切碎（得到一串旧 token）
2. 取这些旧 token 的 embedding 向量
3. 求平均，把均值当作新 token 的初始 embedding

它不完美，但至少让新 token 一开始就在“差不多对的区域”，而不是满天乱飞。

### 6.3 更稳一点：两阶段训练

如果你非常担心新 token 影响旧能力，可以分两步走：

1. **先冻住主干网络**（attention/ffn 不动），只训练 embedding（以及输出端）
2. loss 稳下来之后，再解冻全模型做常规微调

这种方式很像：先把新同学安排好座位，再开始一起上课。

### 6.4 两个很容易踩的坑

- **输入/输出要一起 resize**：很多架构会共享 embedding 和输出头（或至少强相关）。扩充 vocab 时两边要同步
- **别动 special tokens 的 id**：新 token 应该追加在末尾，不要打乱旧 token 的编号

---

## 7) 工程怎么修（2）：数据配比——别把英语全扔了

另一个经常被问的问题是：

“我要做中文模型，能不能把英语数据全扔了，只训练中文？”

经验上：非常危险。

原因不是“英语更高级”这种叙事，而是更现实的东西：

- 英语高质量技术文本、学术、代码、结构化论证太多了
- 它在训练里常常充当“逻辑骨架”

一些常见的经验区间（不是定律，是比较常见的工程实践）：

- 预训练做通用多语言：英语常见在 40%–60%，代码 10%–20%，剩下给目标语言
- 即使做偏中文：也常会留一部分高质量英语 + 代码（比如 20% 左右），用来“保智商”
- SFT 阶段：混一点高质量英文指令（10%–20%）能减轻灾难性遗忘

如果目标语言数据很少，还会用上采样：

温度采样（temperature sampling）常见的直觉是：让小语种“被看到”的概率上来，但别把英语完全稀释掉。

---

## 8) 多语言的安全小坑：越狱为什么经常从“小语种侧门”进来

多语言还有个更“阴暗”的副作用：多语言越狱。

粗略来说是这样：

- 安全对齐（比如 RLHF、安全微调）的大量负样本是英语
- 模型学会了：英语里某些危险意图要拒绝
- 但同样意图换成低资源语言时，约束可能没学到（或学得更弱）

再加上 tokenizer 对低资源语言切得更碎：

英语里一个敏感词可能是一个完整 token，很好抓

小语种里可能被切成碎片，过滤器更难识别

一些常见的推理侧防御思路（不需要把所有语言重训一遍）：

- **翻译-检测**：先把输入翻译成英语，再用成熟的英文安全分类器判断
- **向量探针**：在 embedding 层训练轻量 probe，直接检测“危险意图”的向量方向
- **自我审查提示**：在 system prompt 里要求模型先内部复述/检查再回答

---

## 9) 未来：也许我们会“抛弃 tokenizer”

最后聊个更脑洞的方向：如果 tokenizer 是这么多问题的源头，那能不能不要它？

两个常见路线：

1. **字节级模型（byte-based）**：直接读写 bytes，天然语言无关，也就没了 OOV 和“分词歧视”
2. **把文字当图像**：把文本渲染成图片喂给视觉模型，让模型从像素里读

更远一点的猜想是：未来可能出现“世界模型”：

- 内部是连续的概念状态和动态系统（没有中文/英文）
- 需要输出时，才通过一个接口层把概念“翻译”成人类语言

如果真走到那一步，多语言就不再是一项额外技能，而会像呼吸一样自然。

---

## 结语

我喜欢把现在的多语言能力理解成一种“拼装的巴别塔”：

- 共享概念空间让不同语言能互相借力
- 代码像把这些语言钉在同一套逻辑上
- tokenizer 和数据配比决定了体验差异，甚至决定了你觉得它“聪不聪明”

而我们真正想要的，也许不是更复杂的转接器，而是某种“无线充电”式的智能：不被语言形状限制，直接在概念与逻辑上流动。

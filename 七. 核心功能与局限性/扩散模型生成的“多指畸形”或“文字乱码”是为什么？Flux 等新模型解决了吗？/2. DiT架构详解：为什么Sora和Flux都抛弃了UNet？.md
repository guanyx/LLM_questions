# DiT 架构详解：为什么 Sora 和 Flux 都抛弃了 UNet？

在 2024 年之前，如果你问 AI 绘画的核心是什么，答案毫无疑问是 **UNet**。从 Stable Diffusion 1.5 到 SDXL，这只“U 型怪兽”统治了开源界整整两年。

但到了 2025 年，风向彻底改变。
OpenAI 的 **Sora**、Black Forest Labs 的 **Flux**、腾讯的 **HunyuanVideo** 以及 Stability AI 的 **Stable Diffusion 3.5 (SD3.5)**，全部不约而同地抛弃了 UNet，转而拥抱了 **DiT (Diffusion Transformer)**。

这是一场什么样的架构革命？为什么说 DiT 才是生成式 AI 的未来？

---

## 一、 UNet 的“近视眼”困境

要理解 DiT 的好，先得知道 UNet 差在哪。

UNet 的结构像一个字母 "U"。它的工作流程是：

1.  **下采样（压缩）**：把一张大图不断压缩，提取特征，直到变成很小的一块“精华”。
2.  **上采样（还原）**：再把这块精华慢慢放大，还原成大图，中间通过“跳跃连接”把丢失的细节补回来。

### 这种设计的优缺点非常明显：

- **优点（局部大师）**：它用的是卷积层（CNN），卷积核像一个小窗口在图片上滑来滑去。它非常擅长处理**相邻像素**的关系。所以早期的 AI 画皮肤质感、衣服纹理简直一绝。
- **缺点（全局盲人）**：它很难“看”到离得远的像素。左上角的像素想和右下角的像素“打招呼”，需要经过层层压缩再解压，信息在传递过程中很容易丢失。

这就是为什么 SD1.5 经常画出**“多头人”、“六指琴魔”**或者**“错乱的空间结构”**（比如一个人坐在椅子上，但腿穿过了椅子）。因为它缺乏**全局注意力**，它在画手指的时候，忘了刚画过手掌；在画腿的时候，忘了椅子的位置。

---

## 二、 DiT：把图像切成“方块”的 Transformer

DiT 的全称是 **Diffusion Transformer**。它的核心思想简单而暴力：**既然 Transformer 在自然语言处理（NLP）领域能“理解”长篇大论的上下文，那为什么不能用它来“理解”整张图片的全局结构？**

### 1. Patchify：把画变成“字”

Transformer 原本是处理文字的（Token）。DiT 的第一步，是把一张图片切成无数个小方块（Patches）。
比如一张 512x512 的图，切成 64x64 个小方块。在 AI 眼里，这些小方块就变成了一个个“单词”。

### 2. Global Attention：上帝视角

一旦图片变成了“单词序列”，Transformer 的看家本领——**自注意力机制（Self-Attention）**——就派上用场了。

在 DiT 的每一层运算中，**每一个小方块都能“看见”其他所有的小方块**，不管它们在图片上隔得有多远。

- 当它处理“手指”这个方块时，它能直接查询到“肩膀”、“手腕”甚至“另一只手”的状态。
- 这就像一个拥有**上帝视角**的画师，下笔前已经对整幅画的构图了然于胸。

### 3. Scaling Law：大力出奇迹

UNet 有一个致命弱点：**上限低**。当参数量增加到一定程度（比如 20 亿以后），性能提升就开始变慢，甚至变差。

而 Transformer 也就是 DiT，完美继承了 GPT 的 **Scaling Law（缩放定律）**。

- **模型越大，效果越好**：从 SD3 的 20 亿参数，到 Flux 的 120 亿参数，再到 HunyuanVideo 的数百亿参数，DiT 的性能随着规模增加而持续飙升。
- **数据越多，效果越好**：只要喂给它足够的高质量数据，它的理解能力和生成质量就能线性增长。

这也是为什么 2025 年的旗舰模型（如 HunyuanVideo）能生成长达 10 秒以上、物理规律准确、光影完美的视频，而基于 UNet 的老模型往往只有几秒钟就会崩坏。

---

## 三、 进阶：SD3 的 MM-DiT (多模态 DiT)

Stable Diffusion 3 在 DiT 的基础上更进一步，搞出了 **MM-DiT (Multimodal Diffusion Transformer)**。

在旧模型里，文本（提示词）和图像是分开处理的，最后通过一个“桥梁”（Cross-Attention）硬塞在一起。这导致 AI 经常听不懂复杂的指令。

MM-DiT 的做法是：**把文本 Token 和图像 Patch 扔进同一个大锅里煮。**

- 图像和文本在同一个 Transformer 序列中流动。
- 文本可以看图像，图像可以看文本。
  这种**双向的信息交融**，让 SD3 对提示词的理解能力达到了前所未有的高度。比如你让它画“一个红色的立方体在蓝色的球体上面”，它绝不会搞反颜色和位置。

---

## 四、 总结

从 UNet 到 DiT，是 AI 绘画从**“修图匠”**向**“架构师”**的进化。

- **UNet** 像是拿着放大镜一点点抠细节，虽然细腻但容易迷失方向。
- **DiT** 先在大脑里构建完整的 3D 世界观，解决了结构和逻辑问题。

这就是为什么 **Flux** 能画出逻辑严密的手指，**HunyuanVideo** 能生成长达 10 秒以上的电影级视频。这不仅仅是换了个模型，这是 AI 对视觉世界理解方式的一次降维打击。

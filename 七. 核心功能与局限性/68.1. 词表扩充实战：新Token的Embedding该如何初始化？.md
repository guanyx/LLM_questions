# 68.1. 词表扩充实战：新 Token 的 Embedding 该如何初始化？

在上一篇关于多语言能力的探讨中，我们提到了**“词表扩充（Vocabulary Expansion）”**是解决小语种“分词效率低、费用高、推理慢”的必杀技。

但当你真正动手修改 `tokenizer.json`，往里面塞进 5000 个泰语或越南语的新 Token 后，一个棘手的工程问题立刻摆在面前：

**“这些新加进来的 Token，它们在模型底层的 Embedding 向量该怎么填？”**

如果填全是 0，模型会死机；如果随机生成（Random Initialization），它们就是一堆毫无意义的噪声。直接拿这些噪声去训练，不仅收敛慢，甚至可能像“老鼠屎坏了一锅粥”，把模型原本完美的语义空间搅得稀烂。

本文将从工程视角，手把手教你如何优雅地完成这次“换血”手术。

---

## 一、 为什么不能“随机初始化”？

为了理解初始化的重要性，我们需要重温**“共享语义空间”**的概念。

### 1. 语义空间的“精密仪器”
经过万亿 Token 预训练的 LLM，其 Embedding 层不再是杂乱的数字，而是一个高度精密的拓扑结构。
- 英文的 "King" 和 "Queen" 之间的距离向量，与 "Man" 和 "Woman" 之间的距离向量是平行的。
- 这种几何关系代表了逻辑。

### 2. 随机初始化的破坏力
如果你往这个精密空间里扔进 5000 个随机向量（新 Token）：
- **初始状态**：这些新词分布在空间的各个角落，毫无逻辑。比如新加入的泰语词“你好”，可能被随机扔到了“杀虫剂”和“发动机”的中间。
- **训练后果**：在微调初期，模型为了强行纠正这些离谱的错误，会产生巨大的梯度（Gradient）。这些剧烈的梯度回传到模型深层，可能会震荡甚至破坏原有的、脆弱的权重分布。这就好比为了教会一个插班生，把整个班级的教学大纲都打乱了。

我们需要的是一种**“热启动（Warm Start）”**策略，让新 Token 一出生就“大概其”在正确的位置上。

---

## 二、 策略一：子词均值法 (Sub-word Averaging)

这是目前业界最常用、性价比最高的“热启动”方案。

### 1. 原理：用旧砖盖新房
虽然模型没见过完整的泰语词“สวัสดี”（你好），但原来的词表里肯定包含了一些基础的子词、字节甚至 Unicode 字符。
原来的 Tokenizer 会把这个词切得稀碎，比如切成 `[0xE0, 0xB8, 0xAA, ...]` 等一堆碎片。

虽然碎片很碎，但模型对这些碎片是有 Embedding 记录的。

### 2. 操作步骤
当我们把完整的“สวัสดี”作为一个新 Token 加入时：
1.  **回溯**：先用**旧的 Tokenizer** 把这个新词切分，看看它由哪些旧 Token 组成。
2.  **继承**：把这些旧 Token 对应的 Embedding 向量取出来。
3.  **融合**：计算这些向量的**平均值**，作为新 Token 的初始化向量。

### 3. 效果
这样一来，新词“สวัสดี”的初始位置，就是它所有组成部分的“重心”。虽然不完全准确，但它至少保留了部分语音或字形的特征，比起满天乱飞的随机向量，它离“正确答案”近了十万八千里。

---

## 三、 策略二：两阶段训练法 (Two-Stage Training)

如果你追求极致的稳定性，不想让新词干扰旧知识，可以采用“先外围、后核心”的战术。

### 1. 第一阶段：冻结骨干 (Freeze Backbone)
在刚开始训练时，我们**冻结（Freeze）** Transformer 的所有层（Attention, FFN），**只训练 Embedding 层**（以及最后的输出层 lm_head）。

- **目的**：此时模型的大脑不动，只调整“眼睛”和“嘴巴”。
- **过程**：让模型快速地把新 Token 的向量推到语义空间中合理的位置。因为骨干网络被锁死了，所以无论梯度多大，都不会破坏模型原有的逻辑推理能力。

### 2. 第二阶段：全量解冻 (Full Fine-tuning)
当 Loss 下降到一个平稳的台阶，说明新 Token 已经找到了自己的“座位”。
此时，再解冻整个模型，进行常规的指令微调（SFT）。这时，新旧语言就可以开始进行深层次的语义对齐和知识融合了。

---

## 四、 进阶技巧：多语言对齐初始化

如果你的资源允许，还有一种更高级的玩法，利用**平行语料（Parallel Corpus）**。

假设你有中-泰对照字典。
- 你知道中文的“苹果”在模型里已经有一个非常精准的向量 $V_{apple}$。
- 那么，对于新加入的泰语“แอปเปิ้ล”（苹果），你可以直接把 $V_{apple}$ 复制给它，或者加一点微小的扰动。

这种方法相当于直接把新词“空降”到了语义空间的正确坐标点上。虽然需要额外的字典资源，但收敛速度是所有方法中最快的。

---

## 五、 工程避坑指南

作为工程师，在实操时还要注意以下两个细节：

### 1. 输入与输出的同步
在 LLM 架构（如 Llama, GPT）中，输入的 Embedding 层（将 Token 转为向量）和输出的 LM Head 层（将向量转回 Token 概率）通常是**参数共享（Weight Tying）**的，或者至少是紧密相关的。
当你扩充了词表（`vocab_size` 从 32000 变成 37000），记得**同时 resize** 输入矩阵和输出矩阵。很多初学者只改了头，忘了改尾，导致代码报错或训练无法收敛。

### 2. 特殊 Token 的处理
扩充词表时，千万小心不要覆盖了原有的 Special Tokens（如 `<unk>`, `<s>`, `</s>`）。
新词应该追加（Append）在原有词表的末尾，严禁打乱原有 Token ID 的顺序。因为在模型权重里，第 1024 号位置存的是谁，是绝对不能变的。

---

## 结语

给 LLM 扩充词表，绝不是简单的 `list.append()`。它是一场精细的**神经外科手术**。

通过**子词均值初始化**或**冻结训练策略**，我们能确保新加入的“外语插班生”平滑地融入集体，既学会了新语言，又没有破坏班级原有的优良学风。这才是多语言适配的正确打开方式。

# 68.2. 多语言训练的黄金配比：英语要保留多少才能维持逻辑智商？

在多语言模型（Multilingual LLM）的训练中，工程师往往面临一个灵魂拷问：

**“如果我的目标是做一个中文模型，或者是泰语模型，我能不能把英语数据全扔了，只训练目标语言？”**

直觉上，只学目标语言似乎能让模型更专注。但无数惨痛的实验教训告诉我们：**一旦把英语数据砍得太狠，模型的逻辑推理能力（Reasoning）、代码能力（Coding）甚至数学能力都会出现断崖式下跌。**

这就引出了多语言训练中最核心的玄学——**数据配比（Data Mixture）**。本文将探讨如何寻找那个维持模型智商的“黄金分割点”。

---

## 一、 为什么英语是模型的“逻辑骨架”？

在当前的互联网文本分布中，英语数据不仅数量庞大，更重要的是**质量极高**。

1.  **科学与代码的载体**：GitHub 上的高质量代码、ArXiv 上的学术论文、StackOverflow 上的技术问答，绝大多数是英语。
2.  **逻辑密度的差异**：相比于某些语言偏向情感表达或日常对话，英语语料中包含了更多严谨的论证、推导和结构化知识。

**英语不仅仅是一种语言，它在客观上充当了 LLM 的“逻辑骨架”。**
如果你抽走了这根骨架，模型可能依然能流利地用中文说车轱辘话，但一旦遇到复杂指令（如“请先分析原因，再列出三个步骤，最后总结”），它就会逻辑崩塌。

---

## 二、 寻找黄金配比：经验法则

那么，英语到底该留多少？学术界和工业界并没有一个绝对的定值，但根据 Llama 2、Baichuan、Qwen 等模型的公开报告或反推结果，我们可以总结出一些经验区间。

### 1. 预训练（Pre-training）阶段

如果你的目标是训练一个**多语言通用模型**：

*   **英语（English）**：通常建议保持在 **40% - 60%**。这是底线。低于 30%，模型的泛化能力和逻辑推理能力通常会开始受损。
*   **代码（Code）**：**10% - 20%**。代码是特殊的语言，它能显著增强模型的逻辑推理（CoT）能力。
*   **目标语言（如中文/小语种）**：剩余的 **20% - 40%**。

**注意**：即使你的目标是**纯中文模型**，依然建议保留 **20% 左右的高质量英语和代码**。这部分数据不为了教它说英语，而是为了教它“思考”。

### 2. 微调（SFT）阶段

在指令微调阶段，规则略有不同。此时模型已经具备了逻辑底座，更多是在学习“如何对话”。

*   **混合训练（Mix Training）**：为了防止灾难性遗忘，建议在 SFT 数据集中混入 **10% - 20% 的高质量英语指令**（如 Alpaca, ShareGPT 的精选子集）。
*   **效果**：这种“掺沙子”的做法，能让模型在用中文回答问题时，依然调用到底层由英语构建的逻辑链路，回答得更有条理。

---

## 三、 动态采样：解决“不患寡而患不均”

在实际训练中，如果目标语言（如泰语）的数据量远少于英语（可能只有英语的 1%），直接按自然分布训练，泰语会被英语彻底淹没，模型根本学不会。

这就需要引入**上采样（Up-sampling）**策略。

### 1. 温度采样 (Temperature Sampling)
我们通常使用带温度系数 $T$ 的采样公式来平衡数据：
$$ p_i \propto (n_i)^{1/T} $$
*   其中 $n_i$ 是第 $i$ 种语言的原始数据量。
*   当 $T=1$ 时，按原始比例采样（英语占霸主地位）。
*   当 $T \to \infty$ 时，所有语言被均匀采样（英语和泰语一样多）。

**工程经验**：通常设置 $T$ 在 **1.3 到 2.0** 之间。这既能提升低资源语言的权重，又不至于让英语数据被过度稀释。

### 2. 课程学习 (Curriculum Learning)
一种更高级的策略是**动态调整配比**：
*   **前期**：英语和代码占主导（80%），快速构建逻辑底座和世界观。
*   **中期**：逐渐增加目标语言的比例（50%），进行语言能力的迁移。
*   **后期**：以目标语言的高质量数据为主（70%），进行风格打磨和领域适配。

---

## 四、 灾难性遗忘的急救包

如果你在训练中途发现，模型虽然泰语说得越来越溜，但英语能力（或者逻辑能力）突然暴跌，这就是**灾难性遗忘（Catastrophic Forgetting）**。

除了调整数据配比，还可以尝试以下手段：

1.  **重放（Replay）**：定期把训练初期的高质量英语/代码数据拿出来“复习”一遍。
2.  **MoE 路由干预**：如果你使用的是 MoE（混合专家）架构，可以强制一部分专家（Experts）只在英语数据上更新，把它们锁定为“逻辑专家”，防止被低质量的小语种数据污染。
3.  **学习率调整**：当引入新语言数据时，适当调低学习率，或者只解冻部分参数（如 Adapters），保护核心权重。

---

## 结语

多语言训练是一场精妙的**平衡艺术**。

英语数据在其中扮演了“压舱石”的角色。无论你多么渴望打造一个纯粹的本土化模型，都请记住：**保留适量的英语和代码，不是为了崇洋媚外，而是为了守住模型的智商底线。**

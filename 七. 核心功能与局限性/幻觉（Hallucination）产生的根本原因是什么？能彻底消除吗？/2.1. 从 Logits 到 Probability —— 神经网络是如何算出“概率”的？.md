# 2.1 从 Logits 到 Probability —— 神经网络是如何算出“概率”的？

你可能已经很熟悉怎么调节“温度（Temperature）”参数了：

- 想让 AI 写诗、搞创意，就把温度调高（比如 0.8），它会变得天马行空。
- 想让 AI 写代码、做数学，就把温度调低（比如 0.1），它会变得严谨保守。

但你有没有好奇过：**模型经过几十层复杂的神经网络计算，最后吐出来的到底是什么？** 它是怎么把一堆冷冰冰的数字变成“下个词是 apple 的概率是 80%”的？

今天，我们就钻进大模型的“大脑皮层”，看看**概率诞生的前半程：从 Logits 到 Softmax**。

---

## 一、 神经网络的终点站：Logits（原始得分）

很多人以为，大模型算完之后，直接就输出了概率。
**错。**

模型经过 96 层（以 GPT-3 为例）Transformer 的疯狂矩阵运算，在最后一层（Output Layer），它吐出来的其实是一串**没有任何范围限制的数字**。

这串数字有一个专门的学名，叫 **Logits**。

### 1. 这一步发生了什么？

想象一下，模型的词表里有 50,000 个词。
在最后一层，模型会把当前的“思维向量”（Hidden State）通过一个线性层（Linear Layer），映射成一个长度为 50,000 的向量。

这个向量里的每一个数字，代表了模型对对应词的“打分”。

### 2. Logits 长什么样？

Logits 是**原始得分**，它非常狂野：

- 可以是**负数**（代表模型极其讨厌这个词）。
- 可以是**正无穷**（代表模型极其喜欢这个词）。
- **没有归一化**（加起来不等于 1）。

**举个例子：**
假设词表里只有 3 个词：`["猫", "狗", "车"]`。
当输入是“我喜欢养”时，模型输出的 Logits 可能是：

| 词 (Token) | Logits (原始得分) | 含义                 |
| :--------- | :---------------- | :------------------- |
| **猫**     | **5.0**           | 分数很高，大概率是它 |
| **狗**     | **4.5**           | 分数也不错，备选     |
| **车**     | **-3.0**          | 负分滚粗，语境不通   |

这时候，你还不能直接用这些数，因为它们不是概率。

---

## 二、 概率制造机：Softmax

为了让这些 Logits 变成概率，我们需要请出深度学习界的“魔术师”——**Softmax 函数**。

概率必须满足两个硬性条件：

1.  **所有值都在 0 到 1 之间**。
2.  **所有值加起来必须等于 1**（100%）。

Softmax 做了两件事来实现这个目标：

### 第一步：指数化（Exp）—— 放大差距

它把每个 Logit 变成 $e^{Logit}$。

- **负数变正数**：$e^{-3}$ 是一个很小的正数，不再是负数了。
- **大数变极大**：指数增长是非常恐怖的。
  - $e^{5.0} \approx 148$
  - $e^{4.5} \approx 90$
  - $e^{-3.0} \approx 0.05$

**注意**：原本 5.0 和 4.5 只差 0.5，看起来差不多。但经过指数化后，148 和 90 差了快一倍！**Softmax 会天然地放大“强者”的优势。**

### 第二步：归一化（Normalize）—— 分蛋糕

把上面算出来的三个数加起来：$148 + 90 + 0.05 \approx 238.05$。
然后，每个数除以这个总和。

- **猫**：$148 / 238.05 \approx \mathbf{62\%}$
- **狗**：$90 / 238.05 \approx \mathbf{37.8\%}$
- **车**：$0.05 / 238.05 \approx \mathbf{0.02\%}$

现在，我们得到了完美的概率分布！

---

## 下篇预告

Logits 变成了概率，那“温度”是在哪里起作用的？
为什么温度调高一点点，模型的回答就变得天马行空？
请看下篇：《2.2 温度（Temperature）的数学本质：它是如何“扭曲”现实的？》。

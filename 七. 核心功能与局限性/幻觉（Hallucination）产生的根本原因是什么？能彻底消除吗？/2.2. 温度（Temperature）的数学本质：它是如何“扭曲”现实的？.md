# 2.2 温度（Temperature）的数学本质：它是如何“扭曲”现实的？

在上篇中，我们了解了 Logits 是如何通过 Softmax 变成概率的。但你是否好奇：我们在 API 里设置的 **Temperature（温度）** 参数，到底是在哪个环节介入的？它是如何控制模型“发疯”还是“严谨”的？

今天，我们来揭开温度参数的数学面纱。

---

## 一、 温度在哪里动手脚？

关键来了。**温度参数并不是在采样（抽奖）的时候才用的，而是在 Softmax 之前插了一脚。**

公式变成了这样：
$$ \text{New Logit} = \frac{\text{Logit}}{\text{Temperature}} $$

也就是：**在进行指数化之前，先把所有的 Logits 除以温度 $T$。**

这个简单的除法，彻底改变了“贫富差距”。

### 1. 高温模式（T > 1）：劫富济贫，拉平差距

假设我们要把温度调到 **2.0**（极高）。

- **原始 Logits**：猫 5.0，狗 4.5。差距 0.5。
- **除以 2.0 后**：猫 2.5，狗 2.25。**差距缩小到了 0.25！**

当差距缩小后，再送进 Softmax：

- $e^{2.5} \approx 12.2$
- $e^{2.25} \approx 9.5$
- **猫的概率**：$12.2 / (12.2+9.5) \approx \mathbf{56\%}$
- **狗的概率**：$9.5 / (12.2+9.5) \approx \mathbf{44\%}$

**结果**：猫从 62% 降到了 56%，狗从 37% 升到了 44%。
**概率分布变“平”了（Flatter）。** 各种可能性的概率变得更接近，冷门词被选中的机会大大增加。这就是为什么高温更有“创造力”（也更容易胡说八道）。

### 2. 低温模式（T < 1）：马太效应，强者恒强

假设我们要把温度调到 **0.5**（较低）。

- **原始 Logits**：猫 5.0，狗 4.5。差距 0.5。
- **除以 0.5 后（等于乘 2）**：猫 10.0，狗 9.0。**差距被拉大到了 1.0！**

当差距拉大后，再送进 Softmax：

- $e^{10} \approx 22026$
- $e^{9} \approx 8103$
- **猫的概率**：$22026 / (22026+8103) \approx \mathbf{73\%}$
- **狗的概率**：$8103 / (22026+8103) \approx \mathbf{27\%}$

**结果**：猫的优势被进一步放大了。
如果你把温度调到 **0.1**，猫的概率可能会接近 99.99%。这时候模型就变得极其确定、死板，只敢选分最高的那一个。

---

## 二、 总结：一条数据的奇幻漂流

让我们完整回顾一下模型输出一个 Token 的全过程：

1.  **神经网络计算**：经过几十层 Transformer，输出 Hidden State。
2.  **映射（Linear）**：Hidden State $\rightarrow$ **Logits**（原始得分，有正有负）。
3.  **温度调节（Temperature）**：Logits / $T$。
    - $T > 1$：缩小差距（概率扁平化）。
    - $T < 1$：放大差距（概率尖锐化）。
4.  **概率化（Softmax）**：$e^x$ 放大差距 $\rightarrow$ 归一化 $\rightarrow$ **Probabilities**（概率）。
5.  **采样（Sampling）**：
    - **Greedy**：直接选概率最大的（相当于 T=0）。
    - **Top-P / Top-K**：在这个概率分布里进行随机抽样。
6.  **输出**：选中一个 Token（比如“猫”），输出给用户。

现在你明白了，**温度本质上是一个“对比度调节器”**。

- 调高温度，就是降低对比度，让画面模糊一片（大家都差不多）。
- 调低温度，就是拉高对比度，让黑的更黑，白的更白（确定的更确定）。

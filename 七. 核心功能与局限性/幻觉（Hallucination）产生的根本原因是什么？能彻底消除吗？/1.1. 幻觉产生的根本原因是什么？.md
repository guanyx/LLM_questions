# 1.1 幻觉产生的根本原因是什么？

想象一下，你正在参加一场没有准备的考试。题目是：“请详细描述 18 世纪法国面包师的日常生活。”

你大概知道那是法国大革命时期，人们吃不饱饭。于是你开始调动脑子里所有关于“法国”、“面包”、“18 世纪”的碎片记忆，结合你读过的小说、看过的电影，开始**一本正经地胡说八道**。你写得绘声绘色，甚至编造了一个叫“皮埃尔”的面包师，说他每天早上 4 点起床烤法棍（其实法棍是后来才有的）。

这就是 AI 的“幻觉”。

在大众眼里，ChatGPT 这种大模型有时候像个“满嘴跑火车的骗子”。问它一个正经事实，它能编出一整套看似逻辑严密但完全虚假的内容。

但请注意，**它不是在撒谎**。撒谎的前提是知道真相但故意不说，而 AI 是**真的以为自己在说真话**，或者更准确地说，它根本不在乎什么是真话。

为什么会这样？这是 Bug 吗？

今天我们不谈复杂的数学公式，只用最朴素的逻辑，把“幻觉”产生的四大技术根源彻底讲明白。

---

## 一、 它是“概率接龙”高手，不是百科全书

很多人以为 ChatGPT 脑子里存了一个巨大的数据库，像百度百科一样。你问它问题，它就去数据库里检索，然后把答案念给你听。

**大错特错。**

大模型脑子里**没有任何一条确定的事实**。它本质上是一个**概率预测机器**（Next Token Prediction）。

它的训练过程就是读了整个互联网的文本，然后学习一个规律：**“在这个词后面，接哪个词的概率最大？”**

比如你给它一句：“床前明月光，疑是\_\_\_\_”。
它根据概率算出，后面接“地”的概率是 99%，接“鞋”的概率是 0.01%。于是它输出了“地”。

在这个过程中，它不需要理解什么是“月光”，什么是“霜”。它只是在玩一个超级复杂的**文字接龙游戏**。

**这就是幻觉产生的第一个根源：它只是在通过概率拼凑句子，而不是在陈述事实。** 当它遇到知识盲区时，它不会说“我不知道”，而是会顺着语言的惯性，选一个概率上看起来最通顺的词填上去。只要句子通顺，它就认为自己完成了任务。

## 二、 JPEG 压缩原理：有损压缩的代价

我们可以把大模型看作是对全人类知识的一个**压缩包**。

这就好比一张 100MB 的高清照片，被压缩成了一张 100KB 的 JPEG 图片。

- **宏观上看**：照片里的山、水、人都在，大意没错。
- **微观上看**：如果你放大看某个人的睫毛，你会发现那里是一团模糊的噪点，或者是算法自动脑补出来的虚影。

大模型在训练时，把海量的互联网数据“压缩”进了它的神经网络参数里。这个过程必然是**有损的**。它记住了“拿破仑是法国皇帝”这种大概率出现的宏观知识，但可能会记错“拿破仑哪天早餐吃了什么”这种微观细节。

当你要它恢复这些丢失的细节时，它只能靠“脑补”（插值）来填补空白。这就是幻觉。

## 三、 为了讨好你，它学会了“不懂装懂”

大模型的训练有一个阶段叫 **RLHF（人类反馈强化学习）**。简单说，就是找一堆人来给模型的回答打分：回答得好给糖吃，回答得不好挨板子。

这听起来很棒，但也带来了一个副作用：**迎合（Sycophancy）**。

如果人类提问者表现出某种倾向，模型为了拿高分，会顺着你的话说。
比如你问：“为什么地球是平的？”
如果模型不仅不反驳，还给你编造了一堆“地平说”的理论，那是因为在它的训练数据里，有时候顺着人类说话能得到奖励。它学会了由着你的性子来，而不是坚持真理。

## 四、 知识是有保质期的

还有一个常被忽视的原因：**知识截止（Knowledge Cutoff）**。

模型在训练完成的那一刻，它的知识就定格了。如果你问它今天的新闻，或者最新的技术框架，它的大脑里是一片空白。
但因为它的使命是“回答问题”，所以它会用它脑子里旧的知识强行拼凑出一个答案。这也是一种常见的幻觉来源。

---

## 下篇预告

既然幻觉源于这些底层机制，那是不是意味着我们永远无法消除它？
RAG、Agentic Workflow、推理模型……这些前沿技术究竟是如何抑制幻觉的？
请看下篇：《1.2 幻觉能彻底消除吗？》。

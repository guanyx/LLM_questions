# 多模态融合架构详解：Early Fusion (Concat) vs Middle Fusion (Cross-Attention)

> 作者：技术领军者视角
>
> 标签：#Multimodal #LLM #Architecture #Fusion #LLaVA #Flamingo

在多模态大模型（MLLM）的架构设计中，当我们完成了图像特征的提取（Vision Encoder）和对齐（Projection）之后，面临的下一个核心决策就是：**如何把这些视觉特征“喂”给 LLM？**

这不仅仅是一个维度的拼接问题，更是关乎模型**推理效率、上下文窗口限制以及架构通用性**的战略选择。

目前业界主流的方案主要分为两派：**Early Fusion（早期融合，以 Concat 为代表）** 和 **Middle Fusion（中期融合，以 Cross-Attention 为代表）**。今天我们深入剖析这两者的原理、优劣与工程选择。

---

## 一、 Early Fusion：简单粗暴的“拼接派”

**Early Fusion**，顾名思义，就是在输入层就把模态融合掉。目前开源界最火的 **LLaVA** 系列就是这一派的集大成者。

### 1. 核心原理
它的逻辑非常直观：**把图像看作一种特殊的“外语”**。
既然 LLM 能处理文本 Token，那我们只要把图像特征投影（Project）成和文本 Token 一样的维度（Embedding Size），然后直接**拼接到文本序列的前面或中间**即可。

在代码层面，这通常表现为简单的列表相加：
```python
# 伪代码示意
input_embeddings = concat([
    system_prompt_tokens,   # 系统提示词
    image_feature_tokens,   # 图像转换成的 Token
    user_instruction_tokens # 用户的指令
])
```

### 2. 优势
*   **架构零修改**：这是最大的杀手锏。你不需要修改 LLM（如 Llama-3, Qwen）内部的任何一行 Transformer 代码。模型甚至不知道自己读的是图还是字，它只看到了一串长长的 Embedding。
*   **训练简单**：可以直接复用现成的、强大的纯文本 LLM 权重，收敛速度快。

### 3. 致命弱点：序列长度爆炸
这种“把图当字读”的方法，代价是**极其昂贵的上下文窗口（Context Window）**。
*   一张 336x336 的图片，经过 CLIP 处理后，通常会变成 **576 个 Token**。
*   如果你采用了 AnyRes 技术（切图以提升分辨率），一张高清图可能瞬间变成 **2000-3000 个 Token**。
*   由于 Transformer 的注意力机制计算复杂度是 $O(N^2)$，当输入包含多张图片时，推理延迟和显存占用会呈指数级上升。

---

## 二、 Middle Fusion：优雅复杂的“注入派”

**Middle Fusion** 则认为图像不应该占用宝贵的文本序列长度。它的代表作是 DeepMind 的 **Flamingo** 以及后续的 IDEFICS。

### 1. 核心原理
在这个架构中，文本 Token 依然走 Transformer 的主干道（Self-Attention），而图像特征则通过一个侧面的通道，被“注入”到每一层（或部分层）中。

具体的实现方式通常是引入一个新的层：**Gated Cross-Attention（门控交叉注意力层）**。
*   **Query (Q)**：来自当前的文本 Token。
*   **Key (K) / Value (V)**：来自图像特征。

每一层 Transformer Block 在处理文本时，都会“回头看一眼”图像特征，从中获取必要的信息，然后继续处理文本。

### 2. 优势
*   **序列长度解耦**：这是最大的工程红利。无论你输入 1 张图还是 10 张图，**文本序列的主干长度是不变的**。图像特征只在 Cross-Attention 阶段参与计算。
*   **推理高效**：在多图、长视频理解的场景下，Middle Fusion 的推理速度通常优于 Early Fusion，因为主干序列没有被撑爆。

### 3. 劣势
*   **架构侵入性强**：你必须修改 Transformer 的标准结构，插入新的 Cross-Attention 层。这意味着你无法直接使用 HuggingFace 上现成的纯文本模型加载器。
*   **训练难度大**：由于插入了新参数（从零开始初始化），通常需要海量的数据和更长的训练周期才能让模型“学会”使用这个新的侧侧通道。

---

## 三、 深度对比与工程决策

为了更直观地对比，我们将两者放在同一个维度下审视：

| 特性 | Early Fusion (Concat) | Middle Fusion (Cross-Attention) |
| :--- | :--- | :--- |
| **代表模型** | LLaVA, Qwen-VL, Yi-VL | Flamingo, IDEFICS |
| **实现难度** | ⭐ (极低) | ⭐⭐⭐ (高) |
| **上下文占用** | **极高** (图越多 Token 越多) | **低** (与图片数量解耦) |
| **推理复杂度** | $O((T_{text} + T_{img})^2)$ | $O(T_{text}^2 + T_{text} \times T_{img})$ |
| **LLM 复用性** | 完美复用 | 需要魔改结构 |
| **多图/视频能力** | 较弱 (受限于窗口) | 较强 |

### 我们的决策建议

**场景 A：追求快速落地、单图/少图对话**
*   **选择**：**Early Fusion (Concat)**
*   **理由**：LLaVA 架构已经证明了其强大的统治力。对于大多数电商导购、OCR、看图说话场景，单图产生的 576 个 Token 完全在现代 LLM（通常支持 4k-128k 窗口）的承受范围内。不要为了优化不存在的性能瓶颈而去引入复杂的架构。

**场景 B：长视频理解、多图长篇故事生成**
*   **选择**：**Middle Fusion** 或 **高度优化的 Early Fusion**
*   **理由**：如果你需要处理 1 分钟的视频（抽帧后可能有 60 张图），Concat 产生的 Token 数将突破 30k，导致推理慢如蜗牛。此时，Flamingo 风格的 Cross-Attention 或者针对 Token 进行压缩（Token Compression/Pruning）是必须的。

---

## 四、 总结与展望

目前的趋势看，**Early Fusion (Concat) 凭借其简洁性占据了开源生态的主导地位**。
虽然它有 Token 爆炸的问题，但社区正在通过 **Token Pooling（Token 池化压缩）** 和 **Dynamic Resolution（动态分辨率）** 等技术来缓解这一痛点（例如 LLaVA-Next）。

长远来看，随着 **Native Multimodal（原生多模态）** 模型的兴起（如 GPT-4o, Gemini 1.5 Pro），这种“文本模型外挂视觉”的区分可能会逐渐模糊。未来的模型，也许生来就拥有处理多模态数据的 Transformer 结构，不再区分 Early 还是 Middle，而是融为一体。

# 11.2 归一化之战：Pre-Norm 真的完美吗？

在上一篇文章中，我们提到了 Transformer 架构中一个关键的工程选择：**Normalization（归一化）的位置**。

早期的 BERT 时代推崇 **Post-Norm**，而现在的 GPT-3、LLaMA 等主流大模型几乎清一色转向了 **Pre-Norm**。这一转变被视为训练万亿参数模型的“稳定器”。

但正如工程界没有银弹，Pre-Norm 在带来稳定性的同时，也付出了**“表达能力受限”**的代价。这究竟是怎么回事？现在的 LLaMA 又是如何通过 RMSNorm 来修补这个缺憾的？

---

## 第一回合：Pre-Norm 到底损失了什么？

要理解“表达能力损失”，我们需要先复习一下两者的结构差异（通俗版）：

*   **Post-Norm**：`x = Norm(x + Sublayer(x))`。每一层处理完，先加到主干上，然后**整体**做一个归一化。
*   **Pre-Norm**：`x = x + Sublayer(Norm(x))`。主干道（Residual Path）一直保持原样，每一层只是把自己的输出（经过归一化处理）加到主干上。

### 1. 梯度流的“高速公路”与“拦路虎”
*   **Pre-Norm 的优势**：主干道上没有任何 Norm 层阻挡，梯度可以直接从最后一层无损地传到第一层。这就是为什么它训练极度稳定，不需要复杂的 Warm-up 就能收敛。
*   **Post-Norm 的劣势**：每一层的主干道上都有一个 Norm。梯度往回传时，每经过一个 Norm 都会被重新缩放和调整。层数越深，梯度要么消失要么爆炸，训练极难控制。

### 2. 代价：深层的“躺平”现象
然而，成也萧何败也萧何。Pre-Norm 的主干道因为没有 Norm 的约束，数值幅度（方差）会随着层数的增加而不断累积、变大。

想象一下，你是一个负责第 100 层的神经网络。
*   **在 Post-Norm 中**：输入给你的信号已经被 Norm 强行拉回了标准范围（比如 -1 到 1），你必须努力工作才能对输出产生影响。
*   **在 Pre-Norm 中**：输入给你的信号是从第 1 层一路累加过来的，数值可能已经巨大无比（比如 -100 到 100）。而你这一层（Sublayer）经过 Norm 后输出的数值依然很小（-1 到 1）。
*   **结果**：你这一层的微小贡献，相对于主干道上巨大的数值来说，简直是**九牛一毛**。

这就导致了 Pre-Norm 架构的一个显著问题：**深层网络的权重更新幅度非常小，很多层实际上在“摸鱼”，对最终结果的贡献远不如浅层网络。** 这就是所谓的“表达能力上限受损”。

---

## 第二回合：RMSNorm —— 做减法的艺术

既然 Pre-Norm 有缺陷，为什么 LLaMA 还是坚持用它？因为它通过另一个组件——**RMSNorm（Root Mean Square Layer Normalization）** 进行了优化。

传统的 LayerNorm 做两件事：
1.  **减去均值（Re-center）**：把数据平移到以 0 为中心。
2.  **除以标准差（Re-scale）**：把数据缩放到标准幅度。

**RMSNorm 的洞见**：
Geoffrey Hinton 等人发现，LayerNorm 真正起作用的其实是 **Re-scale（缩放）**，至于 Re-center（平移），不仅计算费时，而且对效果没啥帮助。

于是，RMSNorm 干脆**砍掉了减均值**这一步，只保留**除以均方根**。

### 1. 速度提升
少算一步均值，在大模型动辄千亿次的计算中，能带来显著的训练速度提升（约 10%~40%）。

### 2. 隐形的“增益”
虽然 RMSNorm 并没有直接解决 Pre-Norm 的深层“躺平”问题，但它配合 **SwiGLU 激活函数** 和 **没有 Bias（偏置项）** 的设计，让整个网络的数值分布更加纯粹。

*   **去掉了 Bias**：很多现代 LLM（如 PaLM, LLaMA）都去掉了线性层里的 Bias。因为在海量数据下，Bias 往往引入的是噪音而非特征，去掉它反而能让模型更专注于学习权重之间的**交互关系**。

---

## 第三回合：未来的“修正主义”——DeepNorm

既然 Pre-Norm 有深层退化的问题，难道我们就束手无策吗？微软提出的 **DeepNorm** 试图结合两者的优点。

它的核心思想非常简单粗暴：**既然 Post-Norm 不稳定是因为残差连接使得数值不断膨胀，那我就手动把它“按”下去。**

DeepNorm 采用了 Post-Norm 的结构（保证表达能力），但在进行残差连接时，对主干分支进行了一个**系数缩放（Scaling）**。随着层数加深，这个系数会越来越大，强行把梯度的幅度压住。

*   **效果**：DeepNorm 号称能把 Post-Norm 架构的模型稳定地堆到 1000 层以上，且性能优于同深度的 Pre-Norm。
*   **现状**：虽然理论很美好，但由于 LLaMA 等开源生态的惯性，目前主流社区依然是 **Pre-Norm + RMSNorm** 的天下。毕竟，对于大多数人来说，“稳”比“强那一丢丢”更重要。

---

## 总结

作为中级工程师，在面试或设计模型时，你需要明白：

1.  **Pre-Norm 的“原罪”**：它牺牲了深层的贡献度，换取了训练的稳定性（梯度高速公路）。
2.  **LLaMA 的选择**：通过 **RMSNorm**（简化计算）+ **No Bias**（减少噪音）来从侧面优化性能，虽然没有根治 Pre-Norm 的理论缺陷，但在工程上达到了最佳的性价比。
3.  **未来的方向**：如果你追求极致的性能，可以关注 **DeepNorm** 等尝试复活 Post-Norm 的技术；但在实际 finetune 任务中，老老实实跟随主流架构通常是最安全的选择。

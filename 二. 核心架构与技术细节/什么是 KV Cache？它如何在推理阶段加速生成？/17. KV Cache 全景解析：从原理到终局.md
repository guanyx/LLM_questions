# 17. KV Cache：一张显卡上的“便签纸系统”

KV Cache 这个词听起来很“框架”，但它其实特别朴素：模型在生成时会反复用到“历史信息”，所以干脆把算过的中间结果放进显存里，下一步直接读出来。

这篇文章想回答三个问题：

1. KV Cache 到底缓存了什么？为什么能让生成速度稳定？
2. 为什么只缓存 K/V，不缓存 Q？
3. Prefill 和 Decoding 为什么像两种完全不同的工作？以及接下来会往哪儿走？

---

## 1) 先把问题摆到桌上：自回归到底慢在哪？

模型生成是“自回归”的：一次吐一个 token（你可以先把它当成“一个字/一个词片段”），然后把它拼回输入里，再吐下一个 token。

假设你给它一句话：  
“今天天气真好，我想去……”

它可能会生成“公”，然后下一步要生成“园”。

没有任何缓存的话，每一步都像这样：

```text
第 1 步：处理  [今 天 天 气 真 好 ， 我 想 去]
第 2 步：处理  [今 天 天 气 真 好 ， 我 想 去 公]
第 3 步：处理  [今 天 天 气 真 好 ， 我 想 去 公 园]
...
```

也就是说：你每生成一个新 token，就把前面那一长串又重新算一遍。

这会导致两个非常直观的结果：

- 句子越长，越慢（因为重复计算越来越多）
- 你会感觉模型“越写越慢”，甚至突然卡顿

---

## 2) KV Cache：把“算过的东西”留在显存里

Transformer 里每层的 attention（注意力）会把每个 token 的表示变成三样东西：

- Q（Query）：这一步“我想查什么”
- K（Key）：这个 token “像什么标签”
- V（Value）：这个 token “带着什么内容”

如果用一个非常粗糙的图表示：

```text
token 表示  --线性变换-->  Q
token 表示  --线性变换-->  K
token 表示  --线性变换-->  V
```

KV Cache 做的事很简单：把历史 token 的 K/V 存下来。

### Prefill vs Decoding（先把词对齐）

推理时大致分两段：

**Prefill**：处理你输入的整段 prompt（提示词）（可以并行）  
**Decoding**：开始一个 token 一个 token 往外吐（串行）

把它画成时间轴大概像这样：

```text
prompt token:  [1][2][3][4][5] ... [N]
prefill:        ^^^^^^^^^^^^^^^^^^^^^  (并行算完，写 KV)
decode step 1:                          (读 KV + 写新 KV)
decode step 2:                          (读 KV + 写新 KV)
...
```

关键点：到了 Decoding，你只需要为“新 token”算一次 Q/K/V，然后用 Q 去看历史的 K/V。

所以每一步的计算更像：

```text
历史 K/V：  K1..Kt , V1..Vt   (从显存读)
当前 token：Qt, Kt, Vt        (新算出来)
attention:  Qt  x  (K1..Kt)   -> 权重 -> 加权 (V1..Vt)
```

直观感受就是：生成第 1000 个 token 时，不再需要“从头理解 1..999”，而是“直接翻便签纸”。

---

## 3) 那为什么不缓存 Q？Q 去哪了？

我喜欢用一个很幼稚但很好记的比喻：

- Q 是“手电筒”
- K 是“目录卡片”
- V 是“书的内容”

你现在要找信息（生成下一个 token），你拿起当前这一步的手电筒（当前 Q），去照亮过去所有的目录卡（历史 K），然后把对应的书内容（历史 V）搬出来拼一拼。

那过去的 Q 呢？

过去的 Q 是“当时那一步用过的手电筒光束”，照完就结束了。你不会在第 100 步还需要第 20 步当时那束光。

更技术一点的说法是：推理时是因果 attention（注意力）（下三角）。第 t 步的 attention 只会用到：

- 当前的 $Q_t$
- 历史的 $K_{1..t}$ 和 $V_{1..t}$

没有任何地方会再用到 $Q_{1..t-1}$。

所以缓存 Q 只会：

- 占更多显存
- 不帮你省任何计算

---

## 4) 性能为什么“忽快忽慢”？Prefill 和 Decoding 根本不是一类活

如果你做过推理服务优化，会遇到一种很怪的感觉：

- Prefill 时：GPU 算得很凶，像在跑大矩阵乘
- Decoding 时：GPU 好像在等内存，像在搬东西

这其实是两个不同瓶颈：

### Prefill：算力型（compute-bound）

因为你一次性处理很多 token，能把矩阵做大，Tensor Cores 很开心。

可以把它想成：

```text
一次搬一大车货 -> 在工厂里加工很久 -> 再搬走
```

### Decoding：带宽型（memory-bound）

Decoding 每次只生成 1 个 token，但要把“历史 KV”整段读一遍（越长越多）。

像这样：

```text
每次加工一小颗糖，但要先把整仓库的原料翻一遍
```

所以很多时候 Decoding 的极限不是算力，而是显存带宽（HBM bandwidth）。

---

## 5) Chunked Prefill：别让“大象”踩死“蚂蚁”

真实的线上服务通常会把多个请求一起打包（continuous batching，连续批处理）。这时会发生一个很烦人的事：

- A 请求在 Decoding：用户希望它每 20ms 吐一个 token（体验很重要）
- B 请求刚进来，prompt 超长，需要 Prefill 10k token（一次大计算）

如果你让 B 一口气跑完 Prefill，A 就会在这段时间里“停住”，用户看到的就是：打字机突然卡了一下。

Chunked Prefill 的做法是：把 B 的 Prefill 切成小块，比如每 512 token 一块，夹在每轮 decoding 里。

想象一个时间片：

```text
轮次 1：A decode 1 步 + B prefill 512
轮次 2：A decode 1 步 + B prefill 512
轮次 3：A decode 1 步 + B prefill 512
...
```

它的效果很“系统工程”：

- A 不容易卡（更稳定的延迟）
- 还能顺手把 decoding 里闲着的算力塞满一点（更高吞吐）

---

## 6) KV Cache 太占显存了，大家都在想办法“少存点 / 存得更聪明”

这里先把常见思路分两类：少存点、存得更聪明。

### 减少体积：MQA / GQA

多头 attention 的麻烦在于：如果每个 head 都有一套 K/V，那 KV Cache 会很胖。

- MQA：所有 head 共享同一套 K/V
- GQA：把 head 分组，每组共享一套 K/V（更常见的折中）

这就是“共享便签纸”的思路。

### 改善管理：PagedAttention

另一种麻烦是显存碎片：KV Cache 不是一大块连续内存，而是各种请求不断进出，像内存版的“行李箱收纳失败”。

PagedAttention 借鉴操作系统分页：把 KV Cache 切成小块，用索引表管理，允许它们散落在显存里。

这类方法的核心收益是：更高的显存利用率，更稳定的吞吐。

### 继续硬顶：Ring Attention / Offloading

当上下文长到离谱（比如 1M+），单卡真的放不下了，就会出现更激进的办法：

- Ring Attention：把 KV 分到多卡上，用通信换显存
- Offloading：把一部分 KV 挪到 CPU 内存甚至 SSD，用更慢的带宽换更大的容量

---

## 7) 终局问题：KV Cache 会不会消失？

Transformer 的一个“原罪”是：为了保持全量注意力，推理时 KV Cache 需要随着长度线性增长。

这会把你带到一个很现实的天花板：

- 128k：还能靠 GQA / PagedAttention 勉强扛
- 1M：显存需求开始离谱，多卡多机成本爆炸

于是就有人换思路：既然 KV 让你崩溃，那我们做一个“不需要 KV”的架构。

### Mamba / RWKV（以及其他线性注意力思路）

这些架构的直觉是：别把每一页书都摊在桌上（KV Cache），而是边读边把信息压缩进一个固定大小的“状态”里。

好处很诱人：

- 推理显存更像常数
- 很适合超长上下文

坏处也很直观：

- 压缩一定会丢东西
- “大海捞针”类精确回忆可能不如 Transformer（因为 Transformer 可以真的回头去看那一页）

我更愿意把它看成两种记忆策略：

- Transformer：保留所有便签纸，回忆很准，但桌面会被堆满
- 线性/状态模型：桌面永远很干净，但有些细节可能真的记不住

所以一个很可能的结局是：混合架构。

```text
大量层：   便宜地处理长上下文（状态/线性）
少量层：   关键时刻做精确检索（attention）
```

---

## 8) 一个有点“玄”的视角：KV Cache 像不像“临时权重”？

如果你把 KV Cache 看成“模型在这次对话里写下的工作记忆”，它就不只是缓存了。

粗暴地说：

- 静态权重：模型长期记住的东西（训练得到）
- KV Cache：模型这一次临时记住的东西（推理产生）

有些研究会把它类比成 fast weights：像在推理时给自己加了一层“临时参数”。

这条路继续往前走，会出现一些挺科幻但很实用的方向：

- 把旧 KV 压缩成摘要记忆（而不是直接扔掉）
- 把“短期记忆”持久化成可检索的外部记忆（RAG 也可以看成一种外接记忆）

---

## 小结（给赶时间的人）

- KV Cache 让生成不再重复算历史，把“计算”换成“读显存”
- 不缓存 Q 的原因很简单：历史 Q 不会再被用到，缓存它只会浪费显存
- Prefill 更像算力问题，Decoding 更像带宽问题，所以调度策略（比如 chunked prefill）很重要
- 业界的优化大方向：少存（MQA/GQA）、好管理（PagedAttention）、放到别处（Offloading）、多卡分摊（Ring）
- 更长远的分叉：继续修补 Transformer，或者转向不依赖 KV Cache 的状态/线性架构，最后可能在混合模型里相遇

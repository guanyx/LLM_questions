# 17.2. 为什么 Prefill 和 Decoding 的性能瓶颈完全不同？Chunked Prefill 是什么？

> **核心观点**：大模型推理并非单一的匀速运动，而是由两种截然不同的物理过程组成的混合体。**Prefill（预填充）** 是计算密集型的短跑，受限于 GPU 算力；**Decoding（解码）** 是访存密集型的马拉松，受限于显存带宽。当两者在同一块 GPU 上争抢资源时，**Chunked Prefill（分块预填充）** 成为平衡延迟与吞吐量的关键调度艺术。

## 一、 同一张显卡，两种截然不同的“性格”

如果你是一名正在优化推理服务的中级工程师，你很快会发现一个奇怪的现象：模型在处理用户刚发来的长文章时（Prefill），显卡的核心（Compute）转得飞快，但显存带宽没跑满；而在模型逐字往外蹦的时候（Decoding），显存带宽（Bandwidth）直接拉爆，但计算核心却在“摸鱼”。

这是因为这两个阶段在硬件层面属于完全不同的**工作负载（Workload）**。

### 1. Prefill 阶段：算力受限（Compute-bound）

*   **场景**：用户发来一句“把这篇 5000 字的文章总结一下”。模型需要一次性并行处理这 5000 个 Token。
*   **计算特征**：这是一个巨大的矩阵乘法（GEMM）。输入矩阵很大（Batch Size * Sequence Length），权重矩阵也很大。
*   **瓶颈**：**GPU 的计算单元（Tensor Cores）**。
    *   因为数据量大，GPU 只要把数据从显存搬到计算单元一次，就能在里面进行海量的乘加运算。计算单元忙得不可开交，而显存搬运工相对轻松。
*   **类比**：这就像**做批发**。你开着卡车把一整仓库的面粉拉到工厂（搬运数据），然后机器全速运转做面包（计算）。效率极高。

### 2. Decoding 阶段：带宽受限（Memory-bound）

*   **场景**：模型根据之前的 5000 字，吐出第 1 个字，然后第 2 个字……
*   **计算特征**：这是无数次极小的矩阵向量乘法（GEMV）。每次只能算 1 个 Token。但是！为了算这 1 个 Token，你必须把之前 5000 个 Token 对应的 KV Cache 从显存里完整读一遍。
*   **瓶颈**：**GPU 的显存带宽（HBM Bandwidth）**。
    *   数据搬运量巨大（几 GB 的 KV Cache），但计算量极小（只是算一个字）。计算单元瞬间就做完了，然后眼巴巴地等着下一批数据从显存里慢吞吞地搬过来。
*   **类比**：这就像**送外卖**。你每次开着法拉利（强大的计算核）去送一个汉堡（极小的数据），绝大部分时间都花在路上（数据传输），车速再快也没用。

## 二、 混合调度的灾难：当“大象”踩死“蚂蚁”

在真实的高并发推理服务（如 vLLM, TGI）中，系统通常会采用 **Continuous Batching（连续批处理）**，即在一个 Batch 中混合处理处于不同阶段的请求。

想象一下这个场景：
1.  **Request A**（Decoding）：正在愉快地逐字生成，需要低延迟，每 20ms 吐一个字。
2.  **Request B**（Prefill）：突然进来一个超长请求，有 10k Token 需要预处理。

如果调度器不加干预，直接让 Request B 开始 Prefill，它会瞬间霸占所有的 Tensor Cores 长达 500ms（假设）。
在这 500ms 里，GPU 算力被吃光了。
**结果**：Request A 被迫暂停。用户会感觉到生成突然卡顿了一下（Stall），原本流畅的打字机效果变成了“便秘”。

这就是 **Head-of-Line Blocking（队头阻塞）** 问题：巨大的 Prefill 任务阻塞了轻量级的 Decoding 任务。

## 三、 破局方案：Chunked Prefill（分块预填充）

为了解决这个问题，业界引入了 **Chunked Prefill**（也叫 Sarathi 调度策略）。

### 1. 核心思想：把大象切成片

既然一口气吃掉 10k Token 会噎死人，那我们就把它切成小块（Chunk），比如每块 512 个 Token。

*   **T0 时刻**：处理 Request A 的 1 个 Decoding 步 + Request B 的 **第 1 块**（512 Tokens）。
*   **T1 时刻**：处理 Request A 的 1 个 Decoding 步 + Request B 的 **第 2 块**（512 Tokens）。
*   ……
*   **Tn 时刻**：Request B 处理完了，开始加入 Decoding 队伍。

### 2. 带来的好处

1.  **消灭卡顿（低且稳定的 ITL）**：
    因为每个 Chunk 的计算量是可控的，Request A 可以在每一轮都得到处理机会，不会被长时间挂起。用户的直观感受依然是流畅的。
2.  **“填缝”效应（提升吞吐）**：
    还记得 Decoding 是带宽瓶颈、计算空闲吗？而 Prefill 恰好是计算瓶颈。
    通过把小块的 Prefill 塞进 Decoding 的 Batch 里，恰好利用了 Decoding 阶段闲置的算力！这就像在装满石块（Decoding）的瓶子里倒进沙子（Chunked Prefill），既没让瓶子变大，又装下了更多的东西。

## 四、 总结与思考

作为中级工程师，理解 KV Cache 仅仅是第一步。

*   **初级视角**：KV Cache 存下了历史，避免了重复计算。
*   **中级视角**：KV Cache 的读取速度（带宽）成为了 Decoding 的阿喀琉斯之踵，而 Prefill 的计算压力（算力）又构成了调度的最大干扰源。

**Chunked Prefill** 完美体现了系统设计的哲学：**当两种负载特征互补（一个缺算力，一个缺带宽）时，最好的办法不是隔离它们，而是通过精细的切分，让它们在微观上“交错共舞”。**

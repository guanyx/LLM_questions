# 17.1. 为什么 KV Cache 不缓存 Query (Q)？它去哪了？

> **核心观点**：Q（Query）是“探照灯”，K（Key）和 V（Value）是“藏书”。推理是一个“只顾当下”的过程：现在的探照灯（Current Q）只需要照亮过去的藏书（History K/V）。以前的探照灯（History Q）已经完成了它的历史使命，既不需要被重新计算，也不需要被再次使用，因此不需要缓存。

## 一、 “吉祥三宝”缺了一角？

如果你刚学完 Transformer 的 Attention 公式：

$$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$

你一定会觉得 Q、K、V 是形影不离的“吉祥三宝”。但在大模型推理的 **KV Cache** 技术中，我们却只把 K 和 V 存进了显存，唯独把 Q 扔掉了。

这不禁让人产生疑问：**Q 去哪了？难道历史的 Q 就不重要吗？如果不存它，后面的计算不会出错吗？**

要回答这个问题，我们需要从大模型的“时间观”说起。

## 二、 形象比喻：探照灯与博物馆

想象大模型写文章的过程，就像是一个探险家在通过**探照灯**观察一个**博物馆**。

- **Token（字）**：每一个字都是一个展品。
- **Q (Query)**：**探照灯**。代表当前这个字“想看什么”、“想找什么”。
- **K (Key)**：**展品的标签**。用于匹配探照灯的搜索。
- **V (Value)**：**展品的内容**。真正有价值的信息。

### 1. 推理的“当下”视角

假设模型已经写了 99 个字，现在要写第 100 个字。

1.  **第 100 个字的 Q（探照灯）亮了**：它发出一束光，扫射前面 1 到 99 个位置。
2.  **匹配 K（标签）**：这束光照到了第 1 个字的标签（$K_1$）、第 2 个字的标签（$K_2$）……直到第 99 个。
3.  **提取 V（内容）**：根据标签的匹配程度（Attention Score），探险家从 $V_1$ 到 $V_99$ 中提取信息，组合成第 100 个字的理解。

在这个过程中，你发现了吗？我们只需要：

- **现在的探照灯**（$Q_{100}$）
- **以前所有展品的标签和内容**（$K_{1...99}, V_{1...99}$）

### 2. 历史的 Q 去哪了？

那么，第 1 个字当时的探照灯（$Q_1$），或者第 50 个字的探照灯（$Q_{50}$）去哪了？

**它们熄灭了。**

$Q_{50}$ 是在生成第 50 个字的那一瞬间用的。当时它负责从前 49 个字里找信息。一旦第 50 个字生成完毕，变成了历史，$Q_{50}$ 的任务就彻底结束了。

**现在的第 100 个字，完全不在乎第 50 个字当时在想什么（$Q_{50}$），只在乎第 50 个字现在是什么（$K_{50}, V_{50}$）。**

所以，历史的 Q 就像用过的“一次性筷子”，用完即弃。

## 三、 技术原理：因果掩码（Causal Mask）与单向注意力

从技术角度看，这与 Decoder-only 架构的**因果律（Causality）**有关。

### 1. 往事不可谏（No Backward Pass for Inference）

在推理阶段，Attention 矩阵是**下三角**形状的。这意味着：

- Token $t$ 只能看到 $t$ 之前的信息。
- Token $t$ **永远无法**看到 $t$ 之后的信息。

当我们计算第 $t$ 步时，我们需要计算 $Q_t$ 与 $K_{1:t}$ 的点积。
这个公式里，**根本就没有 $Q_{1:t-1}$ 的位置**。

历史的 Query ($Q_{past}$) 既不参与当前权重的计算（那是 Current Q 和 Past K 的事），也不作为被提取的信息（那是 Past V 的事）。

### 2. 如果强行存了 Q 会怎样？

假设我们非要把历史的 Q 也存进显存，会有什么用吗？

- **没有任何用处。** 除非你想**修改历史**。
- 比如，你突然想把第 50 个字从“猫”改成“狗”。那这下麻烦了，因为第 50 个字变了，它的语义变了，它对第 51 个字的影响也变了……我们需要从第 50 步开始，重新计算 $Q_{50}$ 以及之后所有的步骤。
- 但在正常的文本生成（Append-only）模式下，历史是写死的（Static），我们永远只在末尾追加新字，绝不回头修改。

## 四、 深度剖析：Q 与 KV 的本质区别

这个问题的背后，其实反映了 Attention 机制中 **Active（主动）** 与 **Passive（被动）** 的角色对立。

| 角色         | 符号          | 职责                       | 属性                    | 命运                 |
| :----------- | :------------ | :------------------------- | :---------------------- | :------------------- |
| **观察者**   | **Q (Query)** | 主动发起查询，寻找相关信息 | **瞬时态** (Transient)  | 用完即走，不需保留   |
| **被观察者** | **K (Key)**   | 被动展示特征，等待被匹配   | **持久态** (Persistent) | 必须长久存储 (Cache) |
| **被观察者** | **V (Value)** | 被动提供内容，等待被聚合   | **持久态** (Persistent) | 必须长久存储 (Cache) |

**KV Cache 的本质，就是把模型“过去的所有思考”，固化成了一个可以被随时查阅的“知识库”。** 而 Q 则是每一次查阅时那个动态的“意图”。

## 五、 总结

回到最初的问题：**为什么 KV Cache 不缓存 Q？**

因为在自回归生成的单行道上，我们永远是**拿着现在的 Q，去读过去的 KV**。

- 过去的 KV 是**路基**，必须铺好垫实，后面的车才能走。
- 过去的 Q 是**车灯**，车开过去了，那里的灯光就不再重要了。

理解了这一点，你也就理解了为什么大模型的推理优化（如 PagedAttention, GQA）全都围绕着 K 和 V 做文章，而对 Q 视而不见——因为它本来就是那个“过客”。

# 既然 SFT 不能注入新知识，为什么垂直领域还需要微调？

## 这是一个好问题

这是一个非常深刻且直击灵魂的问题。

在上一篇关于 SFT 的文章中，我们提出了一个反直觉的观点：**SFT（有监督微调）主要是在做“格式转换”和“能力唤醒”，而不是“知识注入”。** 如果你试图用 SFT 强行让模型记住公司的私有文档，它很可能会一本正经地胡说八道（产生幻觉）。

那么问题来了：**如果我们想做一个垂直领域的应用（比如“法律大模型”或“企业内部代码助手”），既然 SFT 不能教给模型那些它没见过的私有知识，我们为什么还要费时费力地去微调它呢？直接用 RAG（检索增强生成）外挂知识库不就行了吗？**

答案是：**在 Agent（智能体）时代，SFT 的价值依然不可替代。它的核心作用不是“背书”，而是“学会像专家一样思考”和“像员工一样干活”。**

---

## 一、 SFT 在垂直领域的核心价值：不仅仅是语气

很多人认为 SFT 只是为了让模型说话像个律师或医生，这只是最表层的理解。在 2024-2025 年的技术视角下，SFT 实际上解决了 RAG 无法解决的三个核心问题。

### 1. 学习“领域逻辑”与“推理范式” (Reasoning Pattern)

虽然 SFT 很难让模型记住具体的**事实**（例如：“王小明的工号是 9527”），但它非常擅长让模型学习**逻辑**（例如：“在处理工伤赔偿时，必须先判断是否存在劳动关系，再计算伤残等级，最后套用赔偿公式”）。

- **RAG 的局限**：RAG 可以把相关的法律条款检索出来扔给模型，但通用模型（Base Model）可能不知道该**如何运用**这些条款来构建一个严密的法律论证逻辑。它可能会像一个读过法律条文的普通人，生硬地堆砌法条。
- **SFT 的价值**：通过高质量的“案情-判决书”数据进行 SFT，模型学到的不是具体的案情，而是**“法官的思维链（Chain of Thought）”**。它学会了如何抽丝剥茧、如何引用法条、如何进行三段论推理。

**结论：SFT 注入的不是“数据”，而是“领域方法论”。**

### 2. 激活“工具调用”与“Agent 能力” (Tool Use & Function Calling)

这是当前 SFT 最重要的价值之一。现在的垂直领域应用往往不是单纯的问答，而是要**干活**（Agent）。

- **通用模型的问题**：你让通用模型“查询一下昨天的销售额”，它可能只会给你写一段 SQL 代码，或者胡编一个数字。
- **SFT 的价值**：通过构建 `User Query -> API Call` 的微调数据，我们可以训练模型**精准地调用企业内部的 API**。
  - 它学会了：当用户问“库存”时，输出特定格式的 JSON 去调用 `query_inventory_api`。
  - 它学会了：参数填错了怎么自我修正。

**结论：SFT 是把“聊天机器人”变成“业务操作员”的关键步骤。**

### 3. 减少幻觉，提升 RAG 的效果 (RAG-Alignment)

这是目前最前沿的玩法：**为了 RAG 而去 SFT（Adaptation for RAG）。**

通用模型在做 RAG 时，经常会遇到“不知道该信谁”的问题。当检索到的文档与它预训练的知识冲突，或者检索到的文档包含噪音时，通用模型容易犯错。甚至在面对几十万字的长文档（Long Context）时，模型可能会“迷路”。

我们可以专门构造一种 SFT 数据集，训练模型：

- **抗噪能力**：给模型一堆包含干扰项的文档，训练它只提取正确答案。
- **诚实性**：“如果检索上下文中没有答案，**必须**直接说不知道，绝对不能利用预训练记忆瞎编。”

**结论：SFT 可以把模型训练成一个“完美的阅读理解机器”，从而极大地提升 RAG 系统的准确率和可信度。**

---

## 二、 最佳实践：垂直领域的“黄金组合”

所以，回到最初的问题，做垂直领域应用，正确的技术路线图应该是怎样的？

不再是简单的 SFT vs RAG，而是一套组合拳：

1.  **SFT（内功与技能）**：

    - **目的**：让模型学会领域的**思维模式**、**工具调用能力**、以及**如何处理长文档**。
    - **数据**：思维链（CoT）数据、API 调用数据、阅读理解数据。

2.  **RAG / Long Context（外挂记忆）**：

    - **目的**：提供实时、准确、私有的**事实知识**。
    - **趋势**：对于几十万字以内的文档，直接塞进 Context（长窗口）往往比 RAG 效果更好，但这同样需要 SFT 来增强模型在长窗口下的注意力（Lost in the Middle 现象优化）。

3.  **DPO（审美对齐）**：
    - **目的**：这是 SFT 之后的进阶步骤。通过**直接偏好优化（Direct Preference Optimization）**，让模型知道“什么样的回答是好的”。
    - **场景**：比如在医疗领域，医生更喜欢简洁、分点的诊断建议，而不是长篇大论。DPO 能比 SFT 更高效地对齐这种“专家品味”。

---

## 三、 总结

如果把垂直领域大模型比作一位**“新入职的专员”**：

- **Pre-training（预训练）**：是他大学毕业，有了通用的智商和百科知识。
- **RAG / 知识库**：是他手边的《员工手册》和《业务档案》，让他随时能查到具体的死知识。
- **SFT（微调）**：是**“入职培训”**。它教会了员工：
  1.  **办事逻辑**（思维链）；
  2.  **怎么使用公司的内网系统**（工具调用）；
  3.  **怎么基于手册回答问题而不瞎编**（RAG 对齐）。

**没有 SFT，他只是一个抱着书本的书呆子；有了 SFT，他才是一个能熟练使用公司工具、遵循公司流程的专业人士。**

# SFT 的天花板：何时该停止微调，转而使用 RLHF？

## “堆数据”的尽头

在训练大模型的过程中，很多团队会陷入一种**“数据迷信”**：只要效果不好，就继续加 SFT（有监督微调）数据。从 1 万条加到 10 万条，效果确实提升了；但从 10 万条加到 100 万条，效果却停滞不前，甚至开始出现“变傻”的迹象。

这就是 **SFT 的边际效应递减**。

作为高级 AI 工程师，我们需要回答一个战略性问题：**什么时候应该停止 SFT，转而启动 RLHF（Online DPO/PPO）？** 也就是找到那个 ROI（投入产出比）最高的**技术切换点**。

---

## 一、 SFT 的“S 曲线”与饱和点

根据 Scaling Law，SFT 的性能提升并非线性的，而是呈现典型的 **Sigmoid（S 型）** 曲线。

### 1. 格式饱和（1k - 10k 条）

在极少量数据下，模型迅速学会了“说话的格式”。

- **表现**：从续写变成对话，能输出 JSON，知道该什么时候闭嘴。
- **收益率**：极高。这是性价比最高的阶段。

### 2. 知识/模式饱和（10k - 100k 条）

随着数据增加，模型开始掌握特定领域的推理模式和知识边界。

- **表现**：代码通过率（Pass@1）提升，数学解题步骤更规范。
- **收益率**：中等。这是大多数垂直领域微调的“甜蜜点”。

### 3. 边际效应递减（> 100k 条）

继续堆砌同质化数据，模型不再变聪明，只是在机械记忆。

- **表现**：Elo 分数增长停滞，甚至因为过拟合导致通用能力下降（Alignment Tax）。
- **警示信号**：当你发现**增加一倍的数据量，验证集 Loss 仅下降不到 1%** 时，SFT 就已经做到头了。

---

## 二、 为什么要切换到 RLHF / Preference Optimization？

SFT 的本质是**“模仿学习”**（Imitation Learning）。它的上限取决于标注员（Teacher）的水平。如果老师只能考 80 分，学生通过模仿撑死也只能考 80 分。

而 RLHF（特别是 2024-2025 年流行的 **Online DPO** 和 **PPO**）的本质是**“探索与强化”**。

### 1. 从 System 1 到 System 2（慢思考）

在 OpenAI o1 和 DeepSeek R1 发布的“推理时代”，SFT 只能教模型**“如何写出推理步骤的格式”**（System 1 直觉），而 RLHF 配合 **PRM (Process Reward Model)** 才能教模型**“每一步推理是否严谨”**（System 2 逻辑）。

### 2. 超越模仿

模型可以生成多种解法，只要最终结果对，就能获得奖励。这允许模型探索出人类未曾教过的更优路径（比如 AlphaGo 下出了人类看不懂的棋）。

### 3. 解决歧义与非配对数据

对于没有标准答案的任务（如创意写作），SFT 很难教。

- **DPO/SimPO**：通过成对比较（A 比 B 好）对齐人类审美。
- **KTO (Kahneman-Tversky Optimization)**：甚至不需要成对数据，只要告诉模型某个回答是“Good”还是“Bad”，利用人类的损失厌恶心理进行优化。这在工业界极其实用，因为“点赞/点踩”数据比“排序”数据更容易获取。

---

## 三、 决策指南：寻找最佳切换点

在实战中，我们可以通过以下量化指标来判断是否该切换技术路线：

### 1. 验证集 Loss 曲线变平 (Loss Plateau)

画出 SFT 的 Loss 曲线。当曲线斜率接近于 0，且持续 2-3 个 Epoch 没有显著下降时，说明模型已经榨干了当前数据的潜力。

### 2. 多样性崩塌 (Diversity Collapse)

计算模型输出的 Token 重复率或 N-gram 多样性。如果模型开始反复说车轱辘话，或者对不同提问输出千篇一律的回答，说明 SFT 过度了，急需 RLHF 来引入熵（Entropy）。

### 3. 拒绝采样效率 (Rejection Sampling Efficiency)

在“拒绝采样”阶段：

- 如果模型生成 100 个答案，只有 < 5 个能通过校验：**回退到 SFT**，基座太弱。
- 如果模型生成 100 个答案，有 60+ 个正确，但不知道哪个**最完美**：**启动 RLHF/DPO**，这是最佳介入时机。

---

## 四、 2025 年的最佳实践路线图 (Frontier Roadmap)

基于 DeepSeek V3、Llama 3.1 等前沿技术报告，目前的黄金配比建议如下：

### 阶段 1：Cold Start SFT (冷启动)

- **数据量**：10k - 50k 条高质量指令。
- **重点**：人工精修，确保无错漏。
- **目标**：教会模型“格式”和“基本规矩”。

### 阶段 2：Expert Iteration (专家迭代 / 拒绝采样)

- **方法**：用阶段 1 的模型生成数据，用规则或更强的模型（如 GPT-4）打分，保留高分数据。
- **作用**：这是一个 Bridge 阶段，通过“自我博弈”低成本扩充高质量数据，进一步压榨 SFT 潜力。

### 阶段 3：Online Preference Optimization (在线偏好优化)

- **技术选择**：
  - **Online DPO / Iterative DPO**：优于传统的 Offline DPO。模型在训练过程中不断生成新数据并进行自我对比，避免了分布偏移（Distribution Shift）。
  - **SimPO (Simple Preference Optimization)**：如果不希望维护复杂的 Reference Model，可以使用 SimPO，它更轻量且效果往往优于 DPO。
- **数据量**：通常是 SFT 数据的 5-10 倍（合成数据为主）。
- **目标**：从 60 分提升到 90 分，激发模型的上限。

---

## 五、 总结

不要试图用战术上的勤奋（无脑堆 SFT 数据）来掩盖战略上的懒惰（拒绝引入 RLHF）。

**SFT 是地基，RLHF 是摩天大楼。**
在 2025 年，随着 **Online DPO**、**KTO** 和 **PRM** 的普及，微调的天花板已经被大大抬高。当地基打好后，请果断切换赛道，用强化学习去探索未知的智能边界。

# 万卡集群的架构演进：当 Scaling Laws 撞上系统瓶颈

对于高级 AI 架构师而言，讨论“几百张卡的 3D 并行”已经是过去式了。当集群规模突破 **10,000 张 H100**，模型参数迈向 **万亿（Trillion）** 级别时，传统的 Megatron-LM 架构（TP+PP+DP）开始显露出疲态。

这不仅仅是硬件堆叠的问题，而是我们正在触碰分布式系统的物理极限。

本文将探讨在后 Scaling Laws 时代，为了支撑 MoE（混合专家模型）和超长上下文（Long Context），训练架构正在发生怎样的**范式转移**。

## 一、 传统 3D 并行的崩溃：为什么 TP 和 PP 不够用了？

在千卡时代，我们习惯了用 TP（张量并行）切分单机，用 PP（流水线并行）跨机。但在万卡时代，这套组合拳打不动了。

### 1. PP 的气泡（Bubble）是万卡之殇
流水线并行（Pipeline Parallelism）最大的原罪是**气泡**。
为了掩盖气泡，我们需要极高的 Micro-Batch 数量。但在万卡规模下，为了保证全局 Batch Size 不爆炸（影响收敛），每个 GPU 分到的 Micro-Batch 变得极少。
**结果**：气泡占比可能会飙升到 30% 甚至 50%，这意味着一半的 H100 都在空转等待。

### 2. TP 的通信墙
TP 依赖全归约（All-Reduce）。随着模型宽度增加，TP Size 必须增加，但这被限制在单机 8 卡 NVLink 内。一旦想跨机做 TP，以太网/IB 的延迟会让训练速度直接归零。

## 二、 新范式：EP + SP + Ring Attention

为了解决上述问题，业界（如 OpenAI, Google, Meta）正在转向一套新的并行范式。

### 1. MoE 带来的 Expert Parallelism (EP)
万亿参数模型如今多半是 MoE 架构（如 GPT-4, Mixtral）。MoE 天然适合分布式。
*   **原理**：不同的 Token 被路由到不同的“专家”（Expert）网络中。
*   **EP (专家并行)**：我们不再切分权重矩阵，而是**把不同的专家放在不同的 GPU 上**。
*   **优势**：EP 的通信只发生在路由阶段（All-to-All），计算阶段是完全独立的，没有任何通信开销。这极大地缓解了 TP 的压力。

### 2. 替代 TP 的 Sequence Parallelism (SP)
为了打破 TP 只能在单机内的限制，**序列并行（Sequence Parallelism）** 应运而生。
*   **原理**：与其切分模型权重（TP），不如切分输入数据（Sequence）。把一句话的 128k Token 切成 100 段，每张卡算一段。
*   **Ring Attention**：在计算 Attention 时，KV Cache 不需要在卡间传输，而是像接力棒一样在卡之间通过 Ring P2P 传递。
*   **优势**：这让我们可以训练 **无限长上下文**（1M+ Context），且通信和计算可以完美重叠（Overlap）。

## 三、 稳定性革命：从“小时级恢复”到“秒级无感”

在万卡集群中，MTBF（平均故障间隔时间）可能只有几小时。如果每次故障都要重启、加载 Checkpoint，训练效率会极低。

### 1. 异步 Checkpointing (Async Checkpointing)
传统的 Checkpoint 是同步的：所有人停下来，把显存数据写入磁盘。这会导致训练暂停几分钟。
**新技术**：利用 CPU 内存做缓冲。GPU 把参数 Dump 到 CPU 内存（速度极快，几秒钟），然后 GPU 继续训练。后台由 CPU 慢慢把数据写入磁盘。

### 2. 冗余计算与即时切换
对于 MoE 架构，如果某个 Expert 所在的 GPU 挂了，我们甚至不需要停机。
*   **策略**：系统中保留少量“热备”节点。一旦发现故障，调度器直接将该 Expert 的计算任务路由到热备节点，或者由其他存活节点分担（虽然会慢一点，但训练不会断）。

## 结语

从“千亿”到“万亿”，从“千卡”到“万卡”，这不仅是数量的量变，更是架构的质变。

高级 AI 工程师的战场，已经从单纯的 PyTorch 代码，转移到了 **CUDA Kernel 优化**、**网络拓扑感知调度**以及**分布式容错系统**的深水区。

在这个层级，没有任何现成的 `pip install` 能解决问题，你编写的每一行代码，都在定义 AI 基础设施的未来标准。

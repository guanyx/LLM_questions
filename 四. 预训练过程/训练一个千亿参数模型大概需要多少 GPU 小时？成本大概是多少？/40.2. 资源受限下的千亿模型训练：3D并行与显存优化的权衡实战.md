# 资源受限下的千亿模型训练：3D并行与显存优化的权衡实战

对于中级 AI 工程师而言，知道“千亿模型需要数千张卡”只是一个背景知识，他们更痛苦的现实往往是：**老板只给了 64 张 A800，甚至更少，却让你把一个 70B 或 100B 的模型跑起来，还要尽可能快。**

这就不是一道简单的算术题了，而是一场在**显存（Memory）**、**计算（Compute）**和**通信（Communication）**这三者之间走钢丝的艺术。

本文将深入探讨在资源受限场景下，如何通过精细化的 **3D 并行策略**和**显存优化技术**，榨干硬件的每一滴性能。

## 一、 3D 并行策略：网络拓扑的“洋葱模型”

当模型大到单卡塞不下时，我们必须切分模型。主流的切分方式有三种，它们对通信带宽的要求截然不同，这决定了它们在物理集群中的摆放位置。

### 1. TP (Tensor Parallel)：张量并行
*   **原理**：把矩阵乘法切开，每张卡算一部分。
*   **特点**：**通信量极大**。每次矩阵运算都需要同步，每层都要通信。
*   **部署原则**：**绝对不要跨机！** TP 必须限制在单台服务器内部（Intra-node），利用 NVLink（600GB/s+）的高带宽。一旦跨机走以太网，速度会跌到无法接受。
*   **推荐配置**：TP Size 通常设为 4 或 8（即占满一台机器的卡）。

### 2. PP (Pipeline Parallel)：流水线并行
*   **原理**：把模型的层切开，卡 1 跑前 10 层，卡 2 跑后 10 层。
*   **特点**：**通信量较小**。只在切分点传递 Activation，通信频率低。但有“气泡”问题（Pipeline Bubbles），部分 GPU 会空转。
*   **部署原则**：**适合跨机**。当 TP 占满一台机器后，如果模型还装不下，就用 PP 跨机器连接。
*   **推荐配置**：PP Size 越小越好，只要显存够用，尽量少用 PP，以减少气泡损耗。

### 3. DP (Data Parallel)：数据并行
*   **原理**：复制多份模型，每份跑不同的数据。
*   **特点**：**通信量中等**。只在反向传播结束时同步梯度。
*   **部署原则**：**最外层**。当 TP 和 PP 把模型塞进显存后，剩下的显存和卡全部用来做 DP，以增大 Batch Size 加速训练。

### **最佳实践总结（洋葱模型）**
假设你有 4 台机器，每台 8 张卡（共 32 卡）：
1.  **最内层（单机内）**：开启 **TP=8**。利用 NVLink 搞定高频通信。
2.  **中间层（跨机）**：如果 8 卡装不下模型，开启 **PP=2**（跨 2 台机器）。
3.  **最外层**：剩余的机器做 **DP**。

## 二、 显存优化：空间换时间的权衡

如果 3D 并行切完，显存还是 OOM（Out of Memory）怎么办？这时候就需要牺牲一点计算速度来换取显存空间。

这里有三个从“轻微”到“激进”的手段：

### 1. Gradient Checkpointing（梯度检查点）
*   **原理**：在前向传播时不保存所有的中间激活值（Activation），只保存几个关键节点的。在反向传播需要用到时，**临时重新计算**出来。
*   **代价**：计算量增加约 30%（因为多算了一遍前向）。
*   **收益**：显存占用通常能砍掉 **60%-70%**。
*   **决策**：**几乎是千亿模型的标配**。那 30% 的计算代价相比于显存节省带来的大 Batch Size 收益，通常是划算的。

### 2. ZeRO (Zero Redundancy Optimizer)
DeepSpeed 的核心绝技，切分优化器状态（Optimizer States）、梯度（Gradients）和参数（Parameters）。
*   **ZeRO-1**：只切优化器状态。**收益极大，通信代价极小**。无脑开。
*   **ZeRO-2**：切优化器+梯度。**收益较大，通信代价中等**。推荐开。
*   **ZeRO-3**：切优化器+梯度+模型参数。
    *   **原理**：每做一次计算，都要临时去别的卡拉取参数，算完马上扔掉。
    *   **代价**：**通信量爆炸**。如果你网络慢（比如只有 100Gbps RoCE 甚至 TCP），ZeRO-3 会让你的训练慢如蜗牛。
    *   **决策**：**不到万不得已（OOM），不要开 ZeRO-3**。

### 3. Offload（卸载）
*   **原理**：显存不够，内存来凑。把优化器状态甚至参数搬到 CPU 的内存（RAM）里，计算时再通过 PCIe 搬回来。
*   **代价**：**极慢**。PCIe 的带宽和 NVLink 差了几个数量级。
*   **决策**：**这是“救护车”策略**。只有当你必须在一张卡上跑一个超大模型做微调（比如 LoRA），且完全不在乎时间时才用。正经的大规模预训练尽量避免。

## 三、 实战中的“调参”心法

作为中级工程师，面对一个新集群，可以遵循以下 **“显存-速度贪心算法”**：

1.  **起手式**：
    *   TP = 单机卡数（如 8）。
    *   PP = 1（不用流水线）。
    *   ZeRO-1 开启。
    *   Gradient Checkpointing 开启。
    *   尝试跑通一个小 Batch Size。

2.  **如果 OOM（显存爆了）**：
    *   **第一步**：开 ZeRO-2。
    *   **第二步**：如果还爆，开 PP=2（跨机）。注意此时要调整 Micro-Batch Size 以减少气泡。
    *   **第三步**：如果还爆，开 ZeRO-3（做好通信瓶颈的心理准备）。

3.  **如果显存很富裕，但速度慢**：
    *   **检查 TP**：是否跨机了？如果是，降低 TP，增加 PP。
    *   **检查 PP**：气泡是否太大？尝试增加 Micro-Batch Size 或减少 PP Stage。
    *   **检查 ZeRO**：是不是开了 Offload？关掉它。是不是开了 ZeRO-3 但网络跟不上？降级到 ZeRO-2。

## 结语

在资源受限的环境下训练大模型，本质上是在解一道**约束优化问题**。
没有一套参数是“天下无敌”的，只有最适合当前硬件拓扑的组合。中级工程师的价值，就体现在能通过 `nvidia-smi` 和 `nsys` 的波形图，快速定位是卡在计算、显存还是通信上，并给出正确的交换策略。

# 我试着算了一下：训练一个千亿参数模型，到底要烧掉多少 GPU 小时？

我最近经常被问到一个问题：

“如果我也想训练一个 100B（千亿）参数的模型，大概需要多久？要花多少钱？”

我每次都想反问一句：你是想听一个“震撼数字”，还是想要一个能拿去做预算/做方案的粗略心算方法？

这篇文章我就用后者来写：不追求精确到小数点后两位（那是不可能的），但希望你读完后，能在纸上自己算出一个大概范围，并且知道钱到底花到哪里去了。

## TL;DR（先把结论放在前面）

- 100B 模型通常需要大约 2T token 训练数据（一个常见经验是 token 数 ≈ 参数量的 20 倍）
- 训练一次的数量级大概是：A100 约 200–300 万 GPU 小时；H100 约 70–100 万 GPU 小时
- 你真正付的钱不只是“GPU 租金”，还有故障重跑、网络互联、电力散热、以及一整支随时救火的系统工程团队
- 为什么这么贵：千卡以上集群的有效算力（MFU）常常只有 30%–50%

## 先说清楚两个词：参数、GPU 小时

### “千亿参数”到底是什么意思？

把模型想成一个有 1000 亿个可调旋钮的机器。训练就是让它读很多文本，然后不停地把这些旋钮往更合适的位置拧一点点。

这里最容易忽略的一点是：训练不是“把旋钮拧一遍就完事”。每读一小段 token，模型都要做一次前向+反向，把很多旋钮都参与到计算里。所以参数越多，每一步越贵；数据越多，步数越多。

### “GPU 小时”为什么好用？

因为“训练多久”这个问题，取决于你有多少张卡。

- 1 张 GPU 跑 100 小时 = 100 GPU 小时
- 100 张 GPU 跑 1 小时 = 100 GPU 小时

所以 GPU 小时更像“总工作量”。你可以用它来做预算，也可以用它来和别人对齐口径。

## 我怎么心算训练量：模型大小 × 数据量 × 有效利用率

我通常用三个抓手：

1. 模型有多大（100B）
2. 数据有多少（token）
3. 你能把 GPU 的峰值算力用到多少（MFU）

### 1) 数据量：为什么大家总说要 20× token？

有一个很常见的经验（来自 scaling law 的实践直觉）：想让模型“别太虚”，训练 token 往往在参数量的 10–30 倍之间。

所以 100B 参数模型，一个很常见的起步数字是：

- 2T token（也就是 2 万亿）

这个数字不是圣旨，但它能把量级拉到正确的地方。

### 2) MFU：你买来的算力，有一半经常在“等”

显卡说明书上的峰值算力很漂亮，但现实训练里，你通常只能用到 30%–50%。这个比例有个名字：MFU（Model FLOPS Utilization）。

如果你觉得“怎么会这么低”，别担心，后面我会解释“消失的算力去哪儿了”。

### 3) 结果：100B 训练一次大概多少 GPU 小时？

把上面的直觉塞进工程现实里（尤其是 MFU），你会得到一个挺常见的估算范围：

- A100：约 200–300 万 GPU 小时
- H100：约 70–100 万 GPU 小时（大致快 2–3 倍，具体看配置和实现）

这意味着什么？

如果你手上有 1024 张 A100，粗略说就是连续跑 3–4 个月量级（中间还不能怎么掉链子）。

## “钱”到底花在了哪里？（我喜欢把它叫做训练税）

很多人算成本时会停在这一步：

“300 万 GPU 小时 × 每小时 2–4 美元 = 几百万到一千多万美元。”

这个算式没错，但它只算了“台面上的钱”。真正让人肉疼的是训练税：你不想付，但它总会来。

我一般会把训练税分成四类：

1. 故障税  
   千卡集群连续跑几个月，硬件故障、网络抖动、作业被抢占、训练发散……都不是意外，是日常。你会做 checkpoint，会重跑，会回滚。10%–20% 的损耗很常见。

2. 通信税  
   几千张卡要一起干活，它们需要一张极贵的“嘴”和“耳朵”：IB/RoCE、交换机、光模块、网卡。这部分在自建集群里非常显性；在云上，它往往被打包进“GPU 单价”里。

3. 电力与散热税  
   这个在账面上可能不如 GPU 显眼，但它会在规模上来之后变得很讨厌（尤其还会带来合规和碳排的压力）。

4. 人力税  
   我见过很多人忽略这一条：要把 30% 的 MFU 拉到 45%，你需要的人可能不是“再加两个训练脚本工程师”，而是懂通信、懂 CUDA、懂拓扑、懂容错的一整套系统团队。

## 消失的算力去哪了？（为什么 MFU 只有 30%–50%）

我喜欢把它想象成三堵墙。每堵墙都在说同一句话：你以为 GPU 在算，其实它在等。

### 1) 通信墙：大家在开会

数据并行时，反向传播结束要 all-reduce 梯度；模型并行时，每一层可能都要传激活值/梯度。

想象 1000 个人一起写一份文档：每写两段就必须同步一次，否则版本会打架。同步本身不产生任何“新内容”，但不做又不行。

### 2) 显存墙：搬数据比算更慢

GPU 的算力涨得很快，显存带宽没跟上。结果是：很多时间花在“把数据搬到该去的地方”，而不是“做矩阵乘法”。

如果你做过性能分析，会看到一个很朴素的事实：你的瓶颈可能不是 FLOPS，而是 bandwidth。

### 3) 气泡墙：流水线里总会有空位

当你用 PP（流水线并行）把模型切成几段，总会遇到启动/收尾阶段的空转，以及 micro-batch 不够多带来的“流水线没填满”。

我脑子里常常是这样的时间轴：

```
GPU1: [算][算][算][算][算][   ][   ]
GPU2: [   ][算][算][算][算][算][   ]
GPU3: [   ][   ][算][算][算][算][算]
```

那一堆空格，就是你付了钱但没买到计算的部分。

## 如果你只有 64 张卡，还想跑 70B/100B：该怎么选并行和显存策略？

这部分我喜欢用一个“洋葱模型”来记：

- 最内层：TP（尽量只在单机内，吃 NVLink）
- 中间层：PP（实在放不下再跨机切层，尽量少）
- 最外层：DP（把剩下的卡用来喂更大的 batch/更多数据）

显存不够怎么办？我一般按这个顺序尝试（从“最不痛”到“最痛”）：

1. 开 gradient checkpointing（很常见，也很划算）
2. 开 ZeRO-1/ZeRO-2（省显存的性价比不错）
3. 必要时上 PP（但盯紧气泡）
4. 不到万不得已别上 ZeRO-3 / offload（它们会把通信/PCIe 成本拉到你怀疑人生）

## 当你有 1 万张卡：传统 TP+PP+DP 开始难受了

在万卡规模，老套路会越来越容易踩到系统极限：

- PP 的气泡会变得很显眼（micro-batch 很难堆起来）
- TP 很难跨机扩展（通信墙会把你按在地上摩擦）

所以大家开始换思路：

- MoE + EP（专家并行）：把不同专家放到不同 GPU，上专家的时候算得很爽，下专家的时候做 all-to-all
- SP（序列并行）+ ring attention：与其切权重，不如切序列，让超长上下文更可控

还有一个很现实的问题：万卡意味着“随时会坏点什么”。所以稳定性变成性能的一部分：

- 异步 checkpointing（不要每次存盘都让全世界停下来）
- 热备/冗余（坏一个节点不要全盘重来）

## Scaling Laws 触顶之后：下一波“增长点”可能在哪里？

如果你问我“堆更多卡是不是永远有效”，我的答案是：它会越来越不优雅。

我更感兴趣的三个方向是：

1. 合成数据  
   让模型自己产数据，但要有强验证器（数学、代码、可执行的规则），不然就会越吃越垃圾。

2. 推理时计算（test-time compute）  
   训练时把模型做得更会“快反”是一条路；推理时让它多想一会儿（搜索、反思、自我纠错）是另一条路，而且经常更划算。

3. 新架构  
   Transformer 的 $O(N^2)$ 注意力对长上下文太贵了。线性注意力/SSM 这些方向，至少在工程上很值得继续押注。

## 最后：如果你只想记住三件事

1. 100B 模型训练是“百万 GPU 小时”量级的工程，不是“多租两台机器”
2. MFU 低不是你不努力，是系统现实：通信、带宽、气泡总要付税
3. 真正的护城河往往不在“把模型跑起来”，而在“把它稳定、便宜、持续地跑下去”

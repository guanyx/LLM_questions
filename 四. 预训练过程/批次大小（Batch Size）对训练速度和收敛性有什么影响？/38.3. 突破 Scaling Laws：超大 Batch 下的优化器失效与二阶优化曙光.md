# 38.3. 突破 Scaling Laws：超大 Batch 下的优化器失效与二阶优化曙光

### 高级工程师的深思

> “在 GPT-4 级别的超大规模预训练中，我们已经不再纠结显存够不够用，而是纠结**优化算法能不能跟上**。
>
> 理论上，随着 Loss 的下降，梯度中的信号噪声比（SNR）会变低，这意味着我们需要更大的 Batch Size 来维持梯度的有效性。OpenAI 的研究表明，**临界批量大小（Critical Batch Size）** 会随着训练进行而动态增长，后期甚至需要百万级（Millions of Tokens）的 Batch。
>
> **但问题来了：当我们真的把 Batch 开到这么大时，AdamW 似乎不动了。**
>
> 简单的一阶优化器（First-Order Optimizers）在面对如此平滑但高维的曲面时，效率极低。我们是否到了必须引入**二阶优化（Second-Order Optimization）**，或者像 **Shampoo / K-FAC** 这种预处理梯度算法的临界点？这是打破 Scaling Laws 边际效应的唯一出路吗？”

这是一个只有站在算力金字塔顶端的工程师才会思考的问题。它关乎 AI 基础设施的未来。

---

### 1. 为什么后期需要超大 Batch？——梯度噪声尺度（Gradient Noise Scale）

要理解这个问题，我们必须引入一个核心概念：**梯度噪声尺度（Gradient Noise Scale, $B_{noise}$）**。

$$ B_{noise} \approx \frac{\text{Trace}(\Sigma)}{\|G\|^2} $$

其中 $\Sigma$ 是梯度的协方差矩阵，$\|G\|^2$ 是梯度的范数。

*   **直观理解**：$B_{noise}$ 衡量了梯度的“混乱程度”。
    *   训练初期，模型什么都不会，梯度方向非常明确（“往那边走！”），$B_{noise}$ 很小。
    *   **训练后期**，模型已经学得差不多了，梯度方向变得微弱且杂乱（“好像是往那边挪一点点？”），**$B_{noise}$ 会变得极其巨大**。

**Scaling Laws 的推论**：为了压制后期巨大的噪声，我们需要指数级增加 Batch Size。只有当 $B > B_{noise}$ 时，我们的一步更新才是有意义的“信号”，否则全是“噪声”。

---

### 2. AdamW 的失效：一阶优化器的天花板

当我们真的按照理论，在训练后期把 Batch Size 拉大到 4M 甚至 10M tokens 时，工程上会出现一个诡异的现象：**Loss 不降了，或者降得极慢，仿佛算力被黑洞吞噬了。**

这是因为 **AdamW 本质上还是一个一阶优化器**。

*   **曲率信息的缺失**：一阶优化器只知道“当前坡度是多少”，不知道“坡度变化率（曲率）是多少”。
*   **大 Batch 的几何困境**：在大 Batch 下，损失函数的几何曲面被极度“平滑化”了。对于一阶优化器来说，这就像在一个一望无际的平原上寻找一个微小的凹坑。它失去了对“地形复杂性”的感知，每一步走得要么太保守（收敛慢），要么太激进（震荡）。

这就是为什么在大 Batch 训练中，单纯增加学习率往往导致发散，而保持学习率又导致收敛停滞。

---

### 3. 二阶优化的曙光：从 K-FAC 到 Shampoo

为了解决这个问题，学术界和工业界开始重新审视**二阶优化（Second-Order Optimization）**。

二阶优化的核心是利用 **Hessian 矩阵（二阶导数矩阵）** 来指导更新。它不仅知道坡度，还知道“地形的弯曲程度”。如果说 Adam 是走直线的盲人，二阶优化就是自带地图的向导。

但是，计算完整的 Hessian 矩阵（参数量平方级别，$N^2$）在显存上是不可能的。因此，**近似二阶算法**成为了救星：

#### A. K-FAC (Kronecker-Factored Approximate Curvature)
*   **原理**：假设 Hessian 矩阵可以用两个小矩阵的 Kronecker 积来近似。
*   **优势**：在卷积神经网络（CNN）时代曾大放异彩，能显著减少收敛所需的 Step 数。
*   **现状**：在 Transformer 架构上，由于 Attention 层的特殊性，K-FAC 的近似效果不如 CNN 好，且实现极其复杂。

#### B. Shampoo (Preconditioned Gradient)
*   **原理**：这是 Google 目前力推的方案（用于训练 PaLM 等大模型）。它不对 Hessian 建模，而是直接对梯度进行**预处理（Preconditioning）**。它利用矩阵的奇异值分解（SVD）或逆根（Inverse Root）来矫正梯度的方向。
*   **战绩**：在超大 Batch（32k+）设置下，Shampoo 能够比 AdamW 节省 **40% 以上的训练步数**。
*   **代价**：计算逆矩阵非常耗时（虽然是在低维空间做），通常需要通过“每 N 步计算一次逆矩阵”来摊薄开销。

---

### 4. 未来展望：Soap 还是 Lion？

除了重型二阶优化，还有一条路是**“魔改一阶”**。

*   **Lion (Evolved Sign Momentum)**：Google 提出的“符号优化器”，完全抛弃了二阶矩估计，只看梯度的符号（Sign）。
    *   **大 Batch 表现**：惊人地好。Lion 在大 Batch 下比 AdamW 更稳定，且显存占用更少。它证明了也许我们不需要更复杂的二阶信息，只需要更纯粹的方向。

*   **Soap (Spectrum-Aware Optimization)**：最新的研究方向，试图结合 Shampoo 的特征值矫正和 Adam 的动量机制。

### 总结

对于高级工程师而言，Scaling Laws 告诉我们：**“只要 Batch 够大，收敛就能无限快。”**
但现实的优化器告诉我们：**“Batch 太大，我就不会走路了。”**

目前的战局是：
1.  **保守派**：坚持 AdamW，通过动态调整 Batch Size 和 Learning Rate Schedule 来勉强维持。
2.  **革新派**：全面转向 **Shampoo** 或 **Lion**，试图在超大 Batch（百万级 Token）的无人区开辟新航路。

**结论**：如果你正在训练 GPT-5 级别的模型，研究优化器算法的收益，可能远高于多买 1000 张 H100。

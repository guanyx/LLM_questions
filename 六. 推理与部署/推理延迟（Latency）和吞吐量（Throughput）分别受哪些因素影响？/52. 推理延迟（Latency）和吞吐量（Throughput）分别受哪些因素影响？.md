# 52. 推理延迟（Latency）和吞吐量（Throughput）分别受哪些因素影响？

在探讨大模型（LLM）的性能时，我们经常听到两个词：**延迟（Latency）**和**吞吐量（Throughput）**。很多人容易将它们混为一谈，认为“速度快”就是一切。但实际上，它们描述的是系统性能的两个完全不同、甚至在某些场景下相互制约的维度。

这就好比评价一家餐厅，你是更在意“上菜快不快”（延迟），还是更在意这家餐厅“每天能接待多少客人”（吞吐量）？

本文将避开复杂的数学公式和代码，从直观的原理出发，层层拆解这两个核心指标背后的影响因素。

---

## 一、 基础科普：从“网红餐厅”说起

为了把概念讲透，我们继续沿用餐厅的例子。

### 1. 什么是延迟（Latency）？

**延迟**，对于用户来说，就是**等待的时间**。
在 LLM 的语境下，它通常指的是从你点击“发送”按钮，到看到 AI 打出第一个字，或者 AI 完成整个回答所需要的时间。

- **首字延迟 (TTFT)**：就像你点完菜，服务员端上来第一盘开胃菜的时间。这直接决定了你觉得这家餐厅“响不响应”。
- **整体延迟**：就像你吃完这顿饭总共花的时间。

### 2. 什么是吞吐量（Throughput）？

**吞吐量**，关注的是**系统的承载能力**。
它是指系统在单位时间内能生成多少个 Token（字/词）。
这就像餐厅的后厨，不管前厅有多少客人排队，后厨一小时能炒出多少份菜，这就是它的吞吐量。

### 3. 核心差异

**延迟关注的是“个体体验”，吞吐量关注的是“宏观效率”。**
如果餐厅只有一个厨师，为了让某个 VIP 客户（你）最快吃上饭（低延迟），他必须停下手里其他人的单子专心为你服务。但这样做，整个餐厅一晚上卖出的菜（吞吐量）就会变少。反之，如果厨师把大家的菜攒在一起炒（高吞吐），每个人的等待时间（延迟）可能就会变长。

---

## 二、 技术进阶：拆解“厨房”的运作流程

理解了基本概念，我们来看看究竟是什么在拖慢“上菜速度”或限制“出餐量”。影响因素主要可以分为三大类：硬件、模型本身、以及系统策略。

### 1. 硬件层面：算力与带宽的博弈

这是最底层的物理限制，相当于厨师的切菜速度和传菜员的走路速度。

- **算力（Compute）**：显卡（GPU）的计算速度。这决定了厨师切菜、炒菜动作有多快。在处理用户的输入（Prompt）阶段，因为是并行计算，算力往往是瓶颈。
- **显存带宽（Memory Bandwidth）**：这是最容易被忽视的因素。模型在生成回答时，需要不断地从显存中读取参数。**带宽就是“传菜员”的速度**。在大模型推理中，大部分时间（尤其是生成阶段）显卡都在等数据从显存搬运到计算单元。**由于“内存墙”的存在，显卡算力再强，如果带宽不够，厨师也得等着米下锅。**

### 2. 模型层面：体量与记忆的负担

- **模型参数量（Model Size）**：模型越大，参数越多，需要搬运的数据量就越大。70B 的模型比 7B 的模型慢，不仅是因为计算量大，更是因为要把庞大的参数在显存和计算单元之间搬来搬去。
- **上下文长度（Context Length）与 KV Cache**：LLM 生成每一个字，都要回顾之前的对话历史。为了不重复计算，系统会把历史状态存下来，这就叫 KV Cache。随着对话越来越长，这个“记忆缓存”会越来越大，不仅占用显存，读取它所消耗的带宽也会增加，从而拖慢速度。

### 3. 系统策略：批量处理（Batching）的艺术

这是软件层面的调度优化。

- **Batch Size（批处理大小）**：就像厨师是决定“一份一份炒”还是“一锅炒十份”。
  - **增大 Batch Size**：可以显著提升**吞吐量**，因为读取一次模型参数可以同时服务多个用户，均摊了“搬运成本”。
  - **减小 Batch Size**：通常有利于降低**延迟**，因为不需要等待凑齐一桌人才开席。

---

## 三、 深度剖析：隐藏在数字背后的“二律背反”

在实际的生产环境中，问题往往比简单的“快慢”更复杂。我们需要深入到推理的两个阶段来理解。

### 1. Prefill（预填充）与 Decode（解码）的分裂人格

LLM 的推理过程其实是分裂的，分为两个截然不同的阶段，它们对性能的诉求完全不同：

- **阶段一：Prefill（阅读理解阶段）**

  - **过程**：模型一次性读入用户发的所有 Prompt。
  - **特性**：这是一个**计算密集型（Compute-bound）**的任务。因为所有输入的 Token 可以并行计算，矩阵乘法满负荷运行。
  - **瓶颈**：主要受限于 **GPU 的算力（FLOPS）**。
  - **影响**：直接决定了 **TTFT（首字延迟）**。

- **阶段二：Decode（逐字生成阶段）**
  - **过程**：模型一个字一个字地往外蹦。
  - **特性**：这是一个**内存带宽密集型（Memory-bound）**的任务。每生成一个字，都要把几十 GB 的模型参数完整过一遍，但计算量却相对很小。
  - **瓶颈**：主要受限于 **显存带宽（Memory Bandwidth）**。
  - **影响**：直接决定了 **TPOT（每个 Token 的生成时间）**，即用户感觉到的“打字速度”。

### 2. 延迟与吞吐量的 Trade-off（权衡）

在资源有限的情况下，延迟和吞吐量往往是矛盾的。

- **追求极致延迟**：我们需要让 Batch Size 尽可能小，甚至为 1。这意味着显卡每次辛辛苦苦把几十 GB 的参数搬出来，只为了生成一个字，利用率极低，吞吐量极差。
- **追求极致吞吐**：我们将 Batch Size 设得很大，显卡满载运行。但新来的请求必须排队，或者等待当前批次处理完，导致单个用户的等待时间（延迟）飙升。

### 3. “内存墙”：不可忽视的物理极限

为什么现在的推理加速这么难？核心痛点在于**计算速度的增长远远超过了内存传输速度的增长**。
现在的 GPU 算力强得惊人，但在推理阶段（Decode），它们大部分时间都在“摸鱼”，等待数据从显存传过来。
因此，**提升推理速度（尤其是生成速度），往往不是靠买算力更强的卡，而是靠买带宽更大的卡（如 HBM 显存）。** 这也是为什么量化（Quantization，把 FP16 变成 Int8 或 Int4）技术如此重要——它本质上是通过降低精度来减少数据搬运量，从而缓解带宽压力，提升速度。

---

## 四、 结语：如何选择你的“战场”？

了解了延迟和吞吐量的影响因素，我们在实际应用中就有了判断依据：

1.  **如果你做的是实时对话机器人（Chatbot）**：

    - **首要目标**：低延迟（特别是低 TTFT）。
    - **策略**：用户对“卡顿”极度敏感。你可能需要较小的 Batch Size，或者使用推测解码（Speculative Decoding）等技术来降低延迟。

2.  **如果你做的是后台批量处理（如文章摘要、离线翻译）**：
    - **首要目标**：高吞吐量。
    - **策略**：用户不在乎是 1 秒还是 5 秒出结果，只在乎一小时能处理多少万字。你可以拉大 Batch Size，榨干显卡的每一滴显存和算力，实现成本效益最大化。

**总结来说，推理性能优化不是一味的“快”，而是在算力、带宽、显存和用户体验之间，寻找那个最完美的平衡点。**

# 52.2. 为什么 Static Batching 不够用？聊聊 Continuous Batching 的魔法

在上文中，我们提到了 **Batch Size（批处理大小）** 是平衡延迟和吞吐量的关键杠杆。
但细心的读者（尤其是做工程落地的同学）可能会发现一个漏洞：
> “文章里说的 Batching 好像是静态的？大家一起上车，一起发车。
> 但现实中，有的人问‘你好’（2个字），有的人问‘请写一篇800字的作文’（800个字）。
> 如果强行把他们凑成一个 Batch，短请求岂不是要陪着长请求干等？”

没错！这就是 **Static Batching（静态批处理）** 的致命伤。为了解决这个问题，业界引入了一项革命性的技术——**Continuous Batching（连续批处理，也叫 Iteration-level Batching）**。

今天我们就来拆解一下，它是如何打破“二律背反”，让延迟和吞吐量双双起飞的。

---

## 一、 传统 Static Batching 的“木桶效应”

想象一辆公交车（GPU），它必须等车上所有乘客（Request）都到站了，才能返程去接下一波人。

### 1. 场景模拟
假设我们设定的 Batch Size = 4，但这 4 个请求的生成长度截然不同：
*   **请求 A**：生成 10 个 Token（短）
*   **请求 B**：生成 20 个 Token
*   **请求 C**：生成 50 个 Token
*   **请求 D**：生成 100 个 Token（长）

### 2. 尴尬的现状
*   **请求 A 的悲剧**：它在第 10 轮就生成完了，本该立刻返回给用户。但因为必须和请求 D 保持同步，它被迫在显存里“空转”了 90 轮，直到请求 D 跑完。
*   **资源的浪费**：在第 11 轮到第 100 轮之间，GPU 的计算单元其实只服务了剩下的请求，原本属于 A 的位置空着，却没法塞进新的人。

这就是典型的 **“尾部延迟”（Tail Latency）** 问题。**整个 Batch 的耗时，取决于最慢的那个请求。**

---

## 二、 Continuous Batching：随进随出的“旋转门”

Continuous Batching 的核心思想是：**不再以“整条请求”为单位调度，而是以“一次迭代（Iteration）”为单位调度。**

这就好比把公交车换成了**旋转门**或**流水线**。

### 1. 运作机制
还是上面的例子，Continuous Batching 是这样工作的：
1.  **第 10 轮**：请求 A 生成完毕。
2.  **立刻释放**：系统立刻把请求 A 的结果返回给用户（延迟极低）。
3.  **无缝衔接**：系统发现 Batch 里空出了一个“槽位”，立刻从排队队列里拉入一个新的 **请求 E**。
4.  **继续运行**：在第 11 轮，GPU 处理的是 {E, B, C, D} 的生成任务。

### 2. 核心优势
*   **消灭空转**：GPU 永远处于满载状态，不会因为某个请求结束了就在那儿“摸鱼”。
*   **降低延迟**：短请求一旦生成完立刻返回，不用陪跑。
*   **提升吞吐**：因为不断有新请求填补空缺，单位时间内处理的 Token 总量大幅增加。

---

## 三、 技术原理：它是如何做到的？

听起来很美好，但实现起来非常复杂。因为它要求推理引擎必须具备极强的**显存管理能力**。

### 1. 这里的难点在哪？
在 Static Batching 中，每个请求占用的显存空间是预先分配好的（通常是连续的）。
但在 Continuous Batching 中，新来的请求 E 可能很长，而刚走的请求 A 可能很短，原来的“坑位”可能塞不下 E。

### 2. PagedAttention 的神助攻
这就不得不提 **vLLM** 框架带火的 **PagedAttention** 技术。
它借鉴了操作系统中“虚拟内存”和“分页”的概念：
*   把 KV Cache 切分成一个个小的 **Block（块）**。
*   这些 Block 在显存里可以是**不连续**的。
*   系统维护一张“页表”，记录每个请求的数据分散在哪些 Block 里。

**有了 PagedAttention，Continuous Batching 才能真正落地：**
新来的请求 E 不需要连续的大块显存，它可以见缝插针地塞进任何零散的空闲 Block 里。这就彻底解决了显存碎片化的问题，让“随进随出”成为可能。

---

## 四、 总结：从“公交车”到“流水线”

如果说 Static Batching 是传统的**公交车模式**（人齐发车，同进同出），那么 Continuous Batching 就是现代化的**工厂流水线模式**。

*   **对于用户**：不再需要为别人的长问题买单，自己的回答生成完就能立刻看到。
*   **对于开发者**：显卡利用率（GPU Utilization）被榨干到了极致，同样的硬件能支撑更高的 QPS。

这也是为什么现在主流的推理框架（如 **vLLM, TGI, TensorRT-LLM**）都默认标配了 Continuous Batching。作为中级工程师，在进行推理服务优化时，这应该是你工具箱里的首选武器。

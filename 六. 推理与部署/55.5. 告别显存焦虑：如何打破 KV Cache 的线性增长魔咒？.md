# 告别显存焦虑：如何打破 KV Cache 的线性增长魔咒？

> 显存不够用，加卡；卡不够用，量化。但当上下文长度（Context Length）拉长到 128k 甚至 1M 时，你会发现：**KV Cache 成了那个永远填不满的无底洞。**
> 对于一个 70B 模型，128k 上下文的 KV Cache 可能比模型本身还要大。

如果你是专家，你一定不会满足于“买更多显卡”。
本文将介绍三种前沿技术，它们试图在**不显著牺牲精度**的前提下，让 KV Cache 变小、变小、再变小。

---

## 第一种流派：只留精华——KV Cache 驱逐算法 (Eviction)

**核心思想**：大部分 Token 都是废话，只有少数 Token 是关键的。为什么不把那些没用的 KV 扔掉？

### 1. StreamingLLM (Attention Sink)
*   **发现**：研究人员发现，如果直接扔掉早期的 Token，模型会立刻崩掉。但只要保留**最初的几个 Token (Attention Sink)** 和**最近的几个 Token (Local Window)**，模型就能正常工作，哪怕中间扔掉几万个 Token 也没事。
*   **做法**：只存头部 4 个 KV + 尾部 256 个 KV。
*   **效果**：KV Cache 变成**常数级**（不再随长度增长）。你可以无限对话下去，显存永远不涨。
*   **缺点**：对于需要“回看”中间细节的任务（比如“总结第 5000 字说了什么”），它会歇菜。

### 2. H2O (Heavy Hitter Oracle)
*   **思想**：不仅仅保留首尾，而是动态筛选出那些“被关注最多”的 Token（Heavy Hitters）。
*   **做法**：实时统计 Attention Score，分数低的 KV 直接踢出显存。
*   **效果**：显存节省 80%，精度损失很小。

---

## 第二种流派：压缩再压缩——KV Cache 量化 (Quantization)

**核心思想**：模型权重可以量化到 INT4，为什么 KV Cache 不行？

### 1. KV Cache INT8 / INT4
*   **原理**：将 FP16 的 KV 向量压缩成 8 位或 4 位整数。
*   **挑战**：KV 的数值分布比权重更离群（Outliers），直接量化精度掉得厉害。
*   **解法**：使用 **Per-Channel** 或 **Group-wise** 量化，或者保留离群值的 FP16 精度。
*   **战果**：显存直接减半（FP16 -> INT8），配合 PagedAttention 使用效果更佳。

### 2. KIVI (2-bit KV Cache)
*   **黑科技**：号称能把 KV Cache 压缩到 **2-bit**。
*   **手段**：利用 KV 矩阵的稀疏性，只存关键特征。虽然极端，但在超长文本下是救命稻草。

---

## 第三种流派：改变架构——根本不需要 Cache

**核心思想**：既然 Attention 机制导致了 KV Cache 爆炸，那我不用 Attention 了行不行？

### 1. Mamba / RWKV (RNN 复活)
*   **原理**：回归 RNN 的循环机制，把所有历史信息压缩进一个固定大小的 **State (状态)**。
*   **神迹**：不管上下文是 1k 还是 100k，推理时的显存占用**恒定不变**（且极小）。
*   **代价**：在“大海捞针”（Passkey Retrieval）等任务上，目前还不如 Transformer 稳健。

### 2. Ring Attention
*   **原理**：这不是压缩，而是分布式。把 KV Cache 切碎了分布在几百个 GPU 上，像接力赛一样传递计算。
*   **意义**：虽然总量没变，但单卡的显存压力被均摊了，从而支持 **1000k+** 的超长上下文。

---

## 专家视角的总结

当你面对 70B 模型 + 长文本的 OOM 挑战时，不要只想着买卡。请按以下顺序思考：

1.  **GQA (Grouped Query Attention)**：这是 Llama-3 自带的，必须开。
2.  **PagedAttention**：这是 vLLM 标配，必须用。
3.  **KV Cache INT8**：性价比最高的工程优化，损失极小。
4.  **StreamingLLM**：如果是聊天机器人（Chatbot）场景，无限对话首选。
5.  **换架构 (Mamba)**：如果是全新的垂类模型训练，考虑是否真的需要 Transformer。

未来的大模型，一定是在**显存有限**的边缘起舞。谁能把 KV Cache 压得更小，谁就能在边缘设备上跑得更远。

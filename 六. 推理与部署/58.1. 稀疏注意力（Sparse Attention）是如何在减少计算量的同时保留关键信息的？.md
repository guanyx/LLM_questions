# 稀疏注意力（Sparse Attention）是如何在减少计算量的同时保留关键信息的？

> 如果说全量注意力（Full Attention）是“一字不差的精读”，那么稀疏注意力（Sparse Attention）就是“高效率的速读”。

在上一篇关于长上下文瓶颈的文章中，我们提到全量注意力的计算量是“平方级”增长的——每增加一个字，都要和之前所有的字“握手”。这种机制虽然保证了不遗漏任何信息，但对于 10 万字、100 万字的输入来说，成本实在太高。

**稀疏注意力（Sparse Attention）** 应运而生。它的核心思想很简单：**不再强求“全员握手”，而是只和“重要的人”握手。**

但问题来了：**如果不看所有内容，怎么知道哪些是重要的？万一漏掉了关键信息怎么办？**

本文将为你揭开稀疏注意力的“偷懒”艺术。

---

## 第一部分：语言的冗余与“偷懒”的资格

首先，我们要明白为什么模型有资格“偷懒”。

人类语言其实包含了大量的冗余信息。
比如这句话：“_今天天气真好，蓝天白云，阳光明媚，非常适合出去郊游。_”
如果你只看：“_今天...天气...好...适合...郊游_”，其实也能理解核心意思。

研究表明，在大多数长文本任务中，一个词真正需要关注的，往往只有**局部的几个词**（比如语法搭配）和**远处的几个关键概念**（比如指代对象）。剩下的 90% 以上的连接可能都是噪音或无效信息。

稀疏注意力就是利用了这一点：**与其在噪音中浪费算力，不如把精力集中在真正有价值的连接上。**

---

## 第二部分：三大法宝——稀疏注意力的核心机制

为了既减少计算量，又不丢失关键信息，科学家们设计了一套“组合拳”。通常，稀疏注意力不是单一的策略，而是以下几种模式的混合（比如著名的 BigBird 或 Longformer 模型）：

### 1. 滑动窗口（Sliding Window）—— 关注“邻居”

这是最基础的策略。模型假设：**离我最近的词，通常最重要。**

- **原理**：每个词只看它前后左右的 $N$ 个词（比如前后 50 个词）。
- **作用**：捕捉语法结构、短语搭配和局部逻辑。比如“苹果”后面跟着“很好吃”，这种紧密的语义关系靠局部窗口就能捕获。
- **计算量**：从 $O(N^2)$ 降到了 $O(N)$（线性），因为无论文章多长，每个词只看固定数量的邻居。

### 2. 全局标记（Global Tokens）—— 设置“灯塔”

光看邻居是不够的，如果文章开头提到了“哈利波特”，结尾提到了“伏地魔”，这两个词相隔十万八千里，滑动窗口肯定够不着。这时候就需要“全局标记”。

- **原理**：选出几个特殊的 Token（比如段落开头的 `[CLS]`，或者人为指定的特定位置），赋予它们“特权”。
  - 这些特权 Token 可以看到**所有**其他的 Token。
  - 所有其他的 Token 也都能看到这些特权 Token。
- **作用**：这些特权 Token 就像是文章的“摘要”或“目录”。当局部的信息需要跨越长距离传递时，它们先传给“灯塔”，“灯塔”再广播给远处的目标。
- **比喻**：就像**航空枢纽**。如果你要处理相邻的信息，走路（滑动窗口）就够了。但如果要联系相隔遥远的内容，你不需要一步步走过去，而是通过**机场**（全局节点）。机场连接了所有区域，通过它，信息可以瞬间“跳跃”到几千公里之外，大大缩短了传输路径。

### 3. 随机采样（Random Attention）—— 随缘“扫视”

为了防止漏掉某些意外的远距离联系，有些模型还会加入随机机制。

- **原理**：除了看邻居和看灯塔，每个词还会随机挑几个远处的词看一眼。
- **作用**：增加信息的流动性，防止局部视角的僵化。虽然是随机的，但在层数够深的情况下，总能“瞎猫碰到死耗子”，捕捉到一些潜在的长程依赖。

---

## 第三部分：小世界网络——为什么这招管用？

你可能会问：**只看邻居和几个灯塔，真的能把信息传遍全文吗？**

这就涉及到了**图论**中的**小世界理论（Small World Theory）**，也就是著名的“六度分隔理论”：你和世界上任何一个陌生人之间，最多只隔着 6 个人。

稀疏注意力构建的正是这样一个“小世界网络”：

1.  **局部窗口**保证了邻里之间的紧密联系。
2.  **全局灯塔**提供了高速公路。

**信息传递路径：**
假设第 1 页的词 A 想和第 1000 页的词 B 建立联系：

- **全量注意力**：A 直接找 B。（一步到位，但连接数太多）
- **稀疏注意力**：A 只要联系上它附近的“灯塔”，灯塔能看到 B（或者 B 附近的灯塔），信息通过 1-2 次中转就能到达 B。

虽然多走了一两步，但连接总数从“亿级”降到了“百万级”，极大地节省了显存和算力，而信息传递的效率几乎没有损失。

---

## 第四部分：进阶——从“固定稀疏”到“动态稀疏”

上面提到的（滑动窗口、全局标记）大多是**静态**的，也就是规则是写死的。不管输入是什么，我都这么看。
现在的 AI 研究正在向更高级的**动态稀疏（Dynamic/Learnable Sparsity）** 演进。

- **聚类与路由（Clustering / Routing）**：
  模型在读文章之前，先对内容做个快速分类。比如把所有讲“体育”的词聚在一起，讲“科技”的词聚在一起。
  在计算注意力时，每个词只去关注**和自己属于同一类**的词。
  - _比喻_：你去参加千人大会，不需要和所有人握手，只需要找到“技术讨论组”，和组里的人交流就够了。

---

## 结语

稀疏注意力告诉我们：**“博闻强记”不一定需要“面面俱到”。**

通过**局部专注**与**全局统筹**的结合，AI 学会了像人类专家一样“跳读”——在海量的信息中，精准地抓住那条贯穿始终的金线，既保留了长上下文的深度，又卸下了计算量的重负。

# 什么是 GQA (Grouped Query Attention)？它是如何将 KV Cache 压缩 8 倍的？

> 在 Llama-2-70B 的显存估算中，我们发现了一个神奇的“压缩因子”：GQA。它像魔法一样把 KV Cache 的显存占用砍掉了 87.5%（从 10.7GB 降至 1.3GB），却几乎没有牺牲模型的智商。它是怎么做到的？

要理解 GQA（分组查询注意力），我们需要先回顾一下它的两位“前辈”：**MHA（多头注意力）** 和 **MQA（多查询注意力）**。这其实是一部关于“如何在显存不够用的情况下，强行让模型多干活”的进化史。

---

## 第一阶段：MHA (Multi-Head Attention) —— 豪横的“全配版”

这是原始 Transformer（如 BERT, GPT-2）采用的标准架构。

### 原理

想象你在做一个复杂的阅读理解题。为了读得更仔细，你分身出了 64 个“分身”（Heads）。

- 每个分身都有自己独立的**查询意图（Query, Q）**。
- 每个分身都有自己独立的**参考资料库（Key, K 和 Value, V）**。

### 显存账单

假设 Hidden Size = 8192，Heads = 64，那么每个 Head 的维度是 128。

- **Query (Q)**: 64 组
- **Key (K)**: 64 组
- **Value (V)**: 64 组

**KV Cache 大小**：正比于 $64 \times 2 = 128$ 份数据。
在训练时这没问题，但在推理时，这 64 组 K 和 V 都要存进显存。对于 70B 这样的大模型，这简直是显存黑洞。

---

## 第二阶段：MQA (Multi-Query Attention) —— 极端的“共享版”

为了解决推理显存爆炸的问题，Google 的研究人员在 2019 年提出了 MQA。

### 原理

既然显存不够，那就让分身们“共享”资料库吧。

- **Query (Q)**: 依然保持 64 个分身，每个分身可以问不同的问题（保持思维的多样性）。
- **Key (K) & Value (V)**: **全员只有 1 组！** 所有 64 个分身必须去同一个资料库里找答案。

### 结果

- **显存节省**：KV Cache 从 64 组变成了 1 组，**压缩了 64 倍！** 显存占用直接忽略不计。
- **代价**：**模型变笨了**。这就好比 64 个专家开会，本来每人手头有一本专业书（物理、化学、历史...），现在强行把书收走，只在桌子中间放一本《百科全书》。虽然大家还能问不同问题，但参考资料的细节丰富度大打折扣，导致推理质量下降，训练也会变得不稳定。

---

## 第三阶段：GQA (Grouped Query Attention) —— 完美的“中庸之道”

MHA 太占显存，MQA 脑子不够用。Llama-2 的作者们想：**能不能折中一下？**
这就是 **GQA（分组查询注意力）**。

### 原理

我们不搞“每人一本”（MHA），也不搞“全村一本”（MQA），我们搞**“小组讨论”**。
将 64 个 Query 分身分成 8 个小组（Group），每组 8 人。

- **Query (Q)**: 64 个，保持不变。
- **Key (K) & Value (V)**: **8 组**。每组里的 8 个 Query 分身，共享这一组 K 和 V。

### 显存账单（以 Llama-2-70B 为例）

- **Query Heads**: 64 个
- **KV Heads**: 8 个

**压缩倍率**：$64 / 8 = 8$ 倍。
也就是说，KV Cache 的大小只有标准 MHA 的 **1/8**。

### 为什么 GQA 是“版本答案”？

实验表明，GQA 的效果非常惊人：

1.  **精度接近 MHA**：只要给每组留一点独立的资料（8 组 KV），模型的智商几乎没有损失（Performance Drop 微乎其微）。
2.  **速度接近 MQA**：显存占用大幅降低，访存带宽压力骤减，推理吞吐量大幅提升。

---

## 直观图解

| 架构    | Query (Q) 数量 | Key (K) 数量 | Value (V) 数量 | KV Cache 大小   | 模型能力   | 备注                           |
| :------ | :------------- | :----------- | :------------- | :-------------- | :--------- | :----------------------------- |
| **MHA** | 64             | 64           | 64             | **100% (基准)** | ⭐⭐⭐⭐⭐ | 原始 Transformer，显存杀手     |
| **MQA** | 64             | **1**        | **1**          | **1.5% (1/64)** | ⭐⭐⭐     | 极致省显存，但精度掉得厉害     |
| **GQA** | 64             | **8**        | **8**          | **12.5% (1/8)** | ⭐⭐⭐⭐✨ | **性价比之王**，Llama-2/3 标配 |

---

## 结语

GQA 的本质是一种**“有损压缩”**的艺术。
它发现了一个真相：**大模型的 Query（提问能力）需要极高的多样性，但 Key/Value（信息索引）其实存在大量的冗余。**

并不需要为每一个提问者都准备一套独立的图书馆，每 8 个人共用一个图书馆，既不会甚至大家排队（带宽瓶颈），也不会让大家查不到资料（精度损失）。这就是 GQA 能够将 70B 模型的 KV Cache 压缩 8 倍，从而让大模型飞入寻常百姓家（单卡/双卡）的秘密武器。

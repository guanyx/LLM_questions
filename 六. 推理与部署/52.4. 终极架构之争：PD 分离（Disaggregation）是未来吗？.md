# 52.4. 终极架构之争：PD 分离（Disaggregation）是未来吗？

在前面的文章中，我们反复提到了一个核心矛盾：**Prefill（预填充）阶段需要极致的算力，而 Decode（解码）阶段需要极致的带宽。**

现在的 GPU（如 H100）虽然很强，但它是一个“全能选手”。当我们把一个请求从头跑到尾时，GPU 其实是在两种人格之间反复横跳：一会儿算力跑满带宽空闲，一会儿带宽跑满算力空闲。

这就像让法拉利去送外卖，又让拖拉机去跑 F1。**资源错配（Resource Mismatch）** 是传统同构集群的阿喀琉斯之踵。

于是，系统架构领域的顶尖专家们提出了一个大胆的想法：**为什么不把它们分开呢？**

---

## 一、 什么是 PD 分离（Prefill-Decode Disaggregation）？

顾名思义，PD 分离就是把推理集群拆分成两类专用的节点：

1.  **Prefill 节点（P-Instance）**：
    *   **任务**：只负责处理 Prompt，生成 KV Cache。
    *   **硬件配置**：堆算力！使用 H100/A100 等高算力卡，甚至可以是专门的矩阵计算加速卡。
    *   **特点**：吞吐量极大，Batch Size 拉满。

2.  **Decode 节点（D-Instance）**：
    *   **任务**：接收 KV Cache，负责一个字一个字地生成结果。
    *   **硬件配置**：堆带宽！使用 HBM 容量大、带宽高的卡（如 L40S，甚至未来的 HBM-only 芯片）。
    *   **特点**：显存大，延迟低。

**工作流程**：
用户请求来了 -> **P 节点** 快速算完 Prompt -> 把巨大的 KV Cache 通过网络传输给 **D 节点** -> **D 节点** 接力生成剩下的 Token。

---

## 二、 看起来很美：解耦的诱惑

这种架构（学术界称为 Splitwise 或 DistServe）带来了显而易见的收益：

1.  **各司其职，利用率起飞**：P 节点算力常年 100%，D 节点带宽常年 100%。没有资源被浪费。
2.  **硬件成本降低**：你不需要买全是 H100 的昂贵集群。Decode 节点可以用便宜点但带宽够用的卡。
3.  **独立扩缩容**：如果用户输入都很长（Prefill 压力大），我就加 P 节点；如果用户生成的都很长（Decode 压力大），我就加 D 节点。

---

## 三、 致命弱点：KV Cache 的搬运噩梦

然而，理想丰满，现实骨感。PD 分离面临一个物理定律的挑战：**KV Cache 太大了**。

一个 70B 的模型，处理 4k 长度的 Prompt，产生的 KV Cache 可能高达几百 MB 甚至上 GB。
要实现 PD 分离，意味着在 P 节点算完的瞬间，必须把这几百 MB 数据通过网络“秒传”给 D 节点。

*   **网络带宽瓶颈**：即使是 400Gbps 的 InfiniBand，传输 1GB 数据也需要几十毫秒。
*   **延迟抵消**：如果 Prefill 阶段省下的计算时间，还没网络传输消耗的时间多，那这个分离就毫无意义。

因此，**PD 分离只适用于“超长 Context”且“拥有超高速互联网络”的土豪集群。**

---

## 四、 替代方案：Chunked Prefill（分块预填充）

鉴于 PD 分离的工程复杂度极高，目前工业界（如 vLLM 团队）更倾向于一种软件层面的折中方案：**Chunked Prefill**（也叫 Sarathi）。

### 1. 核心思想
不要把 Prefill 和 Decode 放在不同的机器上，而是**在同一张卡上通过时间切片混合调度**。

### 2. 怎么做？
如果来了一个 10k 长度的超长 Prompt，传统的做法是一口气算完（占据 GPU 几秒钟），这期间所有 Decode 请求都被卡住（延迟飙升）。
Chunked Prefill 把这 10k 的 Prompt 切成 10 块，每块 1k。
*   **第 1 毫秒**：算 1k 的 Prefill。
*   **第 2 毫秒**：插空处理几个 Decode 请求。
*   **第 3 毫秒**：再算 1k 的 Prefill。
*   ...

### 3. 优势
*   **消灭 Head-of-Line Blocking**：长请求不再阻塞短请求。
*   **无需跨机传输**：KV Cache 都在本地显存，不需要在网络上搬来搬去。

---

## 五、 专家的判断：未来在何方？

作为 AI 专家，目前的判断是：

1.  **短期（1-2年）**：**Chunked Prefill** 统领天下。它是纯软件升级，成本低，收益稳，足以解决大部分延迟问题。
2.  **长期（3-5年）**：随着模型 Context 突破 1M 甚至 10M，单机显存彻底不够用，**PD 分离（Disaggregation）** 可能会结合 **CXL（Compute Express Link）** 等新型内存互联技术卷土重来。

这场架构之争，本质上是 **“计算本地化（Locality）”** 与 **“资源池化（Pooling）”** 的永恒博弈。

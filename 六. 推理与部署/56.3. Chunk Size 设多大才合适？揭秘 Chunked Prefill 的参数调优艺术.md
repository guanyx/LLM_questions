# 56.3. Chunk Size 设多大才合适？揭秘 Chunked Prefill 的参数调优艺术

在上一篇文章中，我们介绍了 **Chunked Prefill（分块预填充）** 是解决“新用户上车阻塞老用户”的神器。它通过将长 Prompt 切成小块（Chunk），实现了“蚂蚁搬家”式的计算。

但是，工程界没有免费的午餐。当你试图在配置文件中设置 `chunk_size` 时，一个灵魂拷问随之而来：
**“这个切片到底要切多大？512？2048？还是动态调整？”**

如果你是一个正在调优 vLLM 或 TGI 的 AI 工程师，这篇文章将为你揭开这个参数背后的**性能博弈论**。

---

## 1. 两个极端的陷阱

Chunk Size 的大小，直接决定了 GPU 是在“干活”还是在“摸鱼”。

### 陷阱 A：切得太细（Chunk Size = 32）

为了让老用户（Decode）感觉极其丝滑，你决定把新用户的 Prompt 切得非常碎，每次只算 32 个 token。

- **后果**：**GPU 空转（Under-utilization）**。
- **原因**：GPU 是一个吞吐型猛兽，启动一个 Kernel（核函数）是有固定开销的（Launch Overhead）。如果任务太小，GPU 还没热身就结束了。这就像用法拉利送外卖，每次只送一盒饭，大部分时间都浪费在发动引擎和等红绿灯上。
- **表现**：虽然延迟（Latency）很低，但整体吞吐量（Throughput）暴跌。

### 陷阱 B：切得太粗（Chunk Size = 4096）

为了榨干 GPU 算力，你决定每次塞一大块数据。

- **后果**：**延迟抖动（High Jitter）**。
- **原因**：处理 4096 个 token 的 Prefill 可能需要 50ms。这意味着在这 50ms 内，所有正在排队等待生成下一个 token 的老用户，都得被迫暂停。
- **表现**：用户感觉到明显的“卡顿感”。文字本来是一个个蹦出来的，突然卡了一下，然后又继续蹦。

---

## 2. 寻找“甜点区”（Sweet Spot）：硬件决定论

那么，最佳的 Chunk Size 在哪里？这取决于你的**硬件算力**和**显存带宽**。

### 算账时间

我们的目标是：让 Chunk 计算的时间，**刚好填满** GPU 的计算单元，同时又不超过用户能感知的延迟阈值（比如 20ms）。

- **A10 / L40S 级别**：算力相对有限。推荐 Chunk Size **512 - 1024**。太大了算不过来，容易卡顿。
- **A100 / H100 级别**：算力怪兽。推荐 Chunk Size **2048 - 4096**。
  - H100 的 FP16/FP8 算力极强，处理 2048 个 token 可能只需要几毫秒。如果你还设成 512，那就是在浪费这块卡皇的性能。

### FlashAttention-3 的助攻

随着 FlashAttention-3 的普及，Attention 计算的开销进一步降低，这实际上允许我们**把 Chunk Size 设得更大**，而不会显著增加延迟。这是一个典型的“软件优化反哺策略”的例子。

---

## 3. 2025 年的 SOTA：自适应调度 (Adaptive Scheduling)

在 2024 年初，我们可能还需要手动去测 vLLM 的 `--max-num-batched-tokens`。但在 2025 年，手动调参正在成为历史。

### 动态感知的调度器

现在的 SOTA 引擎（如 vLLM 的最新调度器、SGLang）已经引入了 **Adaptive Scheduling**：

1.  **监测 ITL (Inter-Token Latency)**：调度器实时监控老用户的生成延迟。
    - 如果发现 ITL 远低于 SLO（比如 10ms < 50ms），说明显卡很闲。
    - **策略**：自动增大 Chunk Size（比如飙到 8192），一口气把新任务吃掉，提升吞吐。
2.  **拥塞感知**：
    - 如果发现队列里堆满了 Decode 请求，ITL 已经临界了。
    - **策略**：自动减小 Chunk Size，甚至暂停 Prefill，优先保老用户的体验。

**结论**：除非你是为了打榜或者针对特定极端业务（如超长文本分析），否则**不要手动锁死 Chunk Size**。相信现代引擎的自动驾驶能力。

---

## 4. 总结

Chunked Prefill 不是一个简单的“开启/关闭”选项，而是一门**平衡的艺术**。

- **太小**：输了吞吐（算力浪费在调度开销上）。
- **太大**：输了延迟（阻塞了其他请求）。
- **趋势**：从“手动调优”走向“自适应调度”。

作为工程师，当你在监控面板上看到 **GPU 利用率忽高忽低** 或者 **P99 延迟异常飙升** 时，不妨去检查一下你的配置。如果你还在用 2023 年的“经验值”（比如无脑设 512）跑在 H100 上，那你可能正在浪费公司一半的算力。

> **思考题**：在 PD 分离（Disaggregation）架构中，Prefill 节点专门负责计算，不负责生成。那么在 Prefill 节点上，还需要开启 Chunked Prefill 吗？开启的目的是什么？（提示：为了流水线传输？）

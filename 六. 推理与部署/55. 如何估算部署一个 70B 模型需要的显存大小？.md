# 如何估算部署一个 70B 模型需要的显存大小？

> **“买得起显卡，不一定跑得动模型。”** —— 这是很多 AI 工程师在部署大模型时的真实写照。

当我们谈论“部署一个 70B 模型”时，很多人第一反应是做个简单的乘法：`700亿参数 x 2字节 = 140GB`。
这个数字没错，但它只是冰山露出水面的一角。在实际工程中，你可能会发现明明买了 160GB 显存的机器，跑起来却频频 OOM（显存溢出），或者吞吐量低得可怜。

本文将带你像“剥洋葱”一样，层层拆解大模型推理背后的显存账单。

---

## 第一层：冰山一角——模型权重（Static Weights）

这是最显眼、最基础的显存开销。模型权重即神经网络中固定的参数（Parameters）。

### 1. 核心公式

显存 = 参数量 $\times$ 存储精度

### 2. 70B 模型的账单

对于 Llama-3-70B 这样的庞然大物：

- **FP16 / BF16 (半精度)**：主流推理精度。
  $70 \times 10^9 \times 2 \text{ Bytes} \approx \mathbf{140 \text{ GB}}$
- **FP8 (8 位浮点)**：H100 等新一代显卡支持的高效精度，兼顾速度与显存。
  $70 \times 10^9 \times 1 \text{ Byte} \approx \mathbf{70 \text{ GB}}$
- **INT8 (8 位量化)**：
  $70 \times 10^9 \times 1 \text{ Byte} \approx \mathbf{70 \text{ GB}}$
- **INT4 (4 位量化)**：端侧或低成本部署的首选。
  $70 \times 10^9 \times 0.5 \text{ Byte} \approx \mathbf{35 \text{ GB}}$

> **🤔 灵魂拷问：大家都是 1 字节（256 个状态），凭什么 FP8 更强？**
> 你说得对，它们能表示的**总状态数**确实都是 $2^8=256$ 个。区别在于**这 256 个“坑位”是如何分配的**：
>
> - **INT8 (均匀分布)**：就像一把**刻度均匀**的尺子。它的坑位是等距的。
>   - _痛点_：神经网络的参数通常是“中间密、两头疏”的钟形分布（大部分数值在 0 附近）。INT8 为了照顾极少数的“大数值”（Outliers），必须把刻度间距拉大，导致中间密集的“小数值”被强行合并，精度损失严重。
> - **FP8 (非均匀分布)**：就像一把**对数刻度**的尺子。
>   - _优势_：它把宝贵的 256 个坑位，密集地分配在 0 附近（保留小数精度），而在远离 0 的地方分配得稀疏（保范围）。这完美契合了神经网络参数的分布特征，因此能用同样的 1 字节存出“更高质量”的信息。

> **残酷现实**：单张 NVIDIA A100 (80GB) 甚至装不下 FP16 的 70B 模型（140GB > 80GB）。如果不做量化，你至少需要两张 A100 做**张量并行（Tensor Parallelism）**。

---

## 第二层：水下巨兽——KV Cache（Dynamic Runtime）

这是最容易被低估，也是导致线上服务 OOM 的头号杀手。
为了让模型在生成文本时不重复计算之前的内容，我们需要把每一层的 Key 和 Value 矩阵缓存下来。

**KV Cache 的可怕之处在于：它是动态增长的，且随着并发量（Batch Size）和上下文长度（Sequence Length）线性爆炸。**

### 1. 计算公式

$$ \text{KV Cache} = 2 \times \text{层数} \times \text{维度} \times \text{上下文长度} \times \text{并发数} \times \text{精度字节数} $$

- _注：现代模型（如 Llama-2/3）普遍使用了 GQA（分组查询注意力），能将 KV Cache 大小压缩为原来的 1/8 甚至更多。_

### 2. 70B 实战推演（以 FP16 为例，含 GQA）

假设我们要处理 4096 (4k) 长度的上下文：

- **并发数 = 1（单用户）**：
  KV Cache 约占用 **1.3 GB**。看起来很少？别急。
- **并发数 = 32（生产环境）**：
  $1.3 \text{ GB} \times 32 \approx \mathbf{41.6 \text{ GB}}$

> **惊人结论**：当你把并发拉满时，光是 KV Cache 占用的显存（41.6GB）就已经超过了一张 A100 40G 版的全部容量！

---

## 第三层：隐形成本——激活值与系统开销

除了权重和 Cache，显存里还住着一些“隐形房客”。

1.  **中间激活值（Activation）**：推理时每一层的临时计算结果。虽然比训练时少得多，但在大 Batch Size 下依然不可忽视。预留 **2-5 GB**。
2.  **框架开销**：PyTorch 上下文、CUDA 核心库、NCCL 通信缓冲（多卡必备）。预留 **2-4 GB**。
3.  **显存碎片**：就像硬盘碎片一样，申请释放由于不连续，会导致即使有空闲显存也无法分配。

---

## 第四层：工程优化——如何把大象装进冰箱？

既然显存这么紧张，工程师们发明了各种“压缩术”：

### 1. PagedAttention (vLLM)

传统显存分配必须是连续的，容易产生碎片。PagedAttention 像操作系统的虚拟内存一样，把 KV Cache 切成小块（Block）离散存储。

- **效果**：显存利用率接近 100%，同样的显存能跑更大的 Batch Size。

### 2. 量化技术 (AWQ / GPTQ)

不要看不起 INT4。对于 70B 这样的大模型，参数冗余度很高，INT4 量化后的精度损失往往微乎其微，但显存直接砍半（140GB -> 35GB），让单卡 4090 跑起来成为可能。

### 3. Offload (显存卸载)

实在塞不下怎么办？把暂时不用的层或 Cache 赶到 CPU 内存里去，用的时候再拉回来。

- **代价**：速度极慢。适合个人玩家体验，不适合生产环境。

---

## 终极指南：70B 部署配置推荐表

为了方便查阅，我们总结了一份“避坑指南”（假设使用 vLLM 等高效框架）：

| 场景需求          | 精度     | 显存刚需 (权重+Cache)   | 硬件方案推荐                            | 备注                                              |
| :---------------- | :------- | :---------------------- | :-------------------------------------- | :------------------------------------------------ |
| **极致性价比**    | **INT4** | ~35GB + 动态            | **2x RTX 3090/4090 (24G)**              | 个人/小团队首选，总显存 48G，够跑                 |
| **平衡之选**      | **INT8** | ~70GB + 动态            | **2x A100 (40G)** 或 **2x A6000 (48G)** | 企业级入门，兼顾精度与成本                        |
| **高效前沿**      | **FP8**  | ~70GB + 动态            | **1x H100 (80G)**                       | 需 Hopper 架构，速度快且省显存                    |
| **满血性能**      | **FP16** | ~140GB + 动态           | **2x A100 (80G)** 或 **4x A100 (40G)**  | 科研/高精度业务，显存充裕                         |
| **土豪/超长文本** | **FP16** | ~140GB + **海量 Cache** | **4x A100 (80G)** 或 **8x A100 (40G)**  | 32k/128k 长文本场景，KV Cache 是无底洞            |

### 黄金估算公式

$$ \text{总显存} = (\text{权重} \times 1.2) + \text{KV Cache}(\text{最大并发}) + 4\text{GB(系统预留)} $$

记住：**算大不算小，留足余量**。显存满了就是服务崩溃，而显存空着只是浪费电费，两者的后果不可同日而语。

# 什么是 Continuous Batching（连续批处理）？它如何提升 GPU 利用率？

在当今的大模型（LLM）推理服务中，显卡（GPU）不仅是算力的核心，更是成本的黑洞。如何压榨每一滴 GPU 的性能，成为了工程优化的终极目标。

你可能听说过“批处理（Batching）”是提升吞吐量的法宝，但在 LLM 领域，传统的批处理方式却遭遇了滑铁卢。为此，**Continuous Batching（连续批处理）** 横空出世，彻底改变了 LLM 的推理调度方式。

然而，技术演进从未停止。**仅仅理解“连续批处理”已经是 2023 年的知识了。** 为了紧跟 2024-2025 年的前沿，本文不仅会讲透基础原理，还将带你深入了解 **Chunked Prefill（分块预填充）** 和 **Disaggregation（分离式架构）** 等最新突破。

---

## 1. 基础科普：传统“静态批处理”的困境

为了理解 Continuous Batching，我们首先得知道它解决了什么问题。

### 传统的“公交车”模式

在深度学习推理中，为了提高效率，我们通常不会一次只处理一个请求，而是将多个请求打包成一个“批次（Batch）”一起送进 GPU 计算。这就像**公交车**：只有等车上坐满了人（或者到了发车时间），司机才发车。

### LLM 的“长短不一”难题

但在大语言模型中，情况完全不同：

- **输入长度不同**：有的用户问“你好”，有的用户粘贴了一篇 5000 字的文章。
- **输出长度未知**：有的回答只需“是的”，有的回答需要写一篇论文。

如果我们强行使用传统的**静态批处理（Static Batching）**，就会出现尴尬局面：
**一辆公交车，明明 90% 的乘客在第一站就下车了，但司机非要等最后一位乘客坐到终点站，才允许新的乘客上车。**
后果是：GPU 显存被锁定，算力在空转，资源被极大浪费。

---

## 2. 技术进阶：Continuous Batching 的“拼车”革命

Continuous Batching（也称为 Iteration-level Batching，迭代级批处理）的出现，打破了这种僵局。

### 从“整车调度”到“座位调度”

它的核心思想是：**不再以“整车（Batch）”为单位调度，而是以“座位（Slot）”或“迭代（Iteration）”为单位。**

- **即时腾退**：一旦某个请求生成了“结束符”，它立即下车。
- **无缝衔接**：系统立刻拉一个新的请求进来，填补空位。
- **动态运行**：GPU 的每一次计算迭代中，都在处理不同进度的请求。

### 形象比喻

如果说静态批处理是“固定班次的公交车”，那么 Continuous Batching 就是**“随停随走的拼车”**：只要有人下车，立马让排队的人上车。通过这种机制，GPU 的利用率曲线从“锯齿状”变成了“平滑的高位直线”。

---

## 3. 前沿突破：解决“新乘客”带来的拥堵 (Chunked Prefill)

**这里是许多入门文章没有讲到的前沿痛点。**

虽然 Continuous Batching 让“下车”变快了，但“上车”却成了新问题。
在 LLM 推理中，处理新请求的第一阶段叫 **Prefill（预填充）**，需要一次性计算用户输入的所有文字。
如果一个新用户突然扔进来一本 10 万字的小说要求总结，GPU 需要花费很长时间来处理这个巨大的 Prefill 任务。

### 痛点：Prefill 阻塞 Decode

在标准的 Continuous Batching 中，一旦这个“巨型新请求”上车，GPU 就会全力处理它的 Prefill。
**后果**：车上其他正在逐字生成（Decode）的老乘客，被迫等待这个巨型任务处理完才能继续生成。
**用户体验**：正在流畅输出的文字，突然卡顿了一下（Inter-token Latency 飙升）。

### 解法：Chunked Prefill（分块预填充）

2024 年的主流推理引擎（如 vLLM, TGI）引入了 **Chunked Prefill**。

- **原理**：不再把新请求当成一个不可分割的整体。如果新请求太长，就把它切成小块（Chunk）。
- **操作**：在每一次迭代中，GPU 既处理老乘客的一个 Token 生成，也只处理新乘客的一小块 Prefill（比如 512 个 Token）。
- **比喻**：新乘客行李太多，不要一次性搬上车堵住车门。而是**“蚂蚁搬家”**，每次迭代只搬一点行李上车，保证车门永远畅通，老乘客感觉不到卡顿。

---

## 4. 终极架构：PD 分离 (Prefill-Decode Disaggregation)

随着模型上下文越来越长（128k, 1M+），仅仅靠 Chunked Prefill 也不够用了。业界开始探索更激进的架构改革：**PD 分离**。

- **Prefill (预填充)**：计算密集型。特点是短时间内算力需求爆炸。
- **Decode (解码)**：显存带宽密集型。特点是算力利用率低，但对带宽要求高。

既然两者的“性格”完全不同，为什么要强行把它们放在同一张显卡上跑？

### 专人专用

最新的推理集群架构（如 DistServe, Mooncake 等）开始将 GPU 分为两类：

1.  **Prefill 实例**：专门负责处理新请求的“阅读理解”。算完后，将 KV Cache（记忆）通过高速网络传输给 Decode 实例。
2.  **Decode 实例**：专门负责“逐字生成”。

这就像餐厅改革：

- **原来**：一个厨师既要备菜（切洗）又要炒菜，手忙脚乱。
- **现在**：**配菜员（Prefill GPU）** 专门切好菜，通过传送带（高速网络）递给 **大厨（Decode GPU）** 专门炒菜。
  虽然增加了网络传输的开销，但在超高并发和超长文本场景下，这种架构能让整体吞吐量（Goodput）再上一个台阶。

---

## 5. 总结与独到见解

从 Static Batching 到 Continuous Batching，再到 Chunked Prefill 和 PD 分离，LLM 推理优化的历史，就是一部**与“显存墙”和“算力墙”博弈的历史**。

- **核心认知**：Continuous Batching 已经成为现代推理引擎的**标配（Baseline）**，而不是“黑科技”。
- **前沿判断**：现在的竞争焦点，已经从简单的“提升 GPU 利用率”，转移到了**“在保证低延迟（SLO）的前提下，最大化有效吞吐量（Goodput）”**。

**独到见解**：
未来的推理系统将更像一个**复杂的分布式数据库**。我们不仅是在调度“计算任务”，更是在调度“显存状态（KV Cache）”。
谁能更高效地在 GPU 之间搬运、复用（Prefix Caching）和管理这些“记忆”，谁就能掌握大模型推理成本的命门。

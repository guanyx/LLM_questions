# 56.2. 每次都要重算 System Prompt？揭秘 Prefix Caching 与 Radix Attention

在上一篇文章中，我们聊到了 Continuous Batching 如何像“拼车”一样提升了 GPU 的座位利用率。

但细心的同学可能发现了一个巨大的**“隐形浪费”**：
在很多业务场景（尤其是 RAG、Agent 或角色扮演）中，我们发给模型的 Prompt 往往长这样：

> **System Prompt (2000 tokens)**: “你是一个资深的法律顾问，请熟读以下《民法典》条款...（此处省略 2000 字）...”
> **User Query (50 tokens)**: “邻居家的树倒了砸了我车，怎么赔？”

如果有 100 个用户来咨询法律问题，他们的 User Query 各不相同，但那 2000 tokens 的 **System Prompt 是一模一样的**。

在普通的 Continuous Batching 机制下，GPU 会老老实实地把这 2000 个 token 的 KV Cache **重复计算 100 遍**。
这就像 100 个乘客上车，每个人都带了一模一样的行李，而司机竟然每次都要重新把行李过一遍安检。

能不能让这份“公共行李”直接免检？
这就引出了 2024-2025 年 LLM 推理优化的另一大杀器：**Prefix Caching（前缀缓存）** 与 **Radix Attention**。

---

## 1. 痛点：被忽视的“重复劳动”

在 RAG（检索增强生成）和长文本 Agent 场景中，**重复前缀（Common Prefix）** 的比例高得吓人。

- **多轮对话**：用户问第二句时，包含了第一句的历史。这其实也是一种“前缀”。
- **复杂 Agent**：往往带有几千字的 Tool Definition（工具定义）和 Few-Shot Examples（少样本示例）。

如果每次请求都重新计算这些静态内容的 KV Cache，不仅浪费了宝贵的算力（Prefill 时间变长），还浪费了显存带宽。

---

## 2. 救星：从“线性列表”到“字典树”

要解决这个问题，我们需要改变 KV Cache 的存储结构。

### 传统视角：KV Cache 是“一维数组”

在传统的推理引擎眼中，每个请求的 KV Cache 是独立的。

- 请求 A：`[System] + [Query A]` -> 显存块 1
- 请求 B：`[System] + [Query B]` -> 显存块 2

即使 `[System]` 部分完全一样，显存块 1 和 2 也是毫无关联的物理空间。

### 进化视角：Radix Attention (基数注意力)

**LMSYS 团队（SGLang 的作者）** 提出了一种天才的想法：把 KV Cache 管理成一颗 **Radix Tree（基数树/字典树）**。

这很像文件系统的目录结构：

- **根节点** -> **节点 A (System Prompt)**
  - -> **子节点 B (Query A)**
  - -> **子节点 C (Query B)**

**工作流程变化：**

1.  **请求 A 到来**：系统发现树里是空的。于是计算 `[System]`，在树上创建节点 A；接着计算 `[Query A]`，创建子节点 B。
2.  **请求 B 到来**：系统对输入的 Prompt 进行**前缀匹配（Prefix Matching）**。
    - 发现：_“咦？前 2000 个 token 的哈希值跟节点 A 一模一样！”_
    - 操作：**直接复用**节点 A 的 KV Cache 指针！不需要重新计算。
    - 只需：计算 `[Query B]` 的部分，挂在节点 A 下面成为子节点 C。

---

## 3. 效果：0ms Prefill

这种技术带来的性能提升是降维打击级别的。

- **首字延迟（TTFT）暴降**：因为 System Prompt 不需要算了，新请求的 Prefill 时间几乎归零，只剩下处理那短短几十个 User Token 的时间。
- **吞吐量飙升**：显存里只需要存一份 System Prompt 的 KV Cache，节省出来的显存可以塞进更多的并发请求（Batch Size 变大）。

---

## 4. 2025 年的进阶挑战：不只是“树”那么简单

仅仅理解 Radix Tree 已经是 2024 年初的知识了。在 2025 年的大规模集群中，事情变得更加复杂。

### (1) 路由难题：Cache-Aware Routing (亲和性调度)

假设你有 8 张 GPU 卡。

- GPU-1 刚刚算过“法律顾问”的 System Prompt，显存里有热乎的缓存。
- GPU-2 从来没算过。

如果负载均衡器（Load Balancer）把下一个“法律咨询”请求随机发给了 GPU-2，那 GPU-1 的缓存就白瞎了。
**SOTA 解法**：调度器必须是**“缓存感知（Cache-Aware）”**的。它需要维护一个全局的“缓存地图”，尽量把请求路由到“持有该前缀缓存”的 GPU 上（Sticky Routing）。这在 **vLLM** 和 **SGLang** 的集群模式中已经成为核心特性。

### (2) 存算分离：Global Prefix Caching

如果 GPU-1 的显存满了，必须踢掉一些缓存，怎么办？
**Mooncake / LMCache** 等架构提出了 **Disaggregated KV Cache**。

- 不要直接删掉，而是把这个热门前缀的 KV Cache **“降级”** 存到 CPU 内存或者远程的高速缓存池里。
- 当其他 GPU 需要时，直接从池子里拉取，虽然比本地 HBM 慢，但绝对比重算（Re-compute）要快！

### (3) 结构化生成：JSON Mode 的神助攻

SGLang 利用 Radix Attention 做到了极致的 **Structured Generation**。
当你要让模型强制输出 JSON 格式时，JSON 的语法结构（如 `{"key": "`）其实也是一种固定的“前缀”。Radix Attention 可以把这些语法骨架缓存在树里，让模型生成结构化数据时如丝般顺滑，且不消耗额外算力。

---

## 5. 总结

如果说 Continuous Batching 是解决了“并发”问题，那么 Prefix Caching 就是解决了“复用”问题。

**技术演进路线图：**

1.  **Static Batching**：死板的公交车。
2.  **Continuous Batching**：灵活的拼车。
3.  **Chunked Prefill**：把大行李切碎了上车。
4.  **Prefix Caching**：发现大家行李一样，直接用仓库里现成的，不用带上车。
5.  **Distributed / Global Caching** (2025)：仓库不够放，我们搞个“云端共享仓库”，配合智能导航（Routing）精准取货。

在 2025 年，一个成熟的 LLM 推理引擎，**必须**同时具备以上这五项能力，才能在激烈的降本增效战争中活下来。

> **思考题**：如果是**多模态模型（LMM）**，输入的不仅是文字，还有一张巨大的图片（Image Token）。这张图片能做 Prefix Caching 吗？如果能，对显存带宽的压力会有什么变化？

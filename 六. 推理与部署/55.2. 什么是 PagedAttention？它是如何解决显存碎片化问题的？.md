# 什么是 PagedAttention？它是如何解决显存碎片化问题的？

> 在 vLLM 横空出世之前，大模型推理框架（如 HuggingFace Accelerate）的显存利用率低得令人发指，往往只有 20%~40%。这并非因为显卡不行，而是因为显存被“碎片”吃掉了。
> PagedAttention 的出现，像操作系统的虚拟内存一样，彻底重构了 KV Cache 的管理方式，将显存利用率拉升到了接近 100%。

本文将深入操作系统的内存管理底层，为您揭开 PagedAttention 的面纱。

---

## 第一部分：传统显存管理的“硬伤”——连续分配

在 PagedAttention 诞生之前，KV Cache 的显存分配遵循一个古老而死板的规则：**必须连续**。

### 场景模拟
假设你正在生成一段文本。
1.  **预分配（Pre-allocation）**：因为不知道你会生成 10 个字还是 1000 个字，为了防止写到一半没地方写，框架通常会按**最大可能的长度**（比如 2048）预先申请一块连续的显存。
2.  **巨大的浪费**：
    *   **内部碎片（Internal Fragmentation）**：如果你只生成了 50 个字就停了，那么剩下的 1998 个位置的显存就白白浪费了，别人也用不了。
    *   **外部碎片（External Fragmentation）**：假设显存里有两块已经使用的区域，中间夹着一段 100MB 的空闲空间。此时来了一个需要 101MB 的请求。虽然总量还有 100MB，但因为这段空间**不够连续**，系统只能拒绝分配（OOM）。

### 后果
就像在一张满是窟窿的纸上写字，明明有很多空白处，但因为找不到一块足够大的完整区域，只能被迫换纸（买新显卡）。

---

## 第二部分：PagedAttention 的灵感——虚拟内存

伯克利的研究人员想：**为什么不能像操作系统管理内存那样管理显存呢？**

在操作系统中，物理内存是被切分成一个个 4KB 的**页（Page）**的。当程序申请内存时，操作系统不需要给它一段连续的物理内存，只需要给它几个离散的页，然后通过**页表（Page Table）**把这些离散的物理页映射成一段连续的**虚拟内存**给程序看。

### PagedAttention 的核心机制
1.  **分块（Blocking）**：将 KV Cache 切分成固定大小的 **Block**（比如 Block Size = 16，即存 16 个 Token 的 KV）。
2.  **离散存储**：这些 Block 在显存中可以是**不连续**的。
3.  **动态映射**：维护一张“Block Table”，记录逻辑上的 Token 顺序对应物理上的哪个显存块。

### 优势
*   **消灭外部碎片**：只要显存里还有任何一个空闲的 Block，就可以拿来用，不需要连续空间。
*   **消灭内部碎片**：按需申请。生成 16 个 Token，就只申请 1 个 Block。生成到第 17 个，再申请第 2 个。最大的浪费永远不超过 1 个 Block 的大小。
*   **灵活共享**：这也为 **Parallel Sampling**（同一个 Prompt 生成多种回复）和 **Beam Search** 提供了巨大的优化空间——多条回复可以共享同一个 Prompt 的物理 Block，显存占用直接砍半。

---

## 第三部分：工程实战——Block Size 该怎么设？

在 vLLM 等框架中，`block_size` 是一个关键的可调参数（默认为 16 或 32）。
**作为一个中级工程师，你需要知道如何调整它：**

### 1. Block Size 太小（比如 4）
*   **优点**：显存颗粒度更细，浪费更少（内部碎片极小）。
*   **缺点**：Block 数量变多，导致 Block Table 变大，寻址开销增加，可能会轻微拖慢推理速度。而且 GPU 的内存读取通常有最佳粒度（Coalesced Access），太小会导致带宽利用率下降。

### 2. Block Size 太大（比如 128）
*   **优点**：寻址快，带宽利用率高。
*   **缺点**：由于必须按 Block 申请，如果最后一个 Block 只用了 1 个 Token，剩下的 127 个位置就浪费了（内部碎片变大）。对于短文本请求较多的场景，这种浪费不可忽视。

### 3. 最佳实践
通常推荐设置为 **16** 或 **32**。
这在显存利用率（碎片率 < 3%）和计算效率（GPU 访存友好）之间达到了完美的平衡。

---

## 结语

PagedAttention 并没有发明新的算法，它只是把计算机系统（OS）中几十年陈酿的智慧（虚拟内存分页）搬运到了 AI 推理系统中。

它告诉我们：**在大模型时代，系统工程（System）的重要性丝毫不亚于算法模型（Model）本身。** 没有 PagedAttention 这样的系统级优化，再强的 70B 模型也难以在有限的硬件上发挥出真正的生产力。

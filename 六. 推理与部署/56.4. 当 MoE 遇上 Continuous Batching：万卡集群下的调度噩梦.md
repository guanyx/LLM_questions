# 56.4. 当 MoE 遇上 Continuous Batching：万卡集群下的调度噩梦

在之前的文章中，我们讨论的 Continuous Batching 大多是基于 **Dense 模型**（如 Llama-3-70B）的。在这种架构下，所有 Token 都会经过每一层的每一个神经元。

但在 2025 年，随着 **DeepSeek-V3**、**GPT-4** 等 **MoE（混合专家模型）** 的统治，推理引擎的调度逻辑面临着前所未有的挑战。

当“随停随走的拼车机制”（Continuous Batching）遇上“反复横跳的动态路由”（MoE Routing），一场关于**通信风暴**与**负载均衡**的噩梦开始了。

---

## 1. MoE 的“任性”：动态路由

在 Dense 模型中，显卡 A 上的计算任务是确定的：不管来什么请求，我都得算。
但在 MoE 模型中，显卡 A 可能只持有“数学专家”和“代码专家”。

- **Token 1 ("Hello")**：被路由到了显卡 B（闲聊专家）。
- **Token 2 ("Compute")**：突然跳到了显卡 A（数学专家）。
- **Token 3 ("def")**：又留在了显卡 A（代码专家）。

这意味着，**同一个请求（Sequence）的计算路径是动态变化的**。
传统的 Continuous Batching 假设一个请求会稳定地占用某张卡的显存和算力，但在 MoE 架构下，这个假设崩塌了。

---

## 2. 调度噩梦：All-to-All 通信风暴

MoE 推理的核心痛点在于 **Expert Parallelism (EP)**。
通常，我们会把不同的专家放在不同的 GPU 上。

当一个 Batch（比如 128 个请求）进入某一层时：

1.  **路由计算**：先算一下每个 Token 该去哪。
2.  **大洗牌（Dispatch）**：
    - Token A 去 GPU 1
    - Token B 去 GPU 2
    - ...
    - 这会触发一次巨大的 **All-to-All 通信**。所有 GPU 都在互相发数据。
3.  **专家计算**：各算各的。
4.  **大聚合（Combine）**：算完后，又要通过 All-to-All 把数据传回原来的位置（或者下一个位置）。

在 Continuous Batching 场景下，Batch Size 往往很大。这就导致 **All-to-All 通信的频率和数据量爆炸**，网络带宽（Inter-connect）瞬间成为瓶颈。

---

## 3. 2025 年的新变量：Shared Experts 与 MLA

为了缓解这些问题，以 **DeepSeek-V3** 为代表的 2025 年架构引入了两个关键变量，改变了调度格局：

### (1) Shared Expert（共享专家）的回归

现在的 MoE 不再全是“动态路由”的。它保留了约 10% 的参数作为 **Shared Expert**，这部分参数**永远被激活**，处理所有 Token。

- **调度影响**：这意味着所有 GPU 都有一个“保底负载”。不管路由怎么跳，Shared Expert 部分的计算是固定的。这在一定程度上稳定了 Continuous Batching 的底噪，但也增加了显存常驻的压力。

### (2) MLA (Multi-Head Latent Attention) 的助攻

MoE 模型通常参数量巨大，显存被权重吃光了，留给 KV Cache 的空间很小。
**MLA** 通过极度压缩 KV Cache（只有传统的 1/10 甚至更小），为 Continuous Batching 腾出了宝贵的显存空间。

- **意义**：如果没有 MLA，在万卡 MoE 上跑大 Batch Size 推理几乎是不可能的（OOM）。

---

## 4. 前沿解法：EP-Aware Scheduling 与 DualPipe

面对“通信墙”和“负载不均”，业界搞出了哪些黑科技？

### (1) EP-Aware（专家感知）调度

调度器不再盲目地把请求塞进 Batch，而是先**“预测”**一下这个请求接下来几层可能会用到哪些专家。
如果发现当前 Batch 里“代码专家”的需求已经饱和了，就暂时压住这个写代码的请求，先放几个“写小说”的请求进来，强行平衡负载。

### (2) DualPipe：计算与通信的重叠

DeepSeek 团队提出的 **DualPipe** 策略（虽然主要用于训练，但思想通用），核心是让 **All-to-All 通信** 和 **专家计算** 在时间上重叠（Overlap）。

- 当 GPU 还在算上一层的 Shared Expert 时，网卡已经开始疯狂传输下一层 Routed Expert 的数据了。
- **效果**：只要计算时间 > 传输时间，通信延迟就被“隐藏”了。

---

## 5. 总结

MoE 架构下的 Continuous Batching，不再是简单的“显存管理”问题，而变成了复杂的**“分布式流量调度”**问题。

- **Dense 时代**：我们担心的是显存够不够大。
- **MoE 时代**：我们担心的是网线够不够粗，以及“偏科”严重的专家们会不会忙的忙死，闲的闲死。

在 2025 年，评价一个推理引擎好坏的标准，不仅看它支持多少 QPS，更看它能否**完美驾驭 MLA、Shared Expert 和 EP 通信**这三驾马车。

> **思考题**：在 MoE 架构中，KV Cache 是跟着 Token 跑（存到计算专家的卡上），还是固定在某张卡上不动？这两种方案对网络带宽有什么不同的影响？

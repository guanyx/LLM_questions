# 多卡推理的艺术：Tensor Parallelism (TP) vs Pipeline Parallelism (PP)

> 当 70B 模型太大，单卡装不下时，我们不得不求助于多卡并行。但“多卡”不仅仅是把显存加起来那么简单。
> 面对 **Tensor Parallelism (TP)** 和 **Pipeline Parallelism (PP)** 这两个选项，很多高级工程师也会犹豫：**到底该横着切，还是竖着切？**

本文将从通信开销、计算效率和显存占用三个维度，深度对比这两种主流的并行策略，并揭示为什么推理场景通常首选 TP。

---

## 第一部分：切蛋糕的两种切法

假设我们有一个 8 层的 Transformer 模型，要部署在 2 张显卡（GPU 0 和 GPU 1）上。

### 1. Pipeline Parallelism (PP) —— 竖着切（层间切分）

这是最直观的切法。

- **做法**：把前 4 层放在 GPU 0 上，后 4 层放在 GPU 1 上。
- **流程**：数据先进入 GPU 0，算完前 4 层，把结果传给 GPU 1，GPU 1 接着算后 4 层。
- **比喻**：**流水线工厂**。GPU 0 是第一道工序（粗加工），GPU 1 是第二道工序（精加工）。

### 2. Tensor Parallelism (TP) —— 横着切（层内切分）

这是更极客的切法。

- **做法**：把**每一层**的权重矩阵（比如 $W_Q, W_K, W_V$）都切成两半。GPU 0 存左半边，GPU 1 存右半边。
- **流程**：每一层的计算都需要 GPU 0 和 GPU 1 同时参与，算完后立即同步结果（All-Reduce），再进行下一层。
- **比喻**：**双人联弹**。两个钢琴家并排坐，每个人负责弹钢琴的一半键盘，必须配合得天衣无缝。

---

## 第二部分：推理场景的王者争霸

在**训练**场景下，PP 和 TP 经常混合使用（3D Parallelism）。但在**推理（Inference）**场景下，**TP 几乎是唯一的王者**。为什么？

### 1. 延迟（Latency）—— TP 完胜

- **PP (流水线)**：因为是串行的，数据必须走完 GPU 0 才能去 GPU 1。此时 GPU 1 是**空闲**的（Bubble）。虽然可以通过 Micro-batch 减少空闲，但在推理时（特别是 Batch=1），**延迟 = GPU 0 计算时间 + GPU 1 计算时间**。没有加速效果，只是解决了显存不够的问题。
- **TP (张量并行)**：两张卡**同时计算**。虽然多了一步通信，但计算时间理论上减半。**延迟 ≈ (单卡计算时间 / 2) + 通信时间**。只要通信够快（NVLink），TP 能显著降低延迟。

### 2. 通信带宽（Bandwidth）—— TP 的软肋

- **PP**：只需要在切分点（第 4 层和第 5 层之间）传一次数据。通信量极小，甚至可以通过 PCIe 慢速总线传输。
- **TP**：**每一层**算完都要同步（All-Reduce）。通信频次极高，通信量巨大。
- **结论**：**TP 必须依赖高带宽互联（NVLink）**。如果是 PCIe（比如两张 3090 插在主板上），TP 的通信延迟会抵消掉计算加速，甚至变得更慢。而 PP 对带宽要求极低，适合多机（跨节点）部署。

### 3. 显存占用 —— 平手

两者都能将模型权重完美平分。

- **PP**：显存里还需要存每一层的中间 Activation，GPU 0 存前 4 层的，GPU 1 存后 4 层的。
- **TP**：每一层的 Activation 也是切分的，或者在同步后由于重复计算可能会有一点冗余（取决于具体实现），但总体差别不大。

---

## 第三部分：Offload 的逆袭——如何让 CPU 不那么慢？

如果连多卡都没有，只能用 **Offload**（卸载到 CPU），还有救吗？
普通 Offload 慢是因为 PCIe 带宽瓶颈。高级工程师会关注 **"Speculative Decoding" (投机采样)**。

### 投机采样 (Speculative Decoding)

- **原理**：用一个极小的模型（比如 Llama-7B，甚至 1B）在显卡上快速生成草稿（Draft），然后利用**系统内存（RAM）**存储 70B 大模型，通过 **Offload 机制**（按需将层加载到 GPU 或利用 CPU 算力）去**验证**这个草稿。
- **注意**：虽然大模型在 CPU/内存中读取很慢，但因为它是并行地“批改”草稿（一次验证多个 Token），而不是逐字生成，所以整体效率远高于大模型单独硬跑。
- **效果**：大模型不需要逐字生成，只需要“批改作业”。如果小模型写得对，大模型一次就能确认 5-10 个 Token。
- **意义**：这让 Offload 场景下的推理速度提升了 2-3 倍，让“穷人版 70B”具备了实用价值。

---

## 结语

作为高级架构师，在设计 70B 部署方案时：

1.  **单机多卡（有 NVLink）**：**首选 TP**。延迟最低，吞吐最高。
2.  **多机多卡（以太网连接）**：**节点内 TP，节点间 PP**。因为跨节点带宽低，跑不起 TP。
3.  **单机多卡（无 NVLink，PCIe）**：**慎用 TP**，由于通信瓶颈，可能不如 PP 甚至单卡量化快。
4.  **资源受限（显存不足）**：考虑 **Offload + 投机采样**，用算力换空间，用小模型换速度。

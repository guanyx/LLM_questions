# 从“用丹”到“铸剑”：普通工程师的 LLM 工程 100 问

---

## 一、 心态与目标 (Mindset & Goals)

_不造“核电站”，但要能把“电”稳定、便宜、可控地送到业务。_

1.  **你的业务到底缺什么**：你想解决的是“写得像人”还是“答得对、可追溯、可上线”？
2.  **ROI 先行**：如果把一个需求拆成“效果、成本、延迟、风险”，你会把哪个指标当北极星？
3.  **从模型能力到产品体验**：同样 10% 的模型提升，为什么有时用户体验几乎不变？
4.  **“堆模型”还是“做系统”**：在你当前场景里，最大瓶颈更可能在数据、流程还是推理工程？
5.  **规模化思维**：当日调用量从 1 万涨到 1000 万，你的方案会先在哪个环节崩掉？
6.  **可解释性门槛**：你的场景是否需要“可举证答案”，需要到什么程度？
7.  **失败预算**：你能接受多少错误率，错误发生时的代价是什么（钱、合规、品牌、生命）？
8.  **灰度策略**：你会如何设计“逐步放量”的上线方案，确保模型出错不变成事故？
9.  **“铸剑师”的定义**：你如何用一句话定义“合格的 LLM 工程师”在你公司意味着什么？
10. **学习路线取舍**：在有限时间里，你会优先补齐“数据、评测、推理、Agent”中的哪一块，为什么？

---

## 二、 基座模型与选型 (Foundation Model Selection)

_强模型很多，选对、用对、算得清才是工程能力。_

11. **能力对齐**：你的任务更依赖知识、推理、风格一致性、还是工具调用可靠性？
12. **开源 vs 闭源**：在成本、合规、可控、迭代速度之间，你的场景更偏向哪一侧？
13. **上下文窗口**：你的真实输入分布有多长？长上下文是刚需，还是能用检索/摘要替代？
14. **输出形态**：你的输出需要结构化（JSON/SQL/DSL）吗？结构化需求如何影响模型选型？
15. **多语言与领域**：你的语料主要是什么语言、什么风格？模型在该分布下的短板是什么？
16. **工具调用能力**：你如何验证一个模型的 function calling 在“缺参/错参/越权”场景下的鲁棒性？
17. **幻觉风险画像**：你如何基于任务类型给幻觉风险分级，而不是泛泛地“模型会胡说”？
18. **可替换性设计**：你如何做抽象层，让未来切换模型（厂商/版本/开源）不重写系统？
19. **成本估算**：你会如何把一次请求的成本拆成“输入 token、输出 token、并发、缓存命中率”？
20. **基线建立**：你会用什么最小评测集来给候选模型排一个“能上线的初始榜单”？

---

## 三、 数据工程：指令数据与清洗 (Data Engineering)

_Scaling Laws 的工程落点：数据质量与分布，决定你能走多远。_

21. **任务定义**：你能把业务需求改写成清晰的“输入-输出-约束-评价标准”吗？
22. **数据来源**：你的高质量样本来自哪里（真实日志、专家标注、合成数据、规则生成）？
23. **覆盖面设计**：你如何设计样本覆盖“常见请求、边界条件、长尾、对抗输入”？
24. **难度分层**：你如何把数据分成易/中/难，并用它控制训练节奏与评测解释性？
25. **去重策略**：你如何定义“重复”（文本相似、语义相似、模板相同）并决定去重阈值？
26. **数据污染**：你的评测集如何防止泄漏进训练集？线上回流样本如何隔离？
27. **格式规范**：你如何制定一套可机器校验的数据 schema（字段、约束、版本）？
28. **质量审计**：你如何抽样审计数据质量，并把问题归因到“标注、合成、清洗、采集”？
29. **合成数据边界**：你如何判断合成数据是在“补覆盖”还是在“放大偏见与幻觉”？
30. **数据闭环**：你如何把线上失败案例变成下一轮训练/评测的“高价值样本”？

---

## 四、 SFT 与 PEFT 实战 (SFT / LoRA / QLoRA)

_让通用模型变专家：少卡也能干出大事，但要懂方法论。_

31. **SFT 目标**：你希望 SFT 改变的是“风格、格式、领域术语、决策边界”中的哪一个？
32. **指令模板**：你的 prompt 模板如何避免“把答案写进问题里”，同时保证可复现？
33. **负样本设计**：你如何构造“错误示例/反例”，让模型学会拒答、追问或澄清？
34. **LoRA 适用性**：为什么 LoRA 往往足够？哪些场景你会考虑全量微调？
35. **LoRA 位置选择**：你会如何选择 target modules，并用实验验证选择对效果的影响？
36. **超参搜索**：rank/alpha/dropout/learning rate 你会如何做“最小成本”的搜索策略？
37. **数据配比**：混合通用指令与领域数据时，你如何防止“能力遗忘”？
38. **训练稳定性**：loss 看起来正常但效果变差时，你会优先排查哪些信号？
39. **多任务冲突**：当两个能力互相拉扯（更严谨 vs 更流畅），你如何做权衡与隔离？
40. **回滚机制**：你如何设计模型版本管理与回滚，让一次坏微调不会拖垮生产？

---

## 五、 偏好对齐与安全 (DPO / RLHF / Guardrails)

_让模型“更像你想要的员工”，而不是“更像它想象的专家”。_

41. **偏好数据**：你如何定义“更好”的答案，是正确性、可读性、合规性、还是可执行性？
42. **成对偏好**：你会如何设计 (chosen, rejected) 样本，让它们只差一个关键因素？
43. **DPO 适配性**：在你的场景里，DPO 相对 RLHF 的最大优势是什么？
44. **拒答策略**：你如何区分“应该拒答”和“应该追问”，并把它变成可训练的行为？
45. **安全边界**：你的业务有哪些不可触碰的红线（隐私、医疗、金融、未成年人），如何落到策略里？
46. **提示注入**：你会如何构建一套 prompt injection 的测试集来打穿你的系统？
47. **越权与工具安全**：当模型能调用工具时，你如何做权限控制、参数校验与审计？
48. **一致性**：同一用户不同轮次问法不同，模型行为不一致时你如何定位问题根因？
49. **红队流程**：你如何组织红队测试（角色、目标、回放、修复），并形成制度化流程？
50. **安全与体验平衡**：过度保守会伤体验，你如何用数据决定“收紧/放松”？

---

## 六、 评测与迭代闭环 (Evaluation)

_没有评测闭环的“炼丹”，只是在自我感动。_

51. **离线评测集**：你的评测集如何覆盖“正确性、格式、鲁棒性、安全性、成本”五类目标？
52. **评分方式**：你何时用规则打分，何时用 LLM-as-a-judge，何时必须人工复核？
53. **一致性校准**：如果用模型当裁判，你如何控制评审偏差与版本漂移？
54. **可重复实验**：你如何固定变量（prompt、解码参数、检索配置）让实验可复现？
55. **误差分桶**：你会如何把失败案例分成“知识缺失、检索失败、推理失败、格式失败、工具失败”？
56. **在线指标**：上线后你最该盯的 3 个指标是什么（转化、投诉、工单、时延、成本）？
57. **A/B 设计**：你如何避免 A/B 被流量差异、季节性、缓存等因素污染？
58. **回放系统**：你如何做请求回放与对比评测，复现线上问题并验证修复有效？
59. **质量门禁**：你如何定义“可上线阈值”，以及每次发版必须过的 gate？
60. **失败样本回流**：你如何把线上失败样本优先级排序，让数据闭环真正提升效果？

---

## 七、 推理系统与性能 (Inference Optimization)

_真正值钱的能力：让模型跑得更快、更便宜、更稳定。_

61. **延迟分解**：一次请求的总延迟，模型推理与系统开销各占多少？你怎么测出来？
62. **吞吐 vs 延迟**：你在什么场景优先吞吐（batching），什么场景必须低延迟（实时）？
63. **KV Cache**：KV Cache 的显存开销如何随上下文长度增长？它如何影响并发上限？
64. **连续批处理**：你如何理解“动态 batching”，它为什么能显著提升吞吐？
65. **量化选型**：FP16/INT8/INT4/FP8 你如何根据硬件与精度需求做取舍？
66. **量化验收**：你如何验证量化后“没崩”，并找到最敏感的任务与样本类型？
67. **投机采样**：你的业务适合 speculative decoding 吗？如何判断“加速收益 > 系统复杂度”？
68. **缓存策略**：你会缓存什么（prompt、检索结果、tool 结果、completion），失效策略怎么定？
69. **多模型路由**：你如何设计“便宜模型优先”的路由策略，同时保证关键请求质量？
70. **容量规划**：你如何估算需要多少 GPU/实例才能扛住峰值，并给出成本预测？

---

## 八、 成本与可靠性工程 (Cost & Reliability)

_工程不是做出 demo，而是做出不会半夜把你叫醒的系统。_

71. **成本账本**：你如何建立一张“单位请求成本”账本，并能解释成本波动原因？
72. **限流与熔断**：当上游流量爆了，你如何保证系统降级但不崩溃？
73. **重试策略**：重试何时救命、何时雪上加霜？你会如何设计幂等与退避？
74. **超时设计**：不同环节（检索、模型、工具）超时怎么定，定错会出现什么症状？
75. **可观测性**：你会记录哪些关键埋点来定位“慢、贵、错、胡说”分别是谁的锅？
76. **日志与隐私**：你如何在可调试与隐私合规之间取舍，并做脱敏与访问控制？
77. **降级路径**：模型不可用时，你的备选方案是什么（规则、模板、小模型、人工）？
78. **多供应商冗余**：如果你依赖外部 API，如何做多厂商切换与一致性测试？
79. **数据漂移**：业务输入分布变化时，你如何早发现、早报警，并快速修复？
80. **灾备演练**：你如何用演练验证你的“应急预案”不是纸上谈兵？

---

## 九、 RAG 与 Agent 工程 (RAG & Agents)

_模型是大脑，RAG/工具/流程才是手脚；“铸剑师”要会造整套兵器。_

81. **是否需要 RAG**：你的问题是“模型不会”还是“模型会但没证据/会瞎编”？
82. **知识边界**：你的知识库应该覆盖哪些内容，哪些不该放（隐私、动态、过期）？
83. **切分策略**：chunk size、overlap、结构化字段会如何影响召回与答案质量？
84. **检索评测**：你如何分别评估“召回质量”和“生成质量”，避免把锅都甩给模型？
85. **重排策略**：你什么时候需要 rerank？如何用指标证明它的收益大于成本？
86. **引用机制**：你如何让回答可引用来源，并防止“引用了但没用到”的假引用？
87. **工具调用成功率**：你如何测量工具调用的成功率、正确率与业务完成率三者差异？
88. **状态机 vs 自由 Agent**：什么场景适合流程编排，什么场景适合更自由的 Agent？
89. **循环与刹车**：Agent 进入死循环时，你如何检测、止损、并给用户合理反馈？
90. **权限与审计**：当 Agent 能执行高风险动作（改数据、发消息、下单），你如何做审计闭环？

---

## 十、 交付与职业护城河 (Delivery & Career)

_能把能力“产品化、制度化、可交接”，才是可持续的工程价值。_

91. **需求对齐**：你如何把“想要更聪明”翻译成可验收的需求与指标？
92. **技术方案书**：一份能落地的 LLM 方案应该包含哪些关键章节（数据、评测、成本、风险）？
93. **跨团队协作**：产品、法务、运营、标注团队分别关心什么，你如何用同一套指标沟通？
94. **上线 Checklist**：上线前你会检查哪些“最常翻车”的点，如何让它自动化？
95. **迭代节奏**：你如何把迭代拆成“每周可交付”，而不是做一个永远做不完的大版本？
96. **知识沉淀**：你如何把失败案例、提示模板、评测集、数据规范变成团队资产？
97. **质量文化**：你如何推动团队接受“先评测再上线”，而不是“感觉不错就发”？
98. **自我作品集**：如果你要证明自己是铸剑师，你会交付哪 3 个可展示的项目证据？
99. **技术选型复盘**：方案上线后效果一般，你如何复盘并把结论变成下一次更快的决策？
100. **长期成长**：在“推理算力占主导”的趋势下，你未来一年最该深挖的 2 个方向是什么？


# 7.2 词表大小（Vocabulary Size）对模型性能有何影响？如何平衡压缩率与计算开销？

在上一篇文章中，我们讨论了更换 Tokenizer 的代价。今天，我们将视角上移，探讨一个更具工程深度的决策问题：**在训练一个新模型时，词表大小（Vocab Size）到底设多少才合适？**

从 GPT-2 的 50k，到 Llama 2 的 32k，再到 Qwen 和 Llama 3 激进的 100k+，业界似乎呈现出一种“词表越来越大”的趋势。但这背后的代价是什么？是否存在一个“黄金比例”？

本文将从**压缩率（Compression Rate）**与**计算开销（Computational Cost）**的博弈出发，为你揭示词表大小背后的 Scaling Laws。

---

## 一、 核心博弈：大词表 vs 小词表

词表大小的选择，本质上是在做一道**“空间换时间”**（或者反过来）的算术题。

### 1. 大词表的诱惑（High Compression Rate）
假设我们将词表从 32k 增加到 100k。
*   **现象**：更多的常见单词、短语甚至汉字有了独立的 Token ID，不再需要切碎。
    *   `unbelievable` (32k) -> `un`, `believ`, `able` (3 tokens)
    *   `unbelievable` (100k) -> `unbelievable` (1 token)
*   **收益**：
    1.  **序列变短**：同样的文本，切分出来的 Token 总数变少了。这意味着模型的**有效上下文窗口（Effective Context Window）变大了**。原本只能塞进 1000 个单词，现在能塞进 1500 个。
    2.  **推理加速**：生成同样的 1000 个单词，模型需要跑的步数（Forward Pass）变少了。这是实打实的端到端延迟降低。
    3.  **语义完整**：减少了碎词，模型更容易捕捉完整的语义信息。

### 2. 大词表的代价（High Computational Cost）
既然大词表这么好，为什么不设成 100 万？因为代价高昂。
*   **参数爆炸**：
    *   Embedding 层：`[V, d_model]`。
    *   Output Head：`[d_model, V]`。
    *   假设 `d_model = 4096`，词表从 32k 涨到 128k，参数量增加了 `(128000 - 32000) * 4096 * 2 ≈ 7.8 亿`。
    *   这接近 1B 参数了！对于小模型（如 7B），这会显著挤占用于推理的核心参数比例。
*   **训练变慢**：
    *   在 Output 层计算 Softmax 时，需要计算 128k 个类别的概率。这在大 Batch Size 下是显著的计算瓶颈。
*   **稀疏性问题（Sparsity）**：
    *   词表越大，每个 Token 在训练数据中出现的频率就越低。长尾 Token 可能几亿个语料才出现一次，导致其 Embedding 训练不充分（Undertrained），容易成为模型的“知识盲区”。

---

## 二、 业界的 Scaling Laws 与经验法则

OpenAI 和 DeepMind 等机构的研究表明，词表大小并非孤立存在，它与**计算预算（Compute Budget）**和**模型参数量**密切相关。

### 1. Chinchilla 的启示
DeepMind 的 Chinchilla 论文虽然主要讲参数与数据的配比，但其后续研究暗示：**最优词表大小与模型大小成正比。**
*   **小模型（< 3B）**：不建议用太大词表（如 >50k）。因为 Embedding 层参数占比过高，导致 Transformer 层的参数被“稀释”，逻辑推理能力反而下降。
*   **大模型（> 7B）**：由于 Embedding 层参数占比微乎其微（70B 模型中占比不到 2%），可以大胆使用大词表（如 100k+）来换取推理速度和长窗口优势。

### 2. 压缩率的边际递减效应
随着词表不断增大，压缩率的提升是**对数级**的（Logarithmic），而参数量的增加是**线性**的（Linear）。
*   从 32k 增到 64k，压缩率可能提升 20%。
*   从 64k 增到 128k，压缩率可能只提升 10%。
*   从 128k 增到 256k，可能只提升 5%，但参数量却翻倍了。

**经验法则**：通常追求每个 Token 平均代表 **3-4 个字符（bytes）** 左右的压缩效率是比较经济的平衡点。

---

## 三、 典型案例分析：Llama 2 vs Llama 3

让我们看看业界最顶尖的模型是如何选择的。

### 1. Llama 2 (32k Vocab)
*   **策略**：保守。
*   **背景**：主要针对英语优化。32k 足够覆盖常用英语单词。
*   **痛点**：多语言支持极差，中文切分效率低，处理代码时效率一般。

### 2. Llama 3 (128k Vocab)
*   **策略**：激进。
*   **改变**：
    *   **TIKTOKEN 算法**：采用了类似 GPT-4 的 BPE 算法，对空格、特殊符号处理更高效。
    *   **大幅扩容**：直接跳到 128k。
*   **收益**：
    *   **编码效率提升 15%-30%**：意味着相比 Llama 2，同样的 8k 上下文窗口，Llama 3 能读更多的内容。
    *   **多语言能力增强**：虽然官方仍主打英语，但大词表自然地覆盖了更多其他语言的高频词。

---

## 四、 给中级工程师的实战建议

如果你负责训练或微调模型，在词表问题上可以参考以下决策树：

1.  **从零训练小模型（< 1B）**：
    *   **建议**：Vocab Size 控制在 32k - 50k。
    *   **理由**：别让 Embedding 吃掉所有显存，把参数留给 Attention 和 FFN 这种“干活”的层。

2.  **从零训练中大模型（> 7B）**：
    *   **建议**：Vocab Size 可以上探到 100k - 128k。
    *   **理由**：推理速度（Throughput）是王道。参数量的这点增加对大模型来说九牛一毛，但推理时 Token 数减少带来的加速是实打实的。

3.  **多语言/代码特定模型**：
    *   **建议**：必须扩大词表。
    *   **理由**：代码中的缩进（4空格、8空格）、特殊标识符，以及中文/日文等字符密集型语言，非常依赖大词表来降低序列长度。

4.  **微调现有模型**：
    *   **建议**：**不动词表**，除非你是做极为垂直的领域（如纯古文、纯分子式）。
    *   **理由**：扩充词表需要重新训练 Embedding，这比普通的 LoRA 微调要麻烦得多，且容易破坏原模型的稳定性。

## 结语

词表大小的选择，没有绝对的“最好”，只有**“最适合”**。它是计算效率、显存占用、训练难度和模型能力之间的一场精妙的走钢丝。

对于中级工程师而言，看到 Llama 3 选择了 128k 词表，不应只看到“数字变大了”，而应看到这是 Meta 在拥有足够算力和数据支撑下，为了极致的**推理效率（Inference Efficiency）**所做的深思熟虑的 Trade-off。

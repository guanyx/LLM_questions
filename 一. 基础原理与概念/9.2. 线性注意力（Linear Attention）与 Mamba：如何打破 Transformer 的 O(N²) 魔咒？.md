# 线性注意力（Linear Attention）与 Mamba：如何打破 Transformer 的 O(N²) 魔咒？

在讨论“上下文窗口”时，我们反复提到一个痛点：标准 Transformer 的计算复杂度是 **$O(N^2)$**。这意味着窗口长度翻倍，计算量翻四倍。这就像一道无形的墙，锁死了大模型向无限上下文进化的道路。

于是，中级工程师们不禁会问：**“一定要算这个 $N^2$ 吗？有没有 $O(N)$ 的方法？为什么 Mamba、RWKV 这种线性架构还没能取代 Transformer？”**

本文将深入底层，剖析这场关于“效率”与“质量”的架构之争。

---

## 一、 为什么 Transformer 是 $O(N^2)$？—— 昂贵的“回头看”

要理解优化的本质，先得知道瓶颈在哪。

标准 Attention 的核心公式是：
$$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d}})V $$

这里有个关键步骤：$Q$（查询）和 $K^T$（键）相乘。

- $Q$ 是 $N \times d$ 矩阵（$N$ 是序列长度，$d$ 是维度）。
- $K^T$ 是 $d \times N$ 矩阵。
- **它们相乘得到一个 $N \times N$ 的分数矩阵**。

这个 $N \times N$ 矩阵代表了“每一个词”与“其他所有词”的关系。**这就是 $O(N^2)$ 的万恶之源。** 它不仅计算慢，还要占用巨大的显存来存储这个矩阵。

---

## 二、 线性注意力（Linear Attention）：换个顺序算乘法

聪明的数学家们发现，矩阵乘法是有**结合律**的：$(AB)C = A(BC)$。

在标准 Attention 里，我们是先算 $(QK^T)$，得到 $N \times N$，再去乘 $V$。
那如果我们**先算 $(K^T V)$** 呢？

1.  $K^T$ 是 $d \times N$，$V$ 是 $N \times d$。
2.  它们相乘得到一个 $d \times d$ 的矩阵！
3.  然后 $Q$ 再去乘这个 $d \times d$ 矩阵。

**奇迹发生了！**
整个过程中，我们从来没有生成过 $N \times N$ 的大矩阵，只生成了一个与 $N$ 无关的 $d \times d$ 矩阵。
**计算复杂度瞬间变成了 $O(N)$！** 无论窗口多长，计算量只随长度线性增长。

这就是线性注意力（Linear Attention）最朴素的原理：**通过核技巧（Kernel Trick）或改变计算顺序，避开 $N \times N$ 矩阵的显式计算。**

---

## 三、 Mamba 与 SSM：RNN 的“借尸还魂”？

除了 Linear Attention，另一派试图复兴 RNN（循环神经网络）。最新的代表就是 **Mamba**（基于状态空间模型 SSM）。

### 1. 它是怎么做到 $O(N)$ 的？

Mamba 的核心思想是：**压缩记忆**。

- **Transformer**：把所有历史记录都摆在桌面上（KV Cache），每次都要重新翻看一遍。
- **Mamba/RNN**：把历史记录压缩成一个固定的“状态（State）”。每读一个新字，就更新一下这个状态，然后把旧字扔掉。

不管读了多少书，它只需要维护这个固定大小的“状态”。因此，它的推理内存占用是恒定的，推理速度也是恒定的 $O(1)$（生成每个 token 的时间），总复杂度是 $O(N)$。

### 2. Mamba 的突破

以前的 RNN（如 LSTM）因为无法并行训练（必须等上一个字算完才能算下一个），被 Transformer 淘汰了。
Mamba 的天才之处在于，它设计了一种特殊的数学结构，使得**训练时可以并行（像 Transformer 一样快），推理时可以递归（像 RNN 一样省）**。

---

## 四、 灵魂拷问：既然这么好，为什么 Transformer 还是霸主？

既然 Linear Attention 和 Mamba 都能做到 $O(N)$，为什么 GPT-4、Gemini、Llama 3 依然坚守 $O(N^2)$ 的 Transformer？

这是因为 $O(N)$ 架构付出了隐形的代价：**“召回能力”的衰退**。

### 1. 压缩是有损的

- **Transformer** 的 $N \times N$ 矩阵虽然大，但它**无损**地记录了任意两个词之间的关系。它想看哪里就看哪里，拥有完美的“随机访问内存”。
- **Linear Attention / Mamba** 为了降维，必须把历史信息压缩到一个固定大小的 $d \times d$ 矩阵或状态里。**这是一种有损压缩。**

### 2. “大海捞针”能力变弱

在“长窗口大海捞针”测试中（比如在 10 万字里找一个密码），Transformer 几乎能做到 100% 准确，因为它保留了所有原始信息。
而线性架构往往表现不佳。当序列太长时，早期的信息在不断的“压缩更新”中逐渐模糊，导致**遗忘**。

### 3. 训练稳定性的挑战

Transformer 的 Softmax 机制非常鲁棒，梯度传播很稳定。而线性 Attention 去掉 Softmax 后，容易出现数值不稳定、梯度爆炸或消失的问题，训练难度大得多。

---

## 五、 总结与展望

这场架构之争，本质上是**“无损高成本” vs “有损高效率”**的权衡。

- **Transformer ($O(N^2)$)**：是目前的**精度天花板**。只要算力允许，它依然是处理复杂推理、高精度召回任务的首选。
- **Mamba / Linear Attention ($O(N)$)**：是未来的**效率之星**。它们在超长文本（如基因序列分析、百万字小说续写）和端侧设备（手机、机器人）上具有统治级的潜力。

**未来的终局可能是“混合架构”**：
比如 **Jamba**（Jamba = Mamba + Transformer），在大部分层使用 Mamba 处理海量背景信息，只在关键的几层使用 Transformer 进行高精度的“注意力聚焦”。

这样，我们或许能同时拥有 $O(N)$ 的速度和 Transformer 级别的“智力”。

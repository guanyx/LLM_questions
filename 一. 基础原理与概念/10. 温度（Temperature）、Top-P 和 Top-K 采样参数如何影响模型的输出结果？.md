# 温度（Temperature）、Top-P 和 Top-K 采样参数如何影响模型的输出结果？

> 💡 **核心摘要**：解码策略（Decoding Strategy）是 LLM 从“概率分布”迈向“确定性文本”的最后一公里。Temperature 控制分布的**平滑度（熵）**，决定了模型的“胆量”；Top-K 进行**硬截断**，剔除绝对的长尾；Top-P 进行**动态截断**，保留核心语义集。三者协同工作，本质上是在**多样性（Diversity）**与**质量（Quality/Coherence）**之间寻找最佳平衡点。

---

## 一、 本质回归：从 Logits 到“下一个词”

在深入参数之前，我们需要先回到模型推理的本质。当 LLM 预测下一个 token 时，它输出的不是具体的字，而是一个巨大的向量——**Logits**（未归一化的对数概率）。

假设词表大小为 $V$（例如 32k 或 100k），模型的输出就是 $V$ 维的向量 $z$。为了将其转化为概率分布 $P$，通常使用 **Softmax** 函数：

$$
P_i = \frac{\exp(z_i / T)}{\sum_{j} \exp(z_j / T)}
$$

其中 $T$ 就是我们常说的 **Temperature**。

采样参数的作用，就是在这个概率分布生成之后、真正进行采样选择之前，对这个分布进行**整形（Reshaping）**和**截断（Truncation）**。

---

## 二、 深度拆解：三大参数的物理含义

### 1. Temperature（温度）：熵的调节器

Temperature 是直接作用于 Softmax 公式的缩放因子。它控制的是概率分布的**陡峭程度**。

- **$T < 1$（低温）**：**放大差距**。高概率的词会变得概率更高，低概率的词概率更低。分布变得尖锐（Peaky）。
  - **极端情况 $T \to 0$**：退化为 **Greedy Decoding（贪婪搜索）**，每次只选概率最大的词。
  - **表现**：模型变得保守、自信、确定性强，但容易重复。适用于代码生成、数学解题等追求标准答案的场景。
- **$T > 1$（高温）**：**缩小差距**。高低概率词之间的差距被拉平。分布变得平坦（Flat）。
  - **极端情况**：当 $T \to 2$ 时，分布趋向于均匀，模型变得极度发散。
  - **表现**：模型变得奔放、富有创造力，但也更容易胡言乱语（Hallucination）。适用于创意写作、头脑风暴。

**【工程视角】**：Temperature 不改变相对排名，只改变被选中的概率绝对值。在大多数 API（如 OpenAI）中，范围被限定在 **0 到 2** 之间。

### 2. Top-K：粗粒度的硬截断

Top-K 是一种简单粗暴的策略。在采样前，仅保留概率最高的 $K$ 个 token，将剩余所有 token 的概率归零，然后对这 $K$ 个 token 的概率重新归一化。

- **机制**：无论分布如何，只看前 $K$ 名。
- **局限性**：
  - **分布平坦时**（例如“今天天气...”后面接很多词都合理），$K$ 太小会丢掉很多合理的词，导致表达单调。
  - **分布尖锐时**（例如“人工智能的缩写是...”后面几乎只有 "AI"），$K$ 太大可能会引入完全不相关的噪声词。
- **现状**：在现代 LLM 实践中，Top-K 通常作为第一道防线，用于剔除明显的低概率长尾，但往往不单独使用，而是配合 Top-P。

### 3. Top-P (Nucleus Sampling)：自适应的动态截断

为了解决 Top-K 缺乏适应性的问题，Holtzman 等人提出了 Top-P（核采样）。它不规定保留多少个词，而是规定保留**累积概率达到 $P$** 的最小 token 集合。

- **机制**：
  - 将词按概率从高到低排序。
  - 逐个累加概率，直到总和超过阈值 $P$（例如 0.9）。
  - 仅在这个集合内采样。
- **优势**：
  - **自适应**：当模型很确定时（分布尖锐），候选集很小（可能只有 1-2 个词）；当模型不确定时（分布平坦），候选集会自动变大，包含更多选择。
- **表现**：在保证语言流畅性（剔除极低概率的离谱词）的同时，最大限度保留了语境的多样性。

---

## 三、 协同效应：参数生效的流水线

在实际的工程实现（如 HuggingFace Transformers, vLLM）中，为了让参数发挥预期的协同作用，生效顺序至关重要。标准的 Pipeline 顺序通常是：

1.  **Temperature 缩放**：首先应用 Temperature 调整 Logits。
    - 这一步改变了分布的形状（尖锐或平坦）。**这一点非常关键**，因为后续的 Top-P 依赖于通过 Temperature 调整后的概率分布。
    - 如果 T < 1，分布变尖，Top-P 选中的候选集会自动缩小；如果 T > 1，分布变平，Top-P 候选集会自动扩大。
2.  **Top-K 过滤**：保留概率最高的 $K$ 个 token。（例如 $K=50$）。
    - 这一步剔除长尾，防止极低概率的词进入后续计算。
3.  **Top-P 过滤**：在剩下的 token 中，截断累积概率达到 $P$ 的集合。（例如 $P=0.9$）。
    - 这是最后一道防线，确保采样的多样性与合理性。
4.  **最终采样**：在经过上述层层筛选和重新归一化的分布中，进行多项式采样（Multinomial Sampling）。

> **注意**：虽然 Top-K 是基于排名的（不受 Temperature 影响），但在标准库（如 Transformers）实现中，通常将其放在 Temperature 之后、Top-P 之前。关键在于 **Temperature 必须在 Top-P 之前**，否则 Temperature 就无法动态控制 Top-P 的候选集大小了。

---

## 四、 产业实战：如何配置最佳参数？

作为技术负责人，在落地应用时，不能只懂原理，要有“手感”。以下是不同场景的最佳实践建议：

| 业务场景                | 核心诉求         | 推荐配置策略                                                   | 理由                                                                                              |
| :---------------------- | :--------------- | :------------------------------------------------------------- | :------------------------------------------------------------------------------------------------ |
| **代码生成 / 数学推理** | 准确性、逻辑严密 | **Temp $\approx$ 0 (0.1-0.2)**<br>**Top-P = 1.0** (或不开启)   | 这类任务通常有“标准答案”。高 Temp 会导致变量名瞎编、逻辑跳跃。低温能强迫模型走最稳妥的路径。      |
| **知识问答 / 客服**     | 事实性、稳重     | **Temp $\approx$ 0.3 - 0.5**<br>**Top-P $\approx$ 0.8 - 0.9**  | 需要一定的自然度，不能像机器人一样僵硬，但绝对不能产生幻觉。中低温配合较为严格的 Top-P 比较安全。 |
| **创意写作 / 角色扮演** | 多样性、惊喜感   | **Temp $\approx$ 0.7 - 1.0**<br>**Top-P $\approx$ 0.9 - 0.95** | 允许模型稍微“发散”一下。高温能激发模型的联想能力，Top-P 保证不至于离题太远。                      |
| **翻译**                | 信达雅           | **Temp $\approx$ 0.3**                                         | 翻译需要忠实原文，不需要太多的自由发挥，但为了流畅度，不能完全贪婪搜索。                          |

### 常见误区与调优心法

1.  **不要同时把 Temp 和 Top-P 调得很低**：这会导致生成内容极其单调，甚至陷入无限循环（Repetition Loops）。
2.  **Top-P 比 Top-K 更重要**：在大多数通用场景下，微调 Top-P 的收益比微调 Top-K 大。通常建议固定 Top-K（如 40 或 50），主要通过 Top-P 控制多样性。
3.  **Temperature 是“放大器”**：如果模型本身微调得不好，容易产生幻觉，调高 Temp 会成倍放大幻觉；如果模型微调得很好，调高 Temp 才能带来高质量的创造力。**参数救不了烂模型。**

---

## 五、 前沿探索：超越传统采样

虽然 Temp/Top-P/Top-K 是标配，但业界并未停止探索：

- **Min-P Sampling**：最近在开源社区（如 `llama.cpp`）非常火。它不设固定的 Top-P 累积阈值，而是设定一个**相对于最高概率 token 的比例阈值**（例如 $0.05 \times P_{max}$）。任何概率低于这个值的 token 都会被剔除。这被认为比 Top-P 更符合直觉，更能动态适应分布。
- **Typical Sampling**：基于信息论的采样，旨在选择那些“信息量”接近预期的词，而不是单纯概率高的词。
- **Mirostat**：一种动态调整 Temperature 的算法，旨在让生成的文本保持恒定的**困惑度（Perplexity）**，避免生成过程中质量忽高忽低。

## 结语

理解采样参数，本质上是理解**“确定性”与“可能性”的博弈**。

- **Temperature** 决定了模型是在“利用”（Exploitation，选最好的）还是在“探索”（Exploration，试错）。
- **Top-P/K** 则是为了防止“探索”过度而设置的安全围栏。

在工程落地中，没有一套通用的“黄金参数”，只有最适合业务场景的“妥协方案”。

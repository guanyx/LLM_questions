# 我以前也在乱调 Temperature：后来我画了 5 张小图

每次看到别人分享参数，我都觉得像在背暗号：

- `temperature=0.7`
- `top_p=0.9`
- `top_k=50`
- 再加个 `repetition_penalty=1.1`

问题是：这些数字到底在“掰”模型的哪根神经？

这篇文章想做一件很朴素的事：把采样参数当成一个“流水线”，画清楚每一步在干嘛，然后你就会发现很多看似玄学的问题，其实是顺序问题、边界问题、以及“你以为它会发生，但它没发生”的问题。

---

## 先画一张总图：采样到底发生在哪？

模型在每一步生成一个 token 的时候，先吐出一堆分数（Logits）。你可以把它理解成“候选词的记分牌”。

然后才轮到我们塞进去的一堆参数开始工作。

大概长这样：

```text
logits
  ↓ (可选：repetition penalty / presence/frequency penalty)
logits'
  ↓ / temperature
scaled logits
  ↓ softmax
probabilities
  ↓ top_k
filtered probs
  ↓ top_p
filtered probs'
  ↓ sample / argmax
next token
```

如果你只记住一个结论：很多“调参翻车”，不是因为参数本身，而是因为你以为它在 A 步生效，但它其实在 B 步生效。

---

## 1) Temperature：不是“更随机”，而是“把差距拉大/拉平”

Temperature 最容易被误解。它做的事情很简单：把 logits 除以一个数 $T$，然后再 softmax。

也就是说：

- $T < 1$：差距被放大（更尖）
- $T > 1$：差距被拉平（更平）

我喜欢用“相机变焦”来想：

- 低温：把第一名放大到占满画面
- 高温：把所有候选都拉到同一个画面里，让边缘选项也能被看见

如果某一步模型的 logits 是这样的（第一名和第二名差得不多）：

```text
):   10.0
args: 9.9
self: 5.0
```

你把 $T$ 调到 0.1，并不会凭空让第一名变成 99%——它只是把“已有差距”放大一点点。

所以当 logits 接近时，低温也可能仍然是“第一名 70%，第二名 30%”这种状态。

---

## 2) Top-K：把“长尾”直接剪掉

Top-K 很好理解：只留下前 K 个候选，其余一律丢掉。

它像是一个“只准前 K 名进决赛”的规则。

```text
sorted candidates:

1  ██████████████
2  ████████
3  ████
4  ██
5  █
6  .
7  .
8  .
```

Top-K 的好处是粗暴有效：有些离谱 token 你根本不想让它出现在候选里。

但它的问题也同样粗暴：

- 分布很平的时候：K 太小，会把合理选项也剪没
- 分布很尖的时候：K 太大，会把“垃圾长尾”也放进来

---

## 3) Top-P：不关心“前多少名”，只关心“把概率凑到多少”

Top-P（核采样）更像是“凑够一篮子合理的候选”，标准不是名次，而是累计概率。

```text
sorted probs:

1  0.52  ██████████
2  0.47  █████████
3  0.01  ▏
4  0.00  .

top_p=0.9  →  1+2 才够
```

Top-P 的好处是自适应：

- 模型特别确定：第一名就能凑够 0.9 → 候选集就 1 个（几乎等于贪婪）
- 模型不确定：要凑很久才够 0.9 → 候选集自动变大（保留表达空间）

这也是为什么工程上经常会“固定一个 top_k 当第一道滤网，然后主要靠 top_p 调口味”。

---

## 4) 低温下 Top-P 还有用吗？大部分时候没用，但它会在关键时刻救你

你可能见过这种建议：推理/代码用 `temp=0.1, top_p=0.9`。

直觉反应是：都 0.1 了，top_p 不是摆设吗？

大部分时候确实是摆设。

但当 logits 处于“真·难分伯仲”的状态时，Top-P 仍然会保留多个候选：

```text
):   10.0
args: 9.9

temp=0.1 之后可能还是：
):   73%
args: 27%

top_p=0.9  →  73% 不够，还得把 args 放进来
```

这会带来两个非常现实的收益：

- 有机会跳出局部最优：一些复读机死循环，本质是“重复词 51%，结束词 49%”。贪婪永远选 51%。
- 允许多条正确路径：比如推理/证明题里，模型可能确实有两条都不错的路（做 self-consistency 时尤其明显）。

Top-P 在这里更像一个安全阀：平时不响，响的时候往往是你最需要的时候。

---

## 5) Repetition Penalty：最容易踩的坑是“它在 temperature 之前”

很多框架的顺序是这样的：

```text
logits → repetition penalty → temperature → softmax → top_k/top_p
```

这意味着：你对 logits 做的“轻微惩罚”，会被低温放大。

想象一下：

- 原始 logits：A=10，B=9（A 稍微高一点）
- 你给 A 做一个不大的惩罚，让它变成 8.3
- 接下来温度是 0.1，相当于把差距乘了 10 倍

结果就是：你本来只是想“给 B 一点机会”，最后却变成“B 100% 胜出，A 直接出局”。

我自己的经验规则是：

- 温度越低，repetition penalty 越要保守（1.05~1.2 往往已经很猛了）
- 如果有 presence/frequency penalty（加减法），更容易控制
- 如果你真正要解决“卡住复读”的问题，n-gram 硬限制有时更直接

---

## 6) 一个经常被忽略的现实：混合采样会拖垮推理吞吐

如果你只在本地单用户跑模型，调参基本没有性能成本。

但在高并发推理服务里（continuous batching），采样策略不统一会很痛：

```text
same batch:

req A: greedy
req B: top_p
req C: top_k
req D: top_p + penalties
```

GPU 喜欢“大家做同一件事”。当一个 batch 里有人只需要 argmax，有人要排序/累加，有人要 top-k 选择，实际执行会出现分支、等待、以及更差的显存访问模式。

常见工程应对是：

- 先按策略分组：greedy 一组，top_p 一组，top_k 一组
- 尝试把采样步骤统一成更通用的算子（比如以排序为核心，后面做轻量后处理）

如果你在做在线服务，采样参数不只是“口味”，还是“成本”。

---

## 最后：一张“别想太多，先这么用”的配置表

| 你在干嘛                 | 推荐起步                     | 为什么                     |
| :----------------------- | :--------------------------- | :------------------------- |
| 代码 / 数学 / SQL        | `temp=0`                     | 你更想要稳定、可复现的输出 |
| 复杂推理（想要一点探索） | `temp=0.1~0.2`, `top_p=0.95` | 给模型留一点岔路口的空间   |
| 客服 / 知识问答          | `temp=0.3~0.5`, `top_p=0.9`  | 需要自然，但不想它瞎编     |
| 创意写作 / 角色扮演      | `temp=0.8~1.0`, `top_p=0.9`  | 允许发散，但用 top_p 兜底  |

如果你今天只想带走一句话：

温度管“差距”，Top-P 管“视野”，Top-K 管“别让太离谱的东西进来”，顺序管“你到底在调谁”。

# 我终于搞懂了：大模型为什么能“看几个例子就会做题”？

我第一次认真用大模型做任务的时候，最震惊的是这一点：

我只是给了它几条例子（比如“苹果对应红色、香蕉对应黄色”），它就像突然明白了规则，能继续往下答。

我没有训练它。
我也没有改它的参数。

它就“学会了”。

这件事通常被叫做“上下文学习”（In-Context Learning，简称 ICL）。

这篇文章想做的事很简单：
不用公式、不写代码，讲清楚一个问题：**模型到底靠什么做到这件事？**

---

## 1. 先说一个很重要的误会：它并不是在“变聪明”

当我们说“它学会了”，很容易脑补成：模型内部真的长出了一条新神经回路。

但 ICL 更像是：

你把一张“任务说明书”塞进了它的工作台。
它读完说明书，用现有能力完成任务。
说明书拿走，它就不会记得你刚才那套规则。

所以 ICL 的关键词其实不是“学习”，而是：

- 临时
- 依赖眼前这段文本
- 结束对话就消失

那它为什么能从说明书里读懂规则？我喜欢用三个小模型来理解。

---

## 2. 模型 1：它特别擅长“照猫画虎”（归纳头）

大模型里有一类很有意思的注意力结构，研究里常叫它“归纳头”（Induction Heads）。

你可以把它想象成一个超努力的“找规律小助手”：

它会盯着上下文里重复出现的模式。

比如你写了很多组“输入 -> 输出”，它就会不断问自己：

“我之前见过类似的输入吗？”
“那次输出是什么？”
“这次要不要也用同样的输出？”

这听起来很朴素，但它非常有用。

因为大部分任务在 Prompt 里表达出来，本质上就是：

“看，这里有一堆例子。现在轮到你了。按这个格式继续。”

当例子足够多（Many-Shot），这个“找规律小助手”就更容易稳定：

- 少量例子时，规则可能含糊。
- 例子变多后，规则越来越像“不得不承认就是这样”。

---

## 3. 模型 2：它在做一件很像“猜题型”的事（贝叶斯）

我经常觉得 ICL 更像“读题”。

同一句话，在不同题型里意思完全不一样。

如果你先给了几条翻译示例，后面再来一句新句子，它就会用“翻译题”的方式去处理。
如果你先给了几条情感分类示例，它就会用“分类题”的方式去处理。

你可以把大模型想象成一个见过无数题型的老考生。

上下文里的示例并不是在教它从零学技能，而是在告诉它：

“今天考的是这套题。”

这也解释了一个常见现象：

同一个问题，换一套示例，它的“性格”和“推理风格”就变了。

它不是变成了另一个模型。
它只是切换到了另一个“题型模式”。

---

## 4. 模型 3：它的“临时状态”真的会变（像一种隐式的临时微调）

还有一种说法我很喜欢：

即使参数不变，模型在读完一段上下文后，内部的激活状态也会变。

这就像你在看一道题的时候，会把一些东西放进脑内的“工作记忆”里：

- 题目要求
- 例子的格式
- 哪些是重点、哪些是噪声

模型也会做类似的事。

它并没有把新知识刻在石头上（参数里），但它把规则暂时写在了白板上（推理过程里的状态里）。

你会看到它“像被微调了一样”按规则答题，但只在这次推理里有效。

---

## 5. 那它和梯度下降训练，到底差在哪？

我觉得最简单的区分是：

- 梯度下降：把知识写进长期记忆（很慢，但会留下来）
- ICL：把规则写进工作记忆（很快，但会被擦掉）

训练像学骑车。
上下文学习像你临时借了一辆陌生的车，看了说明书，先骑起来再说。

还有一个我很喜欢的“递归结论”：

**大模型之所以会 ICL，是因为它在训练时被迫学会了“怎么利用上下文”。**

训练阶段逼着它做的事就是：

“看前文，猜下一个词。”

久而久之，它把“从上下文里提规则”的能力也练出来了。

---

## 6. 2025 年的现实问题：ICL 能不能变得便宜、稳定、可维护？

如果你把 ICL 用在真实系统里，很快会遇到两个麻烦：

1. 例子很多的时候，输入会变长，推理变慢。
2. 规则写在上下文里，怎么管理它、更新它、回滚它？

这两年出现了两个很实用的方向：

### 1) Context Caching：别每次都让模型从头读一遍

很多推理引擎会把“读前缀”产生的中间结果缓存起来。
这样你下一次用同一段说明书，就不用从头再读。

这对 Many-Shot 特别关键：你想靠大量例子换稳定性，就需要让“读例子”这件事尽量别重复付费。

### 2) DSPy：把“写提示词”变成“做工程”

我很喜欢 DSPy 的一点：它承认了一个事实——

Prompt 不是一句话，是一堆选择：

- 用哪些例子
- 例子怎么排序
- 指令怎么写
- 失败了怎么重试

把这些选择系统化、可评估、可迭代，才像软件工程。

---

## 结语

如果你只记住一句话，我希望是这一句：

**ICL 不是“模型突然学会了新技能”，而是“模型从上下文里临时读懂了规则”。**

归纳头让它擅长照猫画虎。
“猜题型”让它知道该用哪套能力。
临时状态让它在这一次推理里像“被微调了一样”稳定执行。

理解这些之后，你会更容易判断：

- 什么时候应该加例子
- 什么时候应该改格式
- 什么时候应该缓存前缀
- 什么时候应该让系统自动优化，而不是手动试错

# 一切皆是“填空题”：LLM 为什么总在预测下一个 Token？

如果你只记住一件事：**大多数 LLM 的核心工作就是“补全下一小段内容”**。

听起来很无聊，但它确实能解释很多现象：为什么它能写代码、为什么它有时候会一本正经地胡说、为什么换个提示词就像换了个人。

下面我用一种更“手绘笔记式”的讲法来串起来（偏科普、偏工程，不走宏大叙事）。

---

## 1. 先把游戏规则说清楚：什么是“预测下一个 Token”？

你给模型一段前缀，它给你补下一小段。然后把自己刚补的东西再喂回去，继续补。循环。

像这样：

```text
输入：床前明月光，疑是地__
输出：上
```

如果你继续让它补：

```text
输入：床前明月光，疑是地上
输出：霜
```

### Token 到底是什么？

Token 可以理解为“模型一次吐出来的最小颗粒”。它不一定等于一个字，也不一定等于一个词。

- 文本里：可能是一个汉字、一个词、一个词根（比如 ing）。
- 代码里：可能把缩进、符号、变量名拆得七零八落。
- 多模态里：图片可能被切成 patch，音频被切成片段，视频被切成帧/块。

所以“预测下一个 Token”在更大范围里，基本等价于：**预测序列的下一个片段**。

### 它不是“想到了答案”，它是在挑一个概率最高的续写

你输入“今天天气真”，模型内部大概在做这件事：

```text
下一个 token 的候选：
好: 0.33
热: 0.21
冷: 0.18
糟: 0.07
...
```

它会从这个分布里“选一个”（怎么选取决于采样策略、温度、top-p 等）。

### 自回归：为什么它能写一大段？

关键是“把自己刚写的继续当输入”：

```text
          ┌──────────────┐
上下文 ──►│   模型(一次)  │──► 下一个 token
          └──────────────┘
                 │
                 ▼
        把 token 拼回上下文
                 │
                 └──(重复很多次)
```

这就是我们看到“它一直写一直写”的原因：不是一次性生成整篇文章，而是很多次小步前进。

---

## 2. 为什么这个“填空题”会逼出看起来像“理解”的东西？

有个很实用的直觉：**想把下一步预测准，光背答案不够，你得学会一些规则**。

举个偏“世界模型”的例子：

> 小明把球放进红盒子。小红把球从红盒子拿出来放进蓝盒子。小明回来后，会去哪里找球？

要补全“合理”的续写，你需要掌握一些关于因果、位置、他人信念的结构。

这也是“压缩即理解”那种说法背后的意思：如果你能把大量现象压缩成少量规则，你就更容易在新句子上继续填空。

---

## 3. 那它怎么学会“思维链”（一步步推理）的？

很多训练数据确实只给“题目 + 最终答案”，看起来没给过程。

我更喜欢把“思维链”拆成两条来源：

### A. 互联网上其实有不少“过程文本”

教材、题解、论坛长回复、代码 review、设计文档……都在不停输出这种结构：

```text
因为 A，所以 B；
再考虑 C；
如果 D 不成立，就换方法；
最后得到答案。
```

这些数据不一定占多数，但在规模足够大时，能提供“推理长相”。

### B. 更关键的是后训练：让“写过程”变成一种更划算的策略

很多推理模型不是只靠预训练“看出来”的，而是在后训练阶段学会了一个朴素的策略：

- 直接猜答案：有时候对，但不稳定。
- 先写几步中间推导：更容易到达正确答案（还能被奖励/验证）。

尤其在数学、代码这类“有裁判”的领域（标准答案、单测、编译器），模型可以通过大量尝试找到更稳的路径。

你可以把它想象成：它学会了在输出最终结果前先打草稿（草稿可能对你不可见，也可能被展示出来）。

---

## 4. 它到底是“复读机”还是“发明家”？

我觉得把它当“复读机/天才发明家”二选一，会卡住。

更像是：**一个巨大的重组器 + 少量外推能力**。

### 组合式创新：其实是主力

很多“新东西”本来就是旧东西的新组合（产品、写作、工程方案都一样）。
模型在这方面很强：它见过的组合太多了，跨领域迁移也方便。

### 外推：在某些规则很强的地方会出现

比如加法、代码语法、某些形式逻辑：一旦它学到规则，就能做没见过的输入。

### 温度：它的“冒险程度”旋钮

- 温度低：更像“稳稳的续写”，常见、可靠、也更无聊。
- 温度高：更容易出怪点子，也更容易出错。

“创造力”和“胡说八道”经常是同一个机制的两面。

---

## 5. 既然它在做填空，我们怎么更容易拿到想要的输出？

一句话版本：**让你的提示词看起来像“你想要的答案的上半截”**。

我习惯用三种非常工程化的方法（不玄学）：

### 1. 让上下文变得“像你要的那类文本”

与其说“你是资深工程师”，不如把场景描述得具体一点：代码语言、输出格式、边界条件、你要的是建议还是实现。

因为你在做的是：把概率分布推向某个子集。

### 2. 给例子（Few-shot / Many-shot）

如果你要 JSON，那就给一两个“你满意的 JSON 例子”。模型很会学模式。

### 3. 用前缀锁住开头（非常好用）

你想让它列缺点，就直接把开头写死：

```text
iPhone 的最大缺点在于：
1.
```

它就很难突然跑去写“我无法评价”或者开始吹彩虹屁。

---

## 6. 这套机制有什么明显的坑？

这里我只列三个最常见、也最“工程上会踩”的：

### 1. 能效：很贵

人脑大概 20W，训练/推理大模型的耗电量大得多。
这不仅是成本问题，也在逼着大家做量化、稀疏、低比特、硬件新架构之类的事情。

### 2. Tokenizer：它经常让“数字/符号”变得很别扭

模型看到的可能是：

```text
9.11  ->  [9][.][11]
9.9   ->  [9][.][9]
```

它并没有天然的“数值直觉”。很多你觉得显然的数学比较，对它来说是“符号序列上的统计问题”。

### 3. 平庸倾向：最大似然会奖励“最常见的说法”

互联网里 99% 的通俗解释 + 1% 的严谨解释，模型天然更容易学到那 99%。

所以后训练里的偏好学习、以及“可验证任务”（代码单测、数学答案）就很重要：它们能把模型从“常见”推向“更好”。

---

## 7. 真的只能走“预测 token”这条路吗？

不一定。

有一条很有意思的思路是：别预测每个像素/每个 token，改为预测更抽象的表示（representation）。

你可以把两者差别粗暴画成这样：

```text
路线 A：预测细节（token/像素）
  今天下一帧每个像素是多少？

路线 B：预测抽象状态（表示/概念）
  猫会跳上桌子（不管尾巴每根毛的 RGB 怎么变）
```

这类世界模型/表示学习路线，可能更接近“在概念层面做计划”。

---

## 结尾：把它当“超大号补全器”，很多事就解释通了

当你把 LLM 看成一个极其强的“续写机器”，很多现象会变得更直观：

- 它能长篇输出：因为自回归循环。
- 它会胡说：因为它在补全一个看起来很像真的续写。
- 它会因提示词而性格大变：因为你在改变它接下来该“像谁”写。
- 它能推理：因为“写草稿”在很多任务上更划算、更容易被验证。

然后你就可以更工程化地使用它：不是求它“理解你”，而是把你想要的续写轨道铺好。

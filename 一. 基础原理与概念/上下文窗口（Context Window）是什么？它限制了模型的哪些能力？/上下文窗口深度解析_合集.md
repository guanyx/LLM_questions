# 上下文窗口：LLM 的临时记事本（以及它带来的各种麻烦）

我经常看到这样的对话：

> 我：我刚刚说过一条很重要的规则！  
> 模型：什么规则？（一脸无辜）

很多时候，问题不在“它不聪明”，而在于：它的桌面太小了。

这篇文章想用一种很直观的方式聊聊 5 件事：

- 上下文窗口到底是什么（把它当成临时记事本就行）
- 为什么它不能随便做大（算力和注意力都要钱）
- 为什么长上下文和 RAG 谁也替代不了谁（工程现实）
- 线性注意力 / Mamba 之类的东西在解决什么（以及代价）
- 如果真的有“无限上下文”，会发生什么（一点点脑洞）

---

## 1）上下文窗口是什么？把它想成“模型的桌面”

上下文窗口就是：模型这一次回答问题时，**能同时放在桌面上看的内容**。

桌面上放的东西通常包括：

- 你这次输入的内容
- 之前的对话历史（你说过什么、它回过什么）
- 它正在生成的输出（是的，输出也占桌面空间）

可以画个很简陋的小图：

```text
┌──────────────────────────┐
│  历史对话 + 你的新问题 +  │
│  模型正在写的回答         │
└──────────────────────────┘
           ↑
      这就是窗口
```

### “token”是什么？

窗口大小通常用 token 计。

你可以把 token 当成“模型计数用的文本小积木”，不等于字数也不等于词数，但大致相关。中文里一个汉字常常接近 1 个 token（但不总是）。

所以“4k 窗口”并不是 4096 个汉字，但你可以粗暴理解成：能放下几千到一万左右的中文字符，具体看内容。

### 为什么会“失忆”？

当桌面放不下更多东西时，系统只能把最早的一些内容从桌面上挪走（或者做摘要，或者直接丢掉）。

于是你就会看到经典场面：

- 你：请一直用 JSON 输出
- 你们聊了 30 轮
- 它：好的（然后输出了一篇散文）

---

## 2）为什么窗口不能无限大？两个很现实的原因

### 原因 A：注意力很贵（$O(N^2)$ 那种贵）

Transformer 的注意力机制有个很朴素的直觉：在生成每个 token 时，它要“回头看看桌面上的所有 token”，再决定这一步该写什么。

如果桌面上有 $N$ 个 token，它需要考虑的“关系”大概是 $N \\times N$ 这么多。

举个特别小的例子：$N = 4$ 时，关系数是 16。

```text
4 个 token：
a b c d

要考虑的配对：
(a,a) (a,b) (a,c) (a,d)
(b,a) (b,b) (b,c) (b,d)
(c,a) ...
```

当 $N$ 变成 10 倍时，关系数变成 100 倍。于是你会感受到两件事：

- 变慢（尤其是首字延迟：它要先“读完”再开始答）
- 变贵（算力、显存、以及最终账单）

### 原因 B：窗口很长时，模型会“走神”（Lost in the Middle）

很多模型在超长上下文里容易出现一个现象：记得住开头、记得住结尾，但中间的东西像“被揉成一团塞进抽屉”。

这有点像我自己开会：

- 开头：还行，我在听
- 中间：我在想中午吃什么
- 结尾：突然精神（因为快结束了）

所以“窗口更长”不自动等于“理解更好”。有时候只是更长、更慢、更贵。

---

## 3）那到底用长上下文，还是用 RAG？

我喜欢把它们想成两种工作方式：

- 长上下文：把整本书搬到桌面上，一口气读
- RAG：书放在书架上，先查目录/搜索，挑几页放桌上

### 长上下文（Long Context）：简单，但不便宜

优点：

- 架构更简单：不用切 chunk、不用向量库、不用重排
- 对“全局问题”更友好：比如“总结这本书的叙事结构”，你很难靠关键词检索做到

缺点：

- 成本高：你每问一句，可能都在让模型把厚书重新读一遍
- 延迟高：尤其是首字延迟（TTFT），用户体验会很明显地“卡”

### RAG：便宜快，但有一个大坑

优点：

- 输入更短：只把最相关的片段塞进窗口
- 响应更快：检索是毫秒级，整体也更稳定
- 数据更新更灵活：文档更新不需要训练模型

大坑：

- 检索不到就完蛋：模型根本看不到答案
- 容易碎片化：你只给了它几段，它就很难做“跨章节的整体比较”

### 所以结论是：看场景（真的）

一个很实用的分法：

- 数据量巨大（TB/PB 级）：你一定需要 RAG（桌面再大也放不下）
- 单本手册/单份财报这种“中等规模”：长上下文往往更舒服
- 最常见的工程解：RAG 先筛一轮，再把筛出来的内容塞进长窗口里精读

---

## 4）$O(N^2)$ 太贵怎么办？两条路：改括号，或者改记忆方式

### 路线 A：线性注意力（Linear Attention）= “把算式换个括号”

标准注意力里，有一个很要命的步骤会产生 $N \\times N$ 的大矩阵。

线性注意力的核心想法非常像小学数学：**能不能换个计算顺序？**

```text
标准（贵）：
(Q · K^T) · V    →  先得到 N×N

线性（便宜）：
Q · (K^T · V)    →  先得到 d×d
```

这样就能绕开那个巨大的 $N \\times N$ 矩阵，把复杂度从 $O(N^2)$ 压到更接近 $O(N)$。

但它通常也会带来新的问题：数值稳定性、表达能力、以及某些“超长文本找针”任务上的准确率。

### 路线 B：Mamba / SSM = “别把所有历史摊开，压缩成一个状态”

如果 Transformer 像“把所有历史都摊在桌上”，那么 Mamba 更像：

- 我不把每一页都摊开
- 我用一个固定大小的状态（state）记住“我目前读到哪里、我觉得重要的是什么”
- 然后继续往下读

这种思路在推理时很省内存、很省时间（尤其是长序列），但代价同样明显：压缩是有损的，早期细节可能会慢慢“融掉”。

我更愿意把它看成一种权衡：

- Transformer：贵，但像随机访问内存（想翻哪页翻哪页）
- Mamba：省，但像把笔记写成摘要（摘要写得好就很好，写得差就丢细节）

---

## 5）如果真的有“无限上下文”会怎样？

这是我最喜欢胡思乱想的部分。

现在的模型很像：

- 权重（weights）里装了一堆“长期记忆”（而且不太好更新）
- 上下文窗口装“短期记忆”（但很贵、很有限）

如果上下文真的可以无限大、而且便宜到像内存条一样，那就会出现一种很奇怪的系统形态：

- 模型更像一个“推理引擎”（擅长理解、推理、跟随指令）
- 知识更像一个“外接硬盘”（随用随读，随时更新）

这听起来很美好，但我觉得至少有两个现实问题会一直存在：

- 成本：即使算法更高效，读很多东西也需要时间
- 深度：读过不等于学会，有些能力可能仍然需要训练来“刻进”模型里

---

## 收尾：把它当成一个工具限制，而不是神秘设定

我更喜欢用一个朴素的方式总结上下文窗口：

- 它不是“模型性格”
- 也不是“模型态度”
- 只是一个很硬的资源限制：桌面大小、阅读速度、以及注意力开销

一旦你把它当成“桌面”，很多现象就能解释了：

- 为什么它会忘记早期的指令
- 为什么长文档问答会很慢
- 为什么 RAG 不会消失
- 为什么大家这么在意 $O(N^2)$

如果你要做工程落地，一个挺实用的心法是：

> 先想清楚你希望模型“看见什么”，再决定怎么把那部分内容放到它的桌面上。  
> 桌面很宝贵，别拿它当仓库。

# 动态采样（Per-Request Sampling）对 GPU 推理吞吐的隐形杀伤

> 💡 **核心摘要**：在单用户场景下，我们随意调整 Temperature、Top-P、Top-K 毫无压力。但在 **Continuous Batching**（如 vLLM, TGI）的高并发推理服务中，当一个 Batch 内的 128 个请求分别要求不同的采样策略时，GPU 面临着巨大的挑战。本文将从 **CUDA Kernel** 和 **Warp Divergence** 的底层视角，解析“混合采样”带来的性能损耗，并探讨推理引擎是如何通过**Kernel Fusion**与**Sorting**来硬抗这一挑战的。

---

## 一、 问题背景：当 Batch 不再“整齐划一”

在传统的深度学习训练中，一个 Batch 内的所有数据都共享相同的计算逻辑（比如都是做矩阵乘法，都是用同样的 Loss 函数）。这种**SIMT（Single Instruction, Multiple Threads）**模式是 GPU 最喜欢的。

但在 LLM 推理服务（Serving）中，情况完全变了：

- **Request A**：写代码，要求 `Temp=0`, `Top-P=1.0` (Greedy)。
- **Request B**：写小说，要求 `Temp=1.0`, `Top-P=0.9` (Nucleus)。
- **Request C**：聊天，要求 `Temp=0.7`, `Top-K=50`。

当推理引擎（如 vLLM）把这三个请求强行打包成一个 Batch 塞进 GPU 时，采样阶段（Sampling Layer）就会变成一个**算子噩梦**。

---

## 二、 底层瓶颈：Warp Divergence 与 显存跳变

### 1. 什么是 Warp Divergence（线程束分歧）？

NVIDIA GPU 以 **Warp（通常 32 个线程）** 为单位调度指令。如果 Warp 内的 32 个线程都在做同样的事情（比如都做加法），效率最高。

**如果我们在 Kernel 里写了这样的逻辑：**

```cpp
if (use_top_p) {
    // 复杂的排序和累加逻辑
    run_top_p_kernel();
} else if (use_top_k) {
    // 另一种排序逻辑
    run_top_k_kernel();
} else {
    // 简单的 Argmax
    run_greedy_kernel();
}
```

当一个 Warp 内的线程分别处理不同请求，有的走 `if`，有的走 `else`，GPU 就必须**串行化**执行这些分支。

- 先让一部分线程跑 Top-P，其他线程**空转等待（Stall）**。
- 再让一部分线程跑 Greedy，之前的线程空转。
- **结果**：并行度崩塌，计算资源严重浪费。

### 2. 显存访问的非连续性（Memory Coalescing 失效）

Top-P 和 Top-K 都需要对 Logits 进行排序或部分排序（Top-K Selection）。

- **Greedy**：只需要一个 `max()` 操作，扫描一遍显存即可，O(V)。
- **Top-P**：需要全排序或部分排序，然后累加。O(V log V) 或 O(V)。
- **Top-K**：需要选出前 K 个。

如果一个 Batch 里混合了这三种操作，显存访问模式（Memory Access Pattern）会变得极其杂乱，导致 L2 Cache 命中率下降，带宽利用率受损。

---

## 三、 业界解决方案：推理引擎是如何硬抗的？

为了解决这个问题，vLLM、TensorRT-LLM 等引擎采用了极其复杂的工程优化。

### 1. 策略分组与 Kernel 分离（Sorting by Strategy）

引擎通常不会在一个 Kernel 里用 `if-else` 处理所有逻辑。而是采用**“分而治之”**的策略：

1.  **CPU 侧预处理**：在 Launch Kernel 之前，CPU 会扫描当前 Batch 的所有请求，根据采样参数将它们**分组（Group）**。
    - Group 1: 所有 Greedy 请求。
    - Group 2: 所有 Top-P 请求。
    - Group 3: 所有 Top-K 请求。
2.  **分别 Launch Kernel**：
    - 先对 Group 1 的 Logits 所在的显存区域调用高效的 `Argmax Kernel`。
    - 再对 Group 2 的区域调用 `Top-P Kernel`。
    - 最后对 Group 3 调用 `Top-K Kernel`。
3.  **结果合并**：最后将各组生成的 Token ID 拼回原来的顺序。

**代价**：增加了 CPU 开销和 Kernel Launch 的次数（Kernel Launch Overhead）。但避免了严重的 Warp Divergence。

### 2. 更加通用的 Flash-Decoding 风格采样

有些激进的优化方案尝试统一算法：

- **将 Top-K 视为 Top-P 的特例**：虽然不完全等价，但可以通过统一的排序算子来处理。
- **基于 Cub/Thrust 库的 Segmented Sort**：利用高度优化的并行排序库，对整个 Batch 的 Logits 进行分段排序。一旦排好序，Top-P（累加截断）和 Top-K（索引截断）就变成了非常轻量的后处理操作。
- **优势**：排序是最重的操作，一旦统一了排序，后面的差异就很小了。

### 3. 限制参数组合（Trade-off）

为了极致性能，某些专用推理服务（如专用于数学竞赛的 API）可能会**限制用户的参数选择**。

- 例如：只允许调整 Temperature，强制 Top-P=1.0，Top-K=0。
- 这样整个 Sampling Kernel 就可以极度精简，没有任何分支判断。

---

## 四、 高级工程师的思考：未来的方向

从系统设计的角度来看，**Per-Request Sampling** 是云计算服务的必然需求，但也是性能优化的拦路虎。

未来的演进方向可能是：

1.  **Speculative Sampling（投机采样）的普及**：
    如果大部分请求都是 Low Temperature，我们可以用一个小模型快速生成（Draft），大模型只负责验证（Verify）。验证阶段通常只需要计算概率，不需要复杂的排序采样，从而绕过这个瓶颈。
2.  **专用采样硬件单元**：
    现在的 GPU 架构是为通用计算设计的。也许未来针对 LLM 的 NPU 会在片上集成专门的 **Top-K/Top-P Unit**（类似于 Tensor Core 加速矩阵乘法），硬件级解决排序和截断问题，彻底释放 Shader Core 的算力。

## 结语

当我们谈论 LLM 推理优化时，往往只盯着矩阵乘法（MatMul）和 Attention（FlashAttention）。但随着 Batch Size 的增大，**采样层（Sampling Layer）** 正在逐渐从“可以忽略不计”变成“不可忽视的长尾延迟来源”。

理解每一个采样参数背后的 **CUDA Cost**，是在高并发场景下榨干 GPU 性能的关键。

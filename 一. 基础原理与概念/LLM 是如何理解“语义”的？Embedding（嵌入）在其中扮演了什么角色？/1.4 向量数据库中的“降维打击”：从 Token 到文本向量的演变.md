# 1.4 向量数据库中的“降维打击”：从 Token 到文本向量的演变

你提出了一个非常核心且极具实战意义的问题：

> “既然 Embedding 是基于 Token 做的向量化（每个 Token 都有一个向量），那在向量数据库里检索‘一段文本’时，是如何做的？一个单词被拆成多个 Token 后，会有什么挑战？”

这直指 **RAG（检索增强生成）** 和 **向量数据库** 的核心痛点：**粒度不匹配（Granularity Mismatch）**。

- **LLM 的视角**：微观的，它是按 Token（字/词元）来理解世界的。
- **向量数据库的视角**：宏观的，它是按 Sentence（句子）或 Chunk（段落）来存储和检索的。

那么，我们是如何把“一堆 Token 向量”捏成“一个文本向量”的？这中间又会丢失多少信息？

---

## 一、 从 Token 到 Text：向量是如何“被代表”的？

假设我们有一句话：`"Apple is great"`。

在 LLM 眼里，它可能是 3 个 Token：`["Apple", "is", "great"]`。
每个 Token 对应一个 4096 维的向量。所以，这句话最初是一个 **3 × 4096 的矩阵**。

但是，向量数据库（如 Pinecone, Milvus）要求**每一条数据必须是一个向量**（1 × 4096），而不是一个矩阵。

这就需要一个压缩操作，学术上叫 **Pooling（池化）**。目前主流的方法有三种：

### 1. 民主投票法：Mean Pooling（平均池化）

这是最常用、也是最直观的方法。
把这句话里所有 Token 的向量加起来，然后除以 Token 数量，取平均值。

$$
V_{text} = \frac{V_{Apple} + V_{is} + V_{great}}{3}
$$

- **优点**：每个词都有发言权，雨露均沾。
- **缺点**：**稀释核心语义**。
  - 想象一句 500 个词的段落，其中只有 5 个词是关键词（比如“核聚变”、“点火”）。
  - 用平均法一算，这 5 个关键词的强烈信号就被其他 495 个无意义的词（“的”、“了”、“是”）给平均掉了。
  - 最后得到的向量，变成了一个“毫无特色”的平庸向量。

### 2. 精英代表法：CLS Token（CLS 标志位）

BERT 时代的经典做法。
在句子开头人为加一个特殊的 Token：`[CLS]`（Classification）。
训练模型时，强行告诉模型：**“请把你对整句话的理解，都压缩进 `[CLS]` 这个 Token 的向量里。”**

$$
V_{text} = V_{[CLS]}
$$

- **优点**：模型专门为了“概括全句”而训练过，语义浓度高。
- **缺点**：**容量瓶颈**。
  - 一个 Token 的向量（比如 768 维或 4096 维）能承载的信息量是有限的。
  - 让它去代表一个 8000 字的长文档？它根本记不住那么多细节，只能记个大概。

### 3. 最后一个词法：Last Token

这是 Decoder-only 模型（如 GPT、Llama）常用的变体。
因为 GPT 是从左往右读的，读到最后一个 Token `great` 时，它已经看过了前面的 `Apple is`。
所以，理论上最后一个 Token 的向量已经包含了前文的所有信息。

$$
V_{text} = V_{great}
$$

- **优点**：符合 GPT 的自回归生成逻辑。
- **缺点**：**近因效应（Recency Bias）**。
  - 模型往往对最近读到的内容印象最深，对开头的记忆已经模糊了。

---

## 二、 挑战：当一个单词被“大卸八块”

你提到了一个很敏锐的点：**Tokenization（分词）会把一个单词拆开**。

比如 `Unbelievable` 可能会被拆成 `["Un", "believ", "able"]`。

### 挑战 1：语义破碎

- `Un` 单独看表示“不”。
- `believ` 单独看可能是“相信”的词根。
- `able` 单独看表示“能”。

如果我们用 **Mean Pooling**，把这三个向量取平均，能不能得到 `Unbelievable`（难以置信）的含义？
**很难说。**
因为向量的加法并不总是等于语义的组合。`V(不) + V(信) + V(能)` 混合出来的向量，可能距离 `V(怀疑)` 更近，而不是 `V(惊讶)`。

**解决办法**：
现在的 Embedding 模型（如 OpenAI text-embedding-3 或 BGE-M3）通常是**经过专门训练的**。它们在训练时就见过 `["Un", "believ", "able"]` 经常一起出现，所以会强行把它们的组合语义向“惊讶”靠拢。但对于罕见词或新造词，这个问题依然严重。

### 挑战 2：检索时的“词不达意”

用户搜索：“How to **embed** data?”

- 用户输入里的 `embed` 是一个 Token。
- 数据库文档里的 `embeddings` 可能是 `["embed", "ding", "s"]` 三个 Token。

虽然它们共享 `embed` 这个词根，但在向量空间里，`V(embed)` 和 `V(embeddings)` 可能并不完全重合。
如果是简单的关键词匹配（BM25），这很好解决。
但在向量检索中，如果 Embedding 模型没训练好，它可能觉得 `embed`（嵌入动作）和 `insert`（插入）更近，而不是和 `embeddings`（名词）更近。

---

## 三、 总结

回到你的问题：

1.  **从 Token 到 文本向量**：

    - 这是一个**有损压缩**的过程。
    - 目前最常用的是 **Mean Pooling**（取平均），虽然简单粗暴，但在大量数据训练下效果惊人地好。
    - `[CLS]` 和 Last Token 也有应用，但不如 Mean Pooling 普适。

2.  **单词拆分的挑战**：

    - 会导致语义分散。
    - 依赖强大的 Embedding 模型（如 BGE, OpenAI）通过海量语料的共现训练，把破碎的 Token 向量“强力胶”粘在一起。

3.  **工程启示**：
    - 不要盲目相信向量检索。对于专有名词、精确匹配，向量往往不如倒排索引（Keyword Search）。
    - **混合检索（Hybrid Search = Vector + Keyword）** 才是目前的王道。

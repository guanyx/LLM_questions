# 1.1 动态 Embedding 的生成时机：从 Input Layer 到 Transformer 输出的演变

很多初学者在理解 Embedding 时，都会卡在一个看似简单却非常关键的问题上：

> “如果 Embedding 是查表（查字典）得到的，那它应该是死的（静态的）。为什么我们又说 LLM 的 Embedding 是动态的、上下文感知的？**这个‘变身’的过程究竟发生在哪一步？**”

这是一个直击灵魂的工程问题。如果你正在做 RAG（检索增强生成）或者向量数据库开发，搞清楚这一点至关重要。

本文将带你深入 LLM 的内部流水线，解剖一个 Token 从“静态 ID”进化为“动态语义向量”的全过程。

---

## 一、 第一步：Input Embedding Layer —— 仅仅是“查字典”

一切的起点，是 **Token ID**。

当你说“我爱苹果”时，分词器（Tokenizer）会把它变成一串数字，比如 `[23, 105, 998]`。
模型的第一层，叫做 **Input Embedding Layer**。

这一层本质上就是一个巨大的查找表（Lookup Table），或者说是一个矩阵 $W_e$，形状是 `[词表大小, 维度]`（例如 `[128000, 4096]`）。

### 这里的操作非常简单粗暴：

- 拿着数字 `998`（代表“苹果”）。
- 去矩阵里找到第 998 行。
- 把这一行向量复制出来。

**关键点来了：**
在这个阶段，**Embedding 是绝对静态的**。
无论你说“我爱苹果”，还是“苹果手机”，只要 Token ID 是 998，从这一层拿出来的向量 **完全一模一样**。

此时的“苹果”，还不知道自己是水果还是手机。它只是一个含糊不清的原材料。

---

## 二、 隐形的关键步骤：RoPE (旋转位置编码)

在进入 Transformer 之前，现代 LLM（如 Llama 3, Mistral, Qwen 2）都会做一个极其关键的操作：**注入位置信息**。

早期的 BERT 只是简单地把“位置向量”加到“词向量”上。但现在的模型普遍使用 **RoPE (Rotary Positional Embedding，旋转位置编码)**。

它不是简单的加法，而是通过数学上的**旋转操作**，把位置信息“刻”进向量的角度里。

- “我”在第 1 个位置，它的向量被旋转了 $\theta$ 度。
- “爱”在第 2 个位置，它的向量被旋转了 $2\theta$ 度。

这意味着，即使两个 Token 是一样的，因为它们在句子里的位置不同，它们进入 Transformer 时的初始状态就已经**微弱地不同**了。这为后续的动态演变埋下了伏笔。

---

## 三、 第三步：Transformer Layers —— 炼丹炉里的化学反应

真正的魔法发生在这一步。

携带了位置信息的静态向量，会被送入 LLM 的主体结构 —— **Transformer Layers**（通常有几十层，比如 Llama 3-70B 有 80 层）。

在这里，发生了一个叫做 **Self-Attention（自注意力）** 的计算。

### 什么是自注意力？

简单来说，就是让句子里的每一个词，都去“看”一眼其他所有词，然后根据关系远近，**混合**它们的信息。

- 当处理“苹果”这个词时，模型会发现句子里还有“手机”、“发布会”这些词。
- Attention 机制会计算出：“手机”和“发布会”对“苹果”很重要。
- 于是，模型会把“手机”和“发布会”的一部分向量信息，**加权融合**进“苹果”的向量里。

### 每一层都在“微调”

这个过程不是一次完成的，而是层层递进：

- **第 1 层**：可能只是简单地感觉到周围有“手机”这个词。
- **第 40 层**：开始理解“苹果”在这里大概率是指科技公司。
- **第 80 层**：完全确认了语义，此时“苹果”的向量里，已经深深地刻入了“科技”、“乔布斯”、“iPhone”等相关信息。

**结论：**
**“动态”不是在查表时发生的，而是在 Transformer 的层层计算中，通过不断吸纳上下文信息“进化”而来的。**

---

## 四、 第四步：Output Embedding —— 最终的“完成品”

经过几十层 Transformer 的洗礼，最后输出的那个向量（Last Hidden State），才是我们常说的 **Contextual Embedding（上下文嵌入）**。

此时的这个向量，虽然名义上还对应着“苹果”这个 Token，但它已经**脱胎换骨**了。

- 如果输入是“我吃了一个苹果”，输出的向量坐标会靠近【食物区】。
- 如果输入是“苹果发布了新手机”，输出的向量坐标会靠近【科技区】。

这两个向量在空间中的距离可能非常远，甚至毫无关系。这就是为什么我们说它是“动态”的。

---

## 五、 工程实战：RAG 中的前沿 Embedding 技术

这直接关系到你的应用效果和成本。

当你使用最新的 Embedding 模型（如 `text-embedding-3` 或 `bge-m3`）时，除了知道要取 Output Layer 之外，还需要了解一个前沿概念：**Matryoshka Embedding (俄罗斯套娃嵌入)**。

### 俄罗斯套娃嵌入 (Matryoshka Representation Learning)

传统的 Embedding 维度是固定的（比如 1536 维）。如果你想省钱、省内存，以前只能重新训练一个小模型。

但现在的模型（如 OpenAI text-embedding-3）采用了“套娃”技术。
它的输出向量**前 256 维**包含了最重要的信息，**前 512 维**包含了次重要的信息……

这意味着你可以**按需截断**：

- **存数据库时**：为了省钱，你可以只存前 512 维，精度损失极小（比如从 99% 降到 98%）。
- **精排时**：如果需要极致精度，再用完整的 1536 维。

这是一个巨大的工程红利，它允许你在**存储成本**和**检索精度**之间灵活切换，而不需要更换模型。

**避坑指南：**
在做 RAG 时，务必检查你使用的 Embedding 模型是否支持 Matryoshka 截断。如果不支持（如老旧的 BERT 模型），随意截断会导致检索效果雪崩。

---

## 总结

Embedding 的“变身”之旅：

1.  **Input Layer**：查字典，获取**静态**身份。
2.  **RoPE**：注入位置信息，通过旋转让向量携带“坐标”。
3.  **Transformer Layers**：化学反应，通过 Self-Attention 吸纳上下文。
4.  **Output Layer**：最终成型，生成**动态**语义向量。如果是先进模型，还支持**套娃截断**。

理解了这个过程，你就明白了为什么 LLM 计算量这么大 —— 它不是在简单的查表，而是在进行几百亿次的矩阵乘法，只为了给每一个词找到它在当前语境下最精准的那个坐标。

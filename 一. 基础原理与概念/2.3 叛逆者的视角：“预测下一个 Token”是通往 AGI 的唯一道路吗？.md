# 2.3 叛逆者的视角：“预测下一个 Token”是通往 AGI 的唯一道路吗？

在 AI 圈子里，几乎所有人都成为了 Transformer 和“Next Token Prediction”的信徒。OpenAI、Google、Anthropic 都在这条赛道上狂奔。

然而，有一位图灵奖得主始终站在对立面，大声疾呼：“**自回归大语言模型（Auto-Regressive LLMs）注定失败！**”

他就是 AI 三巨头之一、Meta 首席 AI 科学家 **Yann LeCun**。

为什么在 ChatGPT 如此成功的今天，他依然坚持认为“预测下一个 Token”是在绕远路？如果不预测 Token，我们还能预测什么？

## 一、 效率的拷问：我们在“死记硬背”还是在“学习概念”？

LeCun 的核心质疑在于**学习效率（Sample Efficiency）**。

**1. 巨大的浪费**
LLM 为了学会一个简单的物理常识（比如“水往低处流”），可能需要阅读上百亿字的文本。而一个人类小孩，只需要观察几次就能理解。
为什么？因为 LLM 是在**预测每一个字（Token）**。这就像为了学会开车，你不仅要背下交通规则，还要背下全世界每一条道路上的每一颗石子的位置。

**2. 离散与连续的鸿沟**
现实世界是连续的（光线、声音、运动），而 Token 是离散的符号。
强行把连续的世界切碎成离散的 Token 来预测，会丢失大量信息。

- _比如预测一段视频的下一帧：_ 现在的做法（如 Sora）是把像素变成离散的 Patches（Token）。但像素层面的变化是极度混乱和高熵的（树叶的轻微抖动）。
- _LeCun 认为：_ 智能体不应该预测每一个像素，而应该预测**抽象的特征（Representation）**。“我不需要知道树叶怎么抖，我只需要知道车在往前开。”

## 二、 替代方案：在“概念空间”里做预测（JEPA）

如果不是预测 Token，那该怎么办？LeCun 提出了 **JEPA（Joint Embedding Predictive Architecture，联合嵌入预测架构）**。

这是一个听起来很拗口，但理念很性感的架构。2024 年发布的 **V-JEPA（Video-JEPA）** 就是这一理念的最新验证。

**1. 放弃细节，抓住本质**
JEPA 不预测下一个 Token（$x_{t+1}$），也不预测下一帧像素。
它预测的是**下一个状态在特征空间里的向量（Representation）**。

- **LLM/Sora**：试图画出下一张图的每一个像素。
- **JEPA**：试图描述下一张图的“含义”。（比如“那只猫会跳到桌子上”，而不是预测猫尾巴那根毛的 RGB 值）。

**2. 世界模型（World Model）的终极形态**
真正的世界模型，应该是在**抽象层面**运行的。
人类思考问题时，脑子里流转的是“概念”和“逻辑”，而不是具体的汉字或像素点。

- 当我们规划旅行时，我们在想“坐飞机去巴黎”，而不是在脑子里生成几万字的详细行程单 Token。

LeCun 认为，只有摆脱了对“生成细节”的执着，AI 才能获得真正的推理和规划能力。

## 三、 为什么 LLM 现在赢了？

既然 JEPA 听起来更合理，为什么现在统治世界的是 GPT（Transformer + Next Token Prediction）？

**1. “大道至简”的胜利**
Transformer 的最大优势在于**通用性**和**可扩展性（Scalability）**。
不管你是文本、图片还是代码，只要变成 Token，丢进同一个模型里算 Loss 就行了。这种简单粗暴的工程美学，极其适合在 GPU 集群上大规模并行训练。Sora 的成功证明了，即便是在视频领域，只要算力够大，离散 Token 依然能产生惊人的效果。

**2. 文本的特殊性**
对于**文本**这种高度抽象的人造符号系统，预测 Token 恰恰是最有效的。因为文字本身就是离散的。
LeCun 的理论在**视频**和**物理世界模拟**上可能更占优势，但在纯文本领域，Next Token Prediction 目前依然是无敌的。

## 四、 结语：两条道路的博弈

这是一场关于 AI 未来的路线之争。

- **OpenAI 派（自回归派）**：相信 **Scale is all you need**。只要算力够大，数据够多，Token 预测就能涌现出一切智能。
- **LeCun 派（世界模型派）**：认为现在的 LLM 只是在堆砌算力掩盖算法的低效。未来的 AGI 必须摆脱对 Token 的依赖，直接在抽象空间进行思考。

**作为工程师，我们现在的饭碗在 Token 里；但作为观察者，我们必须留意那些不在 Token 里的可能性。因为历史告诉我们，颠覆者往往来自边缘。**

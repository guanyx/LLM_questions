# 扩散模型：如何把一张“雪花屏”变回图片

我第一次听到“扩散模型”的时候，脑子里只有一个疑问：

为什么要把一张好好的图先加到几乎看不见，再费劲把它弄回来？

这篇文章想做的事很简单：用尽量少的数学，解释扩散模型在干什么，以及它和 GAN / VAE 的核心差异。你读完应该能回答这三个问题：

- “扩散模型”到底是什么（用一句话说清楚）
- 训练的时候模型在学什么
- 为什么它通常比 GAN 更稳定、比 VAE 更清晰，但生成更慢

---

## 1. 先看一个最直观的图

想象你有一张清晰的猫图 $x_0$，然后你每一步都往上撒一点高斯噪声。

```text
x0(清晰) -> x1 -> x2 -> ... -> xT(几乎纯噪声)
```

这条链叫“前向过程”：从数据走向噪声。它很“容易”，因为你只是在做加噪。

扩散模型真正厉害的地方在反过来：

```text
xT(噪声) -> x(T-1) -> ... -> x1 -> x0(清晰)
```

反向每走一步，模型都要回答一个问题：

在当前这张“带噪图”里，哪些部分更像噪声，应该去掉一点？

你可以把它理解成：不是一次性画出整张图，而是反复做很多次“清理”。

---

## 2. 训练时到底在学什么？

我见过最容易记住的版本是这样的：

1. 从数据集中拿一张真图 $x_0$
2. 随机挑一个时间步 $t$
3. 把噪声按规则加进去，得到 $x_t$
4. 让模型根据 $(x_t, t)$ 去预测“我加了多少噪声”
5. 用 MSE 比较预测噪声与真实噪声

也就是说，很多经典扩散模型（比如 DDPM）并不是直接预测“干净图像”，而是预测噪声 $\epsilon$（常见叫 $\epsilon$-prediction）。

更近一些的路线会让模型去预测别的量，比如 $v$（有时会被叫做 $v$-prediction、速度场、或者在更广义的“流匹配 / flow matching”框架里讨论）。直觉上你可以把它当成：

同样是在学“往哪边去更像一张图”，只是把走路方式优化得更直、更省步数。

---

## 3. 那它和 GAN / VAE 到底差在哪？

我喜欢用“训练目标”和“训练手感”来对比：

### GAN：对抗训练，结果可能很惊艳也可能很折腾

- 训练结构：生成器 + 判别器，互相博弈
- 优点：采样快（通常一步生成）、细节潜力很强
- 常见痛点：训练不稳定、容易模式坍塌（只爱生成少数几类“最容易骗过判别器”的结果）

### VAE：像在做压缩与重建，训练很稳但容易“平均化”

- 训练结构：编码器把图压到潜变量，再解码回来
- 优点：理论和训练都很稳定
- 常见痛点：生成图容易偏“糊”（很多细节在重建时被平均掉）

### 扩散模型：每一步都做一点“去噪”，稳、清晰，但慢

- 训练结构：反复学习“如何从 $x_t$ 往 $x_{t-1}$ 走”
- 优点：训练通常比较稳定；覆盖模式更全面；生成质量通常很高
- 常见痛点：采样要走很多步（虽然现在有各种加速：更好的采样器、蒸馏、LCM、以及更少步数的建模方式等）

如果只留一句话：GAN 是博弈，VAE 是压缩重建，扩散是迭代去噪。

---

## 4. “先破坏再重建”为什么不是自找麻烦？

我觉得可以把它当成“让问题变得可做”。

在高维像素空间里，随机挑一个点，大概率就是一张垃圾图（噪点、乱纹理、看不出东西）。

扩散模型做的事情是：把“从噪声走向数据”的路拆成很多小步，每一小步都足够简单——只要朝“更像数据分布”的方向挪一点点。

这也和你在图像编辑里的体验有点像：

- 一次把整张图画对很难
- 但如果你每次只修一个小错误，就会容易很多

---

## 5. 两个必须的“外挂”：时钟与坐标

反向去噪有个细节：不同时间步要做的事情不一样。

- $t$ 很大时：几乎全是噪声，动作可以大一点
- $t$ 很小时：结构基本出来了，动作要小心一点

所以模型通常会把时间步 $t$ 编码成向量喂进去（timestep embedding）。你可以把它当成给模型一块“现在是第几步”的提示牌。

另一个常见问题来自 Transformer 化的扩散模型（比如把图像切 patch 的 DiT 思路）：一旦图被切成小块，模型得知道每块在什么位置。

因此会用位置编码（positional embedding）。一些实现会用 RoPE（旋转位置编码）之类的方案，让模型在不同分辨率/长宽比下更好泛化。

---

## 6. 为什么现在的模型更“听得懂”长 prompt？

这件事经常被简化成一句话：文本编码器更强了。

早期很多扩散系统用 CLIP 之类的编码器来理解文本，它对“风格/语义”很有帮助，但在更长、更复杂的句子上容易丢掉细节。

而更近期的一些系统会引入更强的语言模型式编码器（例如 T5 系列的文本编码器）来做更细致的语言理解：关系、顺序、属性、甚至更精确的字词约束。

直觉上你可以这么想：

- 文本侧理解得越细，扩散模型在每一步“往哪改”就越不容易跑偏

---

## 7. 怎么知道“画得好不好”？

评估生成模型很难，因为“好不好”里混着很多东西：真实感、遵循提示词、审美偏好……

常见的三种思路：

1. FID：看生成分布和真实分布是否接近（更像“整体气质”）
2. CLIPScore：看图和文本是否匹配（更像“听不听话”）
3. 人类偏好投票：最终还是要人来选（尤其是审美/细节偏好）

---

## 小结

如果你只记得三件事：

- 扩散模型=从噪声开始，反复去噪，走回数据
- 训练里常见做法是“预测噪声”，从而学到每一步该往哪修
- 相比 GAN / VAE，它通常更稳、更清晰，但采样更慢（并且正在被各种加速方法改善）

# 扩散模型怎么知道“现在是第几步”？——揭秘时间步编码（Timestep Encoding）

在扩散模型的代码中，你经常会看到一个神秘的参数 `t` 被传得到处都是。

我们知道，扩散模型的去噪是一个**迭代过程**：从第 1000 步 -> 第 999 步 -> ... -> 第 0 步。

但是，神经网络（无论是 UNet 还是 DiT）本质上是一个**函数** $f(x)$。函数本身是没有“时间观念”的。如果你只给它看一张噪点图 $x_t$，它怎么知道这张图是刚开始加噪（第 5 步，几乎全是猫），还是快结束了（第 900 步，全是雪花点）？

这就需要我们显式地告诉模型：“注意了，现在是第 $t$ 步，请按这个阶段的去噪力度来工作。”

这个过程，就叫做 **Timestep Encoding（时间步编码）**。

---

## 1. 为什么不能直接传一个数字？

初学者最直观的想法可能是：直接把 $t$ 作为一个数字（Scalar）传进去不就行了？
比如把 $t=50$ 归一化成 $0.05$，然后作为一个额外的通道拼接到图片上？

**这样做效果很差。** 原因有两个：

1.  **神经网络对高频信息更敏感**：简单的标量输入很难让深层网络感知到细微的变化。
2.  **区分度低**：第 500 步和第 501 步，如果只是数字上的 $0.500$ 和 $0.501$，在神经网络看来几乎没区别。但实际上，这两个步骤对应的噪声分布系数 $\alpha_t$ 可能有很大差异，模型需要做出不同的反应。

## 2. 借用 Transformer 的智慧：正弦位置编码

扩散模型借用了 Transformer（Attention Is All You Need）中处理序列位置的方法——**Sinusoidal Positional Embeddings（正弦位置编码）**。

如果你还记得 Transformer 里的公式，它长这样：

$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})
$$

$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

别被公式吓跑！它的核心思想非常简单：

**把一个整数 $t$，变成一个高维向量。**

举个例子，假设我们要把 $t=50$ 映射成一个长度为 128 的向量：

- 向量的第 1 位，可能是 $\sin(50)$
- 向量的第 2 位，可能是 $\cos(50)$
- 向量的第 3 位，可能是 $\sin(50/10)$
- ...
- 向量的最后一位，可能是 $\cos(50/10000)$

**为什么要这样做？**
想象很多个不同转速的齿轮。

- 低位的维度变化很快（像秒针），能区分 $t=50$ 和 $t=51$ 的微小差别。
- 高位的维度变化很慢（像时针），能表示 $t=50$ 和 $t=800$ 的宏观差别。

这样一来，任何一个时间步 $t$，都会被转换成一个**独一无二、且特征丰富**的指纹向量。

## 3. 在网络中是如何“注入”的？

好了，现在我们把 $t$ 变成了一个向量（比如长度为 256 的 `time_emb`），接下来怎么喂给神经网络？

通常有两种做法，分别对应 UNet 和 DiT。

### A. 加法与乘法（Scale & Shift）—— UNet 的做法

在 UNet 的每一个卷积层（ResBlock）后面，我们都会把 `time_emb` 融合进去。
这种融合方式被称为 **AdaGN (Adaptive Group Normalization)**。

具体操作是：

1.  把 `time_emb` 通过一个小的全连接层（MLP），变换成两个向量：缩放因子 $\gamma$ (scale) 和 偏移量 $\beta$ (shift)。
2.  把原来的图像特征 $h$ 进行标准化。
3.  **核心操作**：$h_{new} = h \cdot (1 + \gamma) + \beta$

**通俗解释**：
这就是在对图像特征进行**“调制”**。
时间步 $t$ 就像是一个**总指挥**，它通过 $\gamma$ 和 $\beta$ 告诉每一层卷积层：“现在是第 900 步，噪声很大，你们处理的时候信号要放大一点，偏置要调高一点！”

### B. 调制 (Modulation) 与 AdaLN-Zero —— DiT (Sora/Flux/SD3) 的主流做法

在最新的 DiT (Diffusion Transformer) 架构中，直接把 `time_emb` 当作 Token 拼进去的做法（类似于 BERT 的 [CLS]）虽然可行，但已经不是最主流的了。

目前最先进的模型（如 **Stable Diffusion 3**, **Flux**）更倾向于使用 **AdaLN (Adaptive Layer Norm)** 或 **AdaLN-Zero**。

**它是怎么工作的？**
它和 UNet 的 AdaGN 非常像，也是一种“调制”机制，但它作用于 Transformer 的每一个 Block 之前。

1.  **输入**：拿到 `time_emb` 向量。
2.  **回归**：通过一个 MLP（全连接层）预测出 6 个参数（缩放因子 $\gamma$, 偏移量 $\beta$ 等），分别控制 Self-Attention 和 MLP 层的输入与输出。
3.  **零初始化 (Zero-Init)**：在训练开始时，把这些参数初始化为 0。这意味着 Transformer 一开始几乎是一个恒等映射（Identity），这大大加速了训练的稳定性。

**为什么比“Token 拼接”更好？**

- **全局广播**：调制机制让时间信息直接作用于特征图的统计分布上，相当于对整个网络进行了一次“全局广播”，而不需要让每个 Patch 自己去 Attention 里找时间 Token。
- **更强的控制力**：实验证明，这种 Scale & Shift 的调制方式对生成质量的提升更为显著。

## 总结

对于初级 AI 工程师，关于时间步编码需要记住：

1.  **不能只传数字**：神经网络看不懂光秃秃的数字，必须把它变成向量。
2.  **正弦编码是主流**：通过正弦余弦函数，把时间步映射到高维空间，既能区分微小差异，又能保留宏观顺序。
3.  **注入方式**：
    - **UNet**：通过 AdaGN 调制层，对特征图进行缩放和平移（Scale & Shift）。
    - **DiT**：虽然可以像 Token 一样拼接，但**AdaLN-Zero**（自适应层归一化）才是目前的 SOTA 做法，它通过调制机制更高效地注入时间信息。

只要理解了这一点，你在看代码时看到 `t_emb = get_timestep_embedding(t, dim)` 就不会再感到迷茫了。它是模型感知时间流逝的唯一感官。

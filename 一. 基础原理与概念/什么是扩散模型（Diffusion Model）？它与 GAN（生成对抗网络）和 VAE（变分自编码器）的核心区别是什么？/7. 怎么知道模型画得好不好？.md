# 怎么知道模型画得好不好？—— 浅谈 FID 与 CLIP Score

我们训练了一个扩散模型，Loss 曲线一路下降，看起来非常完美。
但是，当我们让它生成一张图片时，却发现它画出来的猫有 5 条腿，或者根本不是我们想要的猫。

这就引出了 AI 绘画领域一个极其棘手的问题：**如何量化评价一张“画”的好坏？**

对于分类任务，我们可以算准确率（Accuracy）；对于预测房价，我们可以算误差（MSE）。
但对于生成艺术，什么是“好”？是画得像真照片？还是构图优美？还是符合 Prompt？

目前业界主要靠两个指标来“打分”：**FID** 和 **CLIP Score**。

---

## 1. FID (Fréchet Inception Distance) —— 测测“真实感”

**FID** 是目前学术界衡量生成图像质量的**金标准**。

它的核心逻辑是：**“不要看具体的某一张图，要看整体的气质（分布）。”**

### FID 是怎么算的？

1.  **找个“评委”**：我们请出一位资深的图像识别模型——**Inception V3**（由 Google 训练，阅图无数）。
2.  **看真图**：让 Inception V3 看 50,000 张真实的猫片（COCO 或 ImageNet 数据集），提取它们的**特征向量**。我们算出这些特征的均值和方差，代表“真实猫”的分布 $D_{real}$。
3.  **看假图**：让 Inception V3 看 50,000 张模型生成的猫片，同样提取特征，算出“假猫”的分布 $D_{fake}$。
4.  **算距离**：计算这两个分布（高斯分布）之间的**数学距离**（Fréchet Distance）。

### FID 越低越好

- **FID = 0**：说明两组图片的分布完全一致。你的模型生成的图和真图在统计学上无法区分。
- **FID 越小**：说明生成的图越**真实**，且**多样性**（Diversity）越好。
- **FID 很大**：说明生成的图要么**很假**（伪影严重），要么**很单调**（Mode Collapse，生成的一万张猫都长得一样）。

**比喻**：
FID 就像是在对比两个“班级”的学生气质。

- A 班（真图）的学生：活泼、穿搭时尚、各具特色。
- B 班（假图）的学生：死气沉沉、穿搭怪异、或者所有人都穿一样的校服。
  FID 就是衡量这两个班级气质差异的分数。

---

## 2. CLIP Score —— 测测“听话程度”

FID 虽然能衡量图片像不像真的，但它有个大缺陷：**它不管你画的是什么。**
如果我让你画“一只狗”，你画了一张极度逼真的“猫”，FID 可能会很低（因为猫也是真实的），但这对用户来说是完全错误的。

这就需要 **CLIP Score** 来补位。

### CLIP Score 是怎么算的？

它利用了我们上一篇文章提到的 **CLIP 模型**。

1.  **文本变向量**：把 Prompt ("A cute dog") 扔进 CLIP Text Encoder，得到文本向量 $V_{text}$。
2.  **图片变向量**：把生成的图片扔进 CLIP Image Encoder，得到图片向量 $V_{image}$。
3.  **算相似度**：计算这两个向量的**余弦相似度**（Cosine Similarity）。

### CLIP Score 越高越好

- **分数高**：说明图片内容和文字描述**高度一致**。
- **分数低**：说明图文不符。

**比喻**：
这就是在做“看图说话”考试。老师（CLIP）看了一眼你的作文（Prompt）和你的配图，判断它们是不是在讲同一个故事。

---

## 3. 进阶：GenEval 与 T2I-CompBench —— 更细粒度的体检

虽然 FID 和 CLIP Score 是“常规体检”，但对于现代模型（如 SD3, Flux）来说，它们太粗糙了。
最新的前沿评估开始关注**细粒度（Fine-grained）**的能力。

### 为什么需要更细的指标？

CLIP Score 只能告诉你“这图大概画对了”，但它分不清：

- **颜色绑定**：“红色的鞋子和蓝色的帽子”有没有画反？
- **空间关系**：“猫在狗的左边”有没有画成右边？
- **数量感知**：“三只苹果”有没有画成四只？

### 前沿指标介绍：

1.  **GenEval**：一个专门设计的 Prompt 测试集。它不看整体分数，而是像“单元测试”一样，逐项检查模型的能力。
    - _测试题_：“画一个红色的球在蓝色的立方体上面。”
    - _检测_：用专门的检测器（Object Detector）去数数、看颜色、看位置。
2.  **T2I-CompBench**：专门测试**组合生成能力（Compositionality）**。
    - 它发现很多 FID 很低的模型，在处理复杂的属性绑定（Attribute Binding）时其实一塌糊涂。

不要只看 CLIP Score 一个数字。如果你的业务场景对“精准度”要求很高（比如电商海报生成），你必须构建自己的**细粒度测试集**。

## 4. 陷阱：数据好看 = 图片好看？

虽然指标很多，但它们并不完美，甚至经常**误导**开发者。

### 陷阱 A：FID 的欺骗性

FID 偏爱“纹理清晰”、“边缘锐利”的图片。
有时候，一张图虽然模糊、充满了噪点，但在人类看来很有艺术感。但 FID 会给它打很差的分。
反之，模型可能会通过生成一些奇怪的高频纹理来“刷”FID 分数，但人眼看起来却很怪异。

### 陷阱 B：CLIP Score 的过拟合 (Reward Hacking)

如果你过度追求 CLIP Score，模型可能会变得**刻板**。
比如 Prompt 是 "Apple"，模型为了拿高分，可能会在画面里堆满苹果，甚至直接把 "Apple" 这个单词写在图片上（因为 CLIP 也能认字）。
这在强化学习（RLHF）微调扩散模型时尤其常见，被称为 **Goodhart's Law**（当一个指标变成目标时，它就不再是一个好指标）。

## 5. 终极裁判：Human Eval & ELO Rating

因为数学指标总有缺陷，所以在顶级论文（如 DALL-E 3, SDXL）和商业产品（Midjourney）的评估中，**人类投票**依然是不可替代的环节。

- **Side-by-Side (SBS)**：把模型 A 和模型 B 生成的图放在一起，让人类选“哪张更好看”或“哪张更符合 Prompt”。
- **Elo Rating (Chatbot Arena)**：
  - 目前公认最权威的榜单（如 HuggingFace 的 **Artificial Analysis Image Arena**）。
  - 它像围棋或王者荣耀的排位赛一样，让不同的模型（Flux, Midjourney, SD3）互相 PK，由真实用户投票，算出天梯分。
  - **结论**：Elo 分数通常比 FID 更能反映用户的真实喜好。

## 总结

1.  **开发阶段**：跑 **FID** 和 **CLIP Score** 快速迭代。
2.  **精细化阶段**：使用 **GenEval** 或 **T2I-CompBench** 测试空间、颜色、数量等特定能力。
3.  **发布阶段**：不要迷信数字。参考 **Elo Rating**，并组织 **Human Eval**（SBS 测试）。

记住：**User Preference (用户偏好) > Elo Rating > GenEval > CLIP Score > FID**。

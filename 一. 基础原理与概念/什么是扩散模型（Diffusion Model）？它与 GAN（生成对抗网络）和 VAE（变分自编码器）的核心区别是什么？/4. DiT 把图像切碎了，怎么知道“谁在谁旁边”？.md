# DiT 把图像切碎了，怎么知道“谁在谁旁边”？——揭秘位置编码（Positional Embedding）

随着 Sora、Stable Diffusion 3 和 Flux 的爆火，**DiT (Diffusion Transformer)** 架构已经成为了扩散模型的新标准。

相比于传统的 UNet，DiT 最大的改变就是**引入了 Transformer**。
而 Transformer 的第一步，就是把一张完整的图片（比如 256x256 像素），切成一个个小方块（比如 32x32 的 Patches），然后把它们拉直成一条长长的序列。

这就带来了一个巨大的问题：**空间信息丢失**。

- **CNN (UNet)**：天生具有空间感。卷积核在图片上滑动，它永远知道（x, y）坐标周围的像素是邻居。
- **Transformer (DiT)**：天生是“脸盲”。一旦图片被切碎拉直成序列，模型就只看到一堆 Token。如果没有额外的提示，它根本不知道第 1 个 Patch（左上角）和第 10 个 Patch（可能是第二行）在原始图片里是不是挨着的。

为了解决这个问题，我们需要给每一个 Patch 发一张“身份证”，上面写着它的家庭住址。这就是 **Positional Embedding (位置编码)**。

---

## 1. 简单粗暴法：绝对位置编码 (Absolute PE) —— 早期的尝试

这是最基础的方法，也是最初 Vision Transformer (ViT) 和初代 DiT 采用的方法。

**原理**：
我们直接训练一组向量，分别代表“位置 1”、“位置 2”、“位置 3”……

- 当 Patch 1 进来时，我们就给它加上“位置 1 向量”。
- 当 Patch 100 进来时，我们就给它加上“位置 100 向量”。

这些“位置向量”是**可学习参数**（Learnable Parameters）。在训练过程中，模型自己去琢磨：“哦，原来带着‘位置 1 向量’的那个家伙总是出现在左上角，带着‘位置 100 向量’的家伙总是在中间。”

**缺点**：
这种方法很死板。如果你训练时用的图片是 256x256 的，模型就只背下了这 256 个位置。
一旦你想生成一张 512x512 的大图，Patch 数量变多了，多出来的那些 Patch 对应的位置向量，模型从来没见过，直接懵圈。这就是为什么早期的 DiT 很难**外推（Extrapolate）**到更高分辨率。

## 2. 进阶法：旋转位置编码 (RoPE - Rotary Positional Embeddings) —— 现在的标配

这是目前 LLM（如 LLaMA）和最新的 DiT（如 Flux, Stable Diffusion 3）最青睐的方法。

RoPE 的数学推导有点复杂（涉及复数旋转），但我们可以用一个直观的几何比喻来理解。

**直觉比喻**：
想象每一个 Token（Patch）都是二维平面上的一个向量（箭头）。

- **传统的加法编码**：是给这个箭头**加上**另一个固定的箭头（位置向量）。
- **RoPE**：是把这个箭头**旋转**一个角度。

**怎么转？**

- 如果是第 1 个 Patch，转 $\theta$ 度。
- 如果是第 2 个 Patch，转 $2\theta$ 度。
- 如果是第 $m$ 个 Patch，转 $m\theta$ 度。

**为什么这样好？**
RoPE 的神奇之处在于，它能完美地保留**相对位置信息**。
当模型计算两个 Patch 之间的关系（Attention）时，它只关心这两个 Patch **“相对转了多少度”**（即位置差 $m-n$），而不在乎它们具体在哪个绝对位置。

这就意味着，即使是模型没见过的超长序列（超大分辨率图片），只要相对位置关系没变，RoPE 依然能工作得很好。这也是 Flux 和 SD3 能够轻松生成各种长宽比、各种分辨率图像的关键秘密武器。

## 3. 2D 还是 1D？——处理图像特有的纠结

文本是一维的（只有前后关系），但图像是二维的（有行和列）。
所以，DiT 的位置编码通常是 **2D** 的。

- **做法**：我们会分别准备两套编码。
  - 一套编码表示 **X 轴坐标**（我是第几列）。
  - 一套编码表示 **Y 轴坐标**（我是第几行）。
- **合成**：对于坐标为 $(x, y)$ 的 Patch，它的最终位置编码就是 `Embedding(x) + Embedding(y)` （或者拼接）。
- **Flux 的黑科技**：Flux 模型甚至引入了 **3D RoPE**（如果是视频生成，还会加上时间轴 T），让模型同时理解长、宽、时间三个维度的相对位置。

这样，模型就能清晰地知道：“哦，这个 Patch 在第 3 行第 5 列，那个 Patch 在第 3 行第 6 列，它俩是左右邻居！”

## 总结

对于初级 AI 工程师，关于 DiT 的位置编码，你需要知道：

1.  **Transformer 是脸盲**：切成 Patch 后，必须手动注入位置信息，否则模型就是瞎子。
2.  **RoPE 是新宠**：相比于老式的绝对位置编码，RoPE（旋转位置编码）通过“旋转向量”的方式注入信息，具有更好的**外推性**（Extrapolation），能更好地支持不同分辨率的生成。
3.  **2D 结构很重要**：图像不同于文本，必须同时编码 X 和 Y 两个维度的坐标信息，才能让模型理解图像的二维空间结构。

下次看到 DiT 的代码里有 `apply_rotary_emb(q, k, freqs_cis)` 这样的函数时，你就知道：**这是在给打散的拼图碎片贴上坐标标签呢！**
